[01/13 16:23:07] detectron2 INFO: Rank of current process: 2. World size: 4
[01/13 16:23:09] detectron2 INFO: Environment info:
-------------------------------  --------------------------------------------------------------------------------------------------
sys.platform                     linux
Python                           3.10.11 (main, May 16 2023, 00:28:57) [GCC 11.2.0]
numpy                            1.26.2
detectron2                       0.6 @/homes/55/anjun/.local/lib/python3.10/site-packages/detectron2
Compiler                         GCC 8.3
CUDA compiler                    CUDA 12.1
detectron2 arch flags            8.6
DETECTRON2_ENV_MODULE            <not set>
PyTorch                          2.1.1+cu121 @/scratch/local/ssd/anjun/anaconda3/envs/ldm/lib/python3.10/site-packages/torch
PyTorch debug build              False
torch._C._GLIBCXX_USE_CXX11_ABI  False
GPU available                    Yes
GPU 0,1,2,3                      NVIDIA A40 (arch=8.6)
Driver version                   530.30.02
CUDA_HOME                        /usr/local/cuda-12.1/
Pillow                           9.5.0
torchvision                      0.16.1+cu121 @/scratch/local/ssd/anjun/anaconda3/envs/ldm/lib/python3.10/site-packages/torchvision
torchvision arch flags           5.0, 6.0, 7.0, 7.5, 8.0, 8.6, 9.0
fvcore                           0.1.5.post20221221
iopath                           0.1.9
cv2                              4.5.4-dev
-------------------------------  --------------------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v3.1.1 (Git Hash 64f6bcbcbab628e96f33a62c3e975f8535a7bde4)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 12.1
  - NVCC architecture flags: -gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.9.2
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=12.1, CUDNN_VERSION=8.9.2, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=old-style-cast -Wno-invalid-partial-specialization -Wno-unused-private-field -Wno-aligned-allocation-unavailable -Wno-missing-braces -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.1.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[01/13 16:23:09] detectron2 INFO: Command line arguments: Namespace(config_file='configs/coco-stuff-164k-156/zero_shot_maskformer_R101c_bs32_60k_proposalmask_featupsample_img512.yaml', resume=True, eval_only=False, num_gpus=4, num_machines=1, machine_rank=0, dist_url='tcp://0.0.0.0:12237', opts=['MODEL.CLIP_ADAPTER.CLIP_ENSEMBLE', 'False', 'MODEL.CLIP_ADAPTER.CLIP_ENSEMBLE_WEIGHT', '-1.0', 'SOLVER.TEST_IMS_PER_BATCH', '1', 'OUTPUT_DIR', 'deop/train_log', 'MODEL.CLIP_ADAPTER.PROMPT_LEARNER', 'learnable', 'SOLVER.IMS_PER_BATCH', '16', 'SOLVER.BASE_LR', '0.00005', 'SOLVER.CHECKPOINT_PERIOD', '1000', 'MODEL.CLIP_ADAPTER.CLIP_MODEL_NAME', 'ViT-B/16', 'MODEL.WEIGHTS', 'pretrained_models/deop_model_final.pth', 'MODEL.META_ARCHITECTURE', 'ZeroShotClipfeatUpsample_addnorm_vit16Query2D_maskvit', 'MODEL.NUM_DECODER_LAYER', '1', 'ORACLE', 'False', 'INPUT.MIN_SIZE_TEST', '512', 'TEST.EVAL_PERIOD', '1000', 'INPUT.SIZE_DIVISIBILITY', '512', 'MODEL.MASK_FORMER.DECODER_DICE_WEIGHT', '0.8', 'MODEL.MASK_FORMER.DECODER_CE_WEIGHT', '2.0', 'MODEL.CLIP_ADAPTER.LEARN_POSITION', 'True', 'MODEL.CLIP_ADAPTER.POSITION_LAYERS', '[1,2,3,4,5,6,7,8,9,10,11]', 'MODEL.CLIP_ADAPTER.LAYERMASKVIT', '[11]', 'MODEL.CLIP_ADAPTER.END2ENDTRAIN', 'False', 'MODEL.CLIP_ADAPTER.PS_SHORTCUT', '1.0', 'DATASETS.TRAIN', "('coco_2017_train_stuff_all_sem_seg',)", 'DATASETS.TEST', "('context_59_test_sem_seg',)", 'MODEL.SEM_SEG_HEAD.NUM_CLASSES', '171', 'MODEL.MASK_FORMER.LOSS_NUM_CLASS', '171'])
[01/13 16:23:09] detectron2 INFO: Contents of args.config_file=configs/coco-stuff-164k-156/zero_shot_maskformer_R101c_bs32_60k_proposalmask_featupsample_img512.yaml:
[38;5;197m_BASE_[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mzero_shot_maskformer_R50_bs32_60k.yaml[39m
[38;5;197mMODEL[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mSEM_SEG_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mNUM_CLASSES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m171[39m
[38;5;15m  [39m[38;5;197mMETA_ARCHITECTURE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mZeroShotMaskFormerClipfeatUpsample[39m[38;5;186m"[39m
[38;5;15m  [39m[38;5;197mBACKBONE[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mbuild_resnet_deeplab_backbone[39m[38;5;186m"[39m
[38;5;15m  [39m
[38;5;15m  [39m[38;5;197mWEIGHTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mdetectron2://DeepLab/R-103.pkl[39m[38;5;186m"[39m
[38;5;15m  [39m[38;5;197mRESNETS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mDEPTH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m101[39m
[38;5;15m    [39m[38;5;197mSTEM_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mdeeplab[39m[38;5;186m"[39m
[38;5;15m    [39m[38;5;197mSTEM_OUT_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m128[39m
[38;5;15m    [39m[38;5;197mSTRIDE_IN_1X1[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mFalse[39m
[38;5;15m    [39m[38;5;197mOUT_FEATURES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;186m"[39m[38;5;186mres2[39m[38;5;186m"[39m[38;5;15m,[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mres3[39m[38;5;186m"[39m[38;5;15m,[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mres4[39m[38;5;186m"[39m[38;5;15m,[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mres5[39m[38;5;186m"[39m[38;5;15m][39m
[38;5;15m    [39m[38;5;242m# NORM: "SyncBN"[39m
[38;5;15m    [39m[38;5;197mRES5_MULTI_GRID[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m1[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15m2[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15m4[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;197mCLIP_ADAPTER[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCLIP_ENSEMBLE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mTrue[39m
[38;5;15m    [39m[38;5;197mCLIP_ENSEMBLE_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.8[39m
[38;5;197mINPUT[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mMIN_SIZE_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;81m!!python/object/apply:eval[39m[38;5;15m [39m[38;5;15m[[39m[38;5;186m"[39m[38;5;186m[int(x[39m[38;5;15m [39m[38;5;186m*[39m[38;5;15m [39m[38;5;186m0.1[39m[38;5;15m [39m[38;5;186m*[39m[38;5;15m [39m[38;5;186m512)[39m[38;5;15m [39m[38;5;186mfor[39m[38;5;15m [39m[38;5;186mx[39m[38;5;15m [39m[38;5;186min[39m[38;5;15m [39m[38;5;186mrange(5,[39m[38;5;15m [39m[38;5;186m16)][39m[38;5;186m"[39m[38;5;15m][39m
[38;5;197mOUTPUT_DIR[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mdeop/train_log[39m[38;5;186m"[39m
[38;5;197mSOLVER[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mIMS_PER_BATCH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m8[39m
[38;5;15m  [39m[38;5;197mTEST_IMS_PER_BATCH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m8[39m
[38;5;197mTEST[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mEVAL_PERIOD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m5000[39m

[01/13 16:23:09] detectron2.utils.env INFO: Using a generated random seed 9605756
[01/13 16:23:10] mask_former.modeling.clip_adapter INFO: Prompt Learner training params: ['prefix_prompt']
[01/13 16:23:18] detectron2.engine.defaults INFO: Model:
ZeroShotClipfeatUpsample_addnorm_vit16Query2D_maskvit(
  (backbone): ResNet(
    (stem): DeepLabStem(
      (conv1): Conv2d(
        3, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
      )
      (conv3): Conv2d(
        64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
      )
    )
    (res2): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv1): Conv2d(
          128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
      )
    )
    (res3): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv1): Conv2d(
          256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
      (3): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
    )
    (res4): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
        (conv1): Conv2d(
          512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (3): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (4): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (5): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (6): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (7): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (8): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (9): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (10): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (11): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (12): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (13): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (14): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (15): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (16): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (17): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (18): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (19): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (20): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (21): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (22): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
    )
    (res5): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
        (conv1): Conv2d(
          1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
      )
    )
  )
  (sem_seg_head): ZeroShotMaskFormerHead(
    (pixel_decoder): BasePixelDecoder(
      (adapter_1): Conv2d(
        256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (layer_1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (adapter_2): Conv2d(
        512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (layer_2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (adapter_3): Conv2d(
        1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (layer_3): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (layer_4): Conv2d(
        2048, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (mask_features): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
    (predictor): ZeroShotTransformerPredictor(
      (pe_layer): PositionEmbeddingSine()
      (transformer): Transformer(
        (encoder): TransformerEncoder(
          (layers): ModuleList()
        )
        (decoder): TransformerDecoder(
          (layers): ModuleList(
            (0-5): 6 x TransformerDecoderLayer(
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (multihead_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (linear1): Linear(in_features=256, out_features=2048, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (linear2): Linear(in_features=2048, out_features=256, bias=True)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (dropout1): Dropout(p=0.1, inplace=False)
              (dropout2): Dropout(p=0.1, inplace=False)
              (dropout3): Dropout(p=0.1, inplace=False)
            )
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (query_embed): Embedding(100, 256)
      (input_proj): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
      (mask_embed): MLP(
        (layers): ModuleList(
          (0-2): 3 x Linear(in_features=256, out_features=256, bias=True)
        )
      )
      (class_embed): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=1024, bias=True)
          (1): Linear(in_features=1024, out_features=512, bias=True)
        )
      )
    )
  )
  (criterion): SetCriterion(
    (matcher): Matcher HungarianMatcher
        cost_class: 1
        cost_mask: 20.0
        cost_dice: 1.0
  )
  (clip_adapter): MaskFormerClipFeatureAdapter(
    (clip_model): CLIP(
      (visual): VisionTransformerLearnPosition(
        (conv1): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False)
        (ln_pre): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (transformer): TransformerVitPosition(
          (resblocks): Sequential(
            (0): ResidualAttentionBlockVitPosition(
              (attn): MultiheadAttentionLoRA(
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (1): ResidualAttentionBlockVitPosition(
              (attn): MultiheadAttentionLoRA(
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (2): ResidualAttentionBlockVitPosition(
              (attn): MultiheadAttentionLoRA(
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (3): ResidualAttentionBlockVitPosition(
              (attn): MultiheadAttentionLoRA(
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (4): ResidualAttentionBlockVitPosition(
              (attn): MultiheadAttentionLoRA(
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (5): ResidualAttentionBlockVitPosition(
              (attn): MultiheadAttentionLoRA(
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (6): ResidualAttentionBlockVitPosition(
              (attn): MultiheadAttentionLoRA(
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (7): ResidualAttentionBlockVitPosition(
              (attn): MultiheadAttentionLoRA(
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (8): ResidualAttentionBlockVitPosition(
              (attn): MultiheadAttentionLoRA(
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (9): ResidualAttentionBlockVitPosition(
              (attn): MultiheadAttentionLoRA(
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (10): ResidualAttentionBlockVitPosition(
              (attn): MultiheadAttentionLoRA(
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (11): ResidualAttentionBlockVitPosition(
              (attn): MultiheadAttentionVit(
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
        (ln_post): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (transformer): Transformer(
        (resblocks): Sequential(
          (0): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (1): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (2): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (3): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (4): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (5): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (6): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (7): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (8): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (9): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (10): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (11): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (token_embedding): Embedding(49408, 512)
      (ln_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (prompt_learner): LearnablePromptExtractor(
      prefix_prompt:16,suffix_prompt:0,dimension:512
      [Normal_Init(mu=0,std=0.02)]
    )
  )
  (decoder): TransformerDecoderLinear(
    (layers): ModuleList(
      (0): TransformerDecoderLayerLinear(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
        )
        (multihead_attn_my): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
        )
        (linear1): Linear(in_features=512, out_features=1024, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=1024, out_features=512, bias=True)
        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
    )
    (outlayer): TransformerDecoderOutLayerLinear(
      (self_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
      )
      (multihead_attn_my): MultiHeadAttentionMy(
        (dropout_F): Dropout(p=0.1, inplace=False)
      )
      (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (dropout1): Dropout(p=0.1, inplace=False)
      (dropout2): Dropout(p=0.1, inplace=False)
      (dropout3): Dropout(p=0.1, inplace=False)
    )
    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  )
  (pe_layer): PositionEmbeddingSine()
  (query_embedding): Embedding(100, 512)
)
[01/13 16:23:18] mask_former.data.dataset_mappers.mask_former_semantic_dataset_mapper INFO: [MaskFormerSemanticDatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=(256, 307, 358, 409, 460, 512, 563, 614, 665, 716, 768), max_size=2560, sample_style='choice'), RandomCrop_CategoryAreaConstraint(crop_type='absolute', crop_size=[640, 640], single_category_max_area=1.0, ignored_category=255), <detectron2.projects.point_rend.color_augmentation.ColorAugSSDTransform object at 0x7f74681c2fe0>, RandomFlip()]
[01/13 16:23:20] detectron2.data.datasets.coco INFO: Loaded 118287 images with semantic segmentation from /scratch/local/ssd/anjun/datasets/coco-stuff/images/train2017
[01/13 16:23:21] mask_former.data.build INFO: Using training sampler TrainingSampler
[01/13 16:23:21] detectron2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[01/13 16:23:21] detectron2.data.common INFO: Serializing 118287 elements to byte tensors and concatenating them all ...
[01/13 16:23:21] detectron2.data.common INFO: Serialized dataset takes 32.38 MiB
[01/13 16:23:21] detectron2.data.build INFO: Making batched data loader with batch_size=4
[01/13 16:23:22] detectron2.checkpoint.detection_checkpoint INFO: [DetectionCheckpointer] Loading from pretrained_models/deop_model_final.pth ...
[01/13 16:23:22] fvcore.common.checkpoint INFO: [Checkpointer] Loading from pretrained_models/deop_model_final.pth ...
[01/13 16:23:22] fvcore.common.checkpoint WARNING: Skip loading parameter 'criterion.empty_weight' to the model due to incompatible shapes: (157,) in the checkpoint but (172,) in the model! You might want to double check if this is expected.
[01/13 16:23:22] fvcore.common.checkpoint WARNING: Some model parameters or buffers are not found in the checkpoint:
[34mclip_adapter.clip_model.token_embedding.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.10.learned_position[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.11.attn.out_proj.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.11.learned_position[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.6.learned_position[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.7.learned_position[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.8.learned_position[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.9.learned_position[0m
[34mcriterion.empty_weight[0m
[01/13 16:23:22] detectron2.engine.train_loop INFO: Starting training from iteration 0
[01/13 16:23:40] detectron2.engine.hooks INFO: Total training time: 0:00:00 (0:00:00 on hooks)
[01/13 16:28:36] detectron2 INFO: Rank of current process: 2. World size: 4
[01/13 16:28:42] detectron2 INFO: Environment info:
-------------------------------  --------------------------------------------------------------------------------------------------
sys.platform                     linux
Python                           3.10.11 (main, May 16 2023, 00:28:57) [GCC 11.2.0]
numpy                            1.26.2
detectron2                       0.6 @/homes/55/anjun/.local/lib/python3.10/site-packages/detectron2
Compiler                         GCC 8.3
CUDA compiler                    CUDA 12.1
detectron2 arch flags            8.6
DETECTRON2_ENV_MODULE            <not set>
PyTorch                          2.1.1+cu121 @/scratch/local/ssd/anjun/anaconda3/envs/ldm/lib/python3.10/site-packages/torch
PyTorch debug build              False
torch._C._GLIBCXX_USE_CXX11_ABI  False
GPU available                    Yes
GPU 0,1,2,3                      NVIDIA A40 (arch=8.6)
Driver version                   530.30.02
CUDA_HOME                        /usr/local/cuda-12.1/
Pillow                           9.5.0
torchvision                      0.16.1+cu121 @/scratch/local/ssd/anjun/anaconda3/envs/ldm/lib/python3.10/site-packages/torchvision
torchvision arch flags           5.0, 6.0, 7.0, 7.5, 8.0, 8.6, 9.0
fvcore                           0.1.5.post20221221
iopath                           0.1.9
cv2                              4.5.4-dev
-------------------------------  --------------------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v3.1.1 (Git Hash 64f6bcbcbab628e96f33a62c3e975f8535a7bde4)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 12.1
  - NVCC architecture flags: -gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.9.2
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=12.1, CUDNN_VERSION=8.9.2, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=old-style-cast -Wno-invalid-partial-specialization -Wno-unused-private-field -Wno-aligned-allocation-unavailable -Wno-missing-braces -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.1.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[01/13 16:28:42] detectron2 INFO: Command line arguments: Namespace(config_file='configs/coco-stuff-164k-156/zero_shot_maskformer_R101c_bs32_60k_proposalmask_featupsample_img512.yaml', resume=True, eval_only=False, num_gpus=4, num_machines=1, machine_rank=0, dist_url='tcp://0.0.0.0:12237', opts=['MODEL.CLIP_ADAPTER.CLIP_ENSEMBLE', 'False', 'MODEL.CLIP_ADAPTER.CLIP_ENSEMBLE_WEIGHT', '-1.0', 'SOLVER.TEST_IMS_PER_BATCH', '1', 'OUTPUT_DIR', 'deop/train_log', 'MODEL.CLIP_ADAPTER.PROMPT_LEARNER', 'learnable', 'SOLVER.IMS_PER_BATCH', '16', 'SOLVER.BASE_LR', '0.00005', 'SOLVER.CHECKPOINT_PERIOD', '1000', 'MODEL.CLIP_ADAPTER.CLIP_MODEL_NAME', 'ViT-B/16', 'MODEL.WEIGHTS', 'pretrained_models/deop_model_final.pth', 'MODEL.META_ARCHITECTURE', 'ZeroShotClipfeatUpsample_addnorm_vit16Query2D_maskvit', 'MODEL.NUM_DECODER_LAYER', '1', 'ORACLE', 'False', 'INPUT.MIN_SIZE_TEST', '512', 'TEST.EVAL_PERIOD', '1000', 'INPUT.SIZE_DIVISIBILITY', '512', 'MODEL.MASK_FORMER.DECODER_DICE_WEIGHT', '0.8', 'MODEL.MASK_FORMER.DECODER_CE_WEIGHT', '2.0', 'MODEL.CLIP_ADAPTER.LEARN_POSITION', 'True', 'MODEL.CLIP_ADAPTER.POSITION_LAYERS', '[1,2,3,4,5,6,7,8,9,10,11]', 'MODEL.CLIP_ADAPTER.LAYERMASKVIT', '[11]', 'MODEL.CLIP_ADAPTER.END2ENDTRAIN', 'False', 'MODEL.CLIP_ADAPTER.PS_SHORTCUT', '1.0', 'DATASETS.TRAIN', "('coco_2017_train_stuff_all_sem_seg',)", 'DATASETS.TEST', "('context_59_test_sem_seg',)", 'MODEL.SEM_SEG_HEAD.NUM_CLASSES', '171', 'MODEL.MASK_FORMER.LOSS_NUM_CLASS', '171'])
[01/13 16:28:42] detectron2 INFO: Contents of args.config_file=configs/coco-stuff-164k-156/zero_shot_maskformer_R101c_bs32_60k_proposalmask_featupsample_img512.yaml:
[38;5;197m_BASE_[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mzero_shot_maskformer_R50_bs32_60k.yaml[39m
[38;5;197mMODEL[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mSEM_SEG_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mNUM_CLASSES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m171[39m
[38;5;15m  [39m[38;5;197mMETA_ARCHITECTURE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mZeroShotMaskFormerClipfeatUpsample[39m[38;5;186m"[39m
[38;5;15m  [39m[38;5;197mBACKBONE[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mbuild_resnet_deeplab_backbone[39m[38;5;186m"[39m
[38;5;15m  [39m
[38;5;15m  [39m[38;5;197mWEIGHTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mdetectron2://DeepLab/R-103.pkl[39m[38;5;186m"[39m
[38;5;15m  [39m[38;5;197mRESNETS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mDEPTH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m101[39m
[38;5;15m    [39m[38;5;197mSTEM_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mdeeplab[39m[38;5;186m"[39m
[38;5;15m    [39m[38;5;197mSTEM_OUT_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m128[39m
[38;5;15m    [39m[38;5;197mSTRIDE_IN_1X1[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mFalse[39m
[38;5;15m    [39m[38;5;197mOUT_FEATURES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;186m"[39m[38;5;186mres2[39m[38;5;186m"[39m[38;5;15m,[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mres3[39m[38;5;186m"[39m[38;5;15m,[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mres4[39m[38;5;186m"[39m[38;5;15m,[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mres5[39m[38;5;186m"[39m[38;5;15m][39m
[38;5;15m    [39m[38;5;242m# NORM: "SyncBN"[39m
[38;5;15m    [39m[38;5;197mRES5_MULTI_GRID[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m1[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15m2[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15m4[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;197mCLIP_ADAPTER[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCLIP_ENSEMBLE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mTrue[39m
[38;5;15m    [39m[38;5;197mCLIP_ENSEMBLE_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.8[39m
[38;5;197mINPUT[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mMIN_SIZE_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;81m!!python/object/apply:eval[39m[38;5;15m [39m[38;5;15m[[39m[38;5;186m"[39m[38;5;186m[int(x[39m[38;5;15m [39m[38;5;186m*[39m[38;5;15m [39m[38;5;186m0.1[39m[38;5;15m [39m[38;5;186m*[39m[38;5;15m [39m[38;5;186m512)[39m[38;5;15m [39m[38;5;186mfor[39m[38;5;15m [39m[38;5;186mx[39m[38;5;15m [39m[38;5;186min[39m[38;5;15m [39m[38;5;186mrange(5,[39m[38;5;15m [39m[38;5;186m16)][39m[38;5;186m"[39m[38;5;15m][39m
[38;5;197mOUTPUT_DIR[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mdeop/train_log[39m[38;5;186m"[39m
[38;5;197mSOLVER[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mIMS_PER_BATCH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m8[39m
[38;5;15m  [39m[38;5;197mTEST_IMS_PER_BATCH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m8[39m
[38;5;197mTEST[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mEVAL_PERIOD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m5000[39m

[01/13 16:28:42] detectron2.utils.env INFO: Using a generated random seed 42319499
[01/13 16:28:42] mask_former.modeling.clip_adapter INFO: Prompt Learner training params: ['prefix_prompt']
[01/13 16:28:47] detectron2.engine.defaults INFO: Model:
ZeroShotClipfeatUpsample_addnorm_vit16Query2D_maskvit(
  (backbone): ResNet(
    (stem): DeepLabStem(
      (conv1): Conv2d(
        3, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
      )
      (conv3): Conv2d(
        64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
      )
    )
    (res2): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv1): Conv2d(
          128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
      )
    )
    (res3): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv1): Conv2d(
          256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
      (3): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
    )
    (res4): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
        (conv1): Conv2d(
          512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (3): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (4): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (5): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (6): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (7): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (8): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (9): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (10): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (11): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (12): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (13): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (14): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (15): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (16): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (17): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (18): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (19): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (20): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (21): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (22): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
    )
    (res5): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
        (conv1): Conv2d(
          1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
      )
    )
  )
  (sem_seg_head): ZeroShotMaskFormerHead(
    (pixel_decoder): BasePixelDecoder(
      (adapter_1): Conv2d(
        256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (layer_1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (adapter_2): Conv2d(
        512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (layer_2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (adapter_3): Conv2d(
        1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (layer_3): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (layer_4): Conv2d(
        2048, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (mask_features): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
    (predictor): ZeroShotTransformerPredictor(
      (pe_layer): PositionEmbeddingSine()
      (transformer): Transformer(
        (encoder): TransformerEncoder(
          (layers): ModuleList()
        )
        (decoder): TransformerDecoder(
          (layers): ModuleList(
            (0-5): 6 x TransformerDecoderLayer(
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (multihead_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (linear1): Linear(in_features=256, out_features=2048, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (linear2): Linear(in_features=2048, out_features=256, bias=True)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (dropout1): Dropout(p=0.1, inplace=False)
              (dropout2): Dropout(p=0.1, inplace=False)
              (dropout3): Dropout(p=0.1, inplace=False)
            )
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (query_embed): Embedding(100, 256)
      (input_proj): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
      (mask_embed): MLP(
        (layers): ModuleList(
          (0-2): 3 x Linear(in_features=256, out_features=256, bias=True)
        )
      )
      (class_embed): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=1024, bias=True)
          (1): Linear(in_features=1024, out_features=512, bias=True)
        )
      )
    )
  )
  (criterion): SetCriterion(
    (matcher): Matcher HungarianMatcher
        cost_class: 1
        cost_mask: 20.0
        cost_dice: 1.0
  )
  (clip_adapter): MaskFormerClipFeatureAdapter(
    (clip_model): CLIP(
      (visual): VisionTransformerLearnPosition(
        (conv1): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False)
        (ln_pre): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (transformer): TransformerVitPosition(
          (resblocks): Sequential(
            (0): ResidualAttentionBlockVitPosition(
              (attn): MultiheadAttentionLoRA(
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (1): ResidualAttentionBlockVitPosition(
              (attn): MultiheadAttentionLoRA(
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (2): ResidualAttentionBlockVitPosition(
              (attn): MultiheadAttentionLoRA(
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (3): ResidualAttentionBlockVitPosition(
              (attn): MultiheadAttentionLoRA(
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (4): ResidualAttentionBlockVitPosition(
              (attn): MultiheadAttentionLoRA(
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (5): ResidualAttentionBlockVitPosition(
              (attn): MultiheadAttentionLoRA(
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (6): ResidualAttentionBlockVitPosition(
              (attn): MultiheadAttentionLoRA(
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (7): ResidualAttentionBlockVitPosition(
              (attn): MultiheadAttentionLoRA(
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (8): ResidualAttentionBlockVitPosition(
              (attn): MultiheadAttentionLoRA(
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (9): ResidualAttentionBlockVitPosition(
              (attn): MultiheadAttentionLoRA(
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (10): ResidualAttentionBlockVitPosition(
              (attn): MultiheadAttentionLoRA(
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (11): ResidualAttentionBlockVitPosition(
              (attn): MultiheadAttentionVit(
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
        (ln_post): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (transformer): Transformer(
        (resblocks): Sequential(
          (0): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (1): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (2): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (3): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (4): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (5): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (6): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (7): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (8): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (9): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (10): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (11): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (token_embedding): Embedding(49408, 512)
      (ln_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (prompt_learner): LearnablePromptExtractor(
      prefix_prompt:16,suffix_prompt:0,dimension:512
      [Normal_Init(mu=0,std=0.02)]
    )
  )
  (decoder): TransformerDecoderLinear(
    (layers): ModuleList(
      (0): TransformerDecoderLayerLinear(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
        )
        (multihead_attn_my): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
        )
        (linear1): Linear(in_features=512, out_features=1024, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=1024, out_features=512, bias=True)
        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
    )
    (outlayer): TransformerDecoderOutLayerLinear(
      (self_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
      )
      (multihead_attn_my): MultiHeadAttentionMy(
        (dropout_F): Dropout(p=0.1, inplace=False)
      )
      (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (dropout1): Dropout(p=0.1, inplace=False)
      (dropout2): Dropout(p=0.1, inplace=False)
      (dropout3): Dropout(p=0.1, inplace=False)
    )
    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  )
  (pe_layer): PositionEmbeddingSine()
  (query_embedding): Embedding(100, 512)
)
[01/13 16:28:47] mask_former.data.dataset_mappers.mask_former_semantic_dataset_mapper INFO: [MaskFormerSemanticDatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=(256, 307, 358, 409, 460, 512, 563, 614, 665, 716, 768), max_size=2560, sample_style='choice'), RandomCrop_CategoryAreaConstraint(crop_type='absolute', crop_size=[640, 640], single_category_max_area=1.0, ignored_category=255), <detectron2.projects.point_rend.color_augmentation.ColorAugSSDTransform object at 0x7ff76c093100>, RandomFlip()]
[01/13 16:28:49] detectron2.data.datasets.coco INFO: Loaded 118287 images with semantic segmentation from /scratch/local/ssd/anjun/datasets/coco-stuff/images/train2017
[01/13 16:28:49] mask_former.data.build INFO: Using training sampler TrainingSampler
[01/13 16:28:49] detectron2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[01/13 16:28:49] detectron2.data.common INFO: Serializing 118287 elements to byte tensors and concatenating them all ...
[01/13 16:28:50] detectron2.data.common INFO: Serialized dataset takes 32.38 MiB
[01/13 16:28:50] detectron2.data.build INFO: Making batched data loader with batch_size=4
[01/13 16:28:50] detectron2.checkpoint.detection_checkpoint INFO: [DetectionCheckpointer] Loading from pretrained_models/deop_model_final.pth ...
[01/13 16:28:50] fvcore.common.checkpoint INFO: [Checkpointer] Loading from pretrained_models/deop_model_final.pth ...
[01/13 16:29:51] detectron2 INFO: Rank of current process: 2. World size: 4
[01/13 16:29:58] detectron2 INFO: Environment info:
-------------------------------  --------------------------------------------------------------------------------------------------
sys.platform                     linux
Python                           3.10.11 (main, May 16 2023, 00:28:57) [GCC 11.2.0]
numpy                            1.26.2
detectron2                       0.6 @/homes/55/anjun/.local/lib/python3.10/site-packages/detectron2
Compiler                         GCC 8.3
CUDA compiler                    CUDA 12.1
detectron2 arch flags            8.6
DETECTRON2_ENV_MODULE            <not set>
PyTorch                          2.1.1+cu121 @/scratch/local/ssd/anjun/anaconda3/envs/ldm/lib/python3.10/site-packages/torch
PyTorch debug build              False
torch._C._GLIBCXX_USE_CXX11_ABI  False
GPU available                    Yes
GPU 0,1,2,3                      NVIDIA A40 (arch=8.6)
Driver version                   530.30.02
CUDA_HOME                        /usr/local/cuda-12.1/
Pillow                           9.5.0
torchvision                      0.16.1+cu121 @/scratch/local/ssd/anjun/anaconda3/envs/ldm/lib/python3.10/site-packages/torchvision
torchvision arch flags           5.0, 6.0, 7.0, 7.5, 8.0, 8.6, 9.0
fvcore                           0.1.5.post20221221
iopath                           0.1.9
cv2                              4.5.4-dev
-------------------------------  --------------------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v3.1.1 (Git Hash 64f6bcbcbab628e96f33a62c3e975f8535a7bde4)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 12.1
  - NVCC architecture flags: -gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.9.2
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=12.1, CUDNN_VERSION=8.9.2, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=old-style-cast -Wno-invalid-partial-specialization -Wno-unused-private-field -Wno-aligned-allocation-unavailable -Wno-missing-braces -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.1.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[01/13 16:29:58] detectron2 INFO: Command line arguments: Namespace(config_file='configs/coco-stuff-164k-156/zero_shot_maskformer_R101c_bs32_60k_proposalmask_featupsample_img512.yaml', resume=True, eval_only=False, num_gpus=4, num_machines=1, machine_rank=0, dist_url='tcp://0.0.0.0:12237', opts=['MODEL.CLIP_ADAPTER.CLIP_ENSEMBLE', 'False', 'MODEL.CLIP_ADAPTER.CLIP_ENSEMBLE_WEIGHT', '-1.0', 'SOLVER.TEST_IMS_PER_BATCH', '1', 'OUTPUT_DIR', 'deop/train_log', 'MODEL.CLIP_ADAPTER.PROMPT_LEARNER', 'learnable', 'SOLVER.IMS_PER_BATCH', '16', 'SOLVER.BASE_LR', '0.00005', 'SOLVER.CHECKPOINT_PERIOD', '1000', 'MODEL.CLIP_ADAPTER.CLIP_MODEL_NAME', 'ViT-B/16', 'MODEL.WEIGHTS', 'pretrained_models/deop_model_final.pth', 'MODEL.META_ARCHITECTURE', 'ZeroShotClipfeatUpsample_addnorm_vit16Query2D_maskvit', 'MODEL.NUM_DECODER_LAYER', '1', 'ORACLE', 'False', 'INPUT.MIN_SIZE_TEST', '512', 'TEST.EVAL_PERIOD', '1000', 'INPUT.SIZE_DIVISIBILITY', '512', 'MODEL.MASK_FORMER.DECODER_DICE_WEIGHT', '0.8', 'MODEL.MASK_FORMER.DECODER_CE_WEIGHT', '2.0', 'MODEL.CLIP_ADAPTER.LEARN_POSITION', 'True', 'MODEL.CLIP_ADAPTER.POSITION_LAYERS', '[1,2,3,4,5,6,7,8,9,10,11]', 'MODEL.CLIP_ADAPTER.LAYERMASKVIT', '[11]', 'MODEL.CLIP_ADAPTER.END2ENDTRAIN', 'False', 'MODEL.CLIP_ADAPTER.PS_SHORTCUT', '1.0', 'DATASETS.TRAIN', "('coco_2017_train_stuff_all_sem_seg',)", 'DATASETS.TEST', "('context_59_test_sem_seg',)", 'MODEL.SEM_SEG_HEAD.NUM_CLASSES', '171', 'MODEL.MASK_FORMER.LOSS_NUM_CLASS', '171'])
[01/13 16:29:58] detectron2 INFO: Contents of args.config_file=configs/coco-stuff-164k-156/zero_shot_maskformer_R101c_bs32_60k_proposalmask_featupsample_img512.yaml:
[38;5;197m_BASE_[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mzero_shot_maskformer_R50_bs32_60k.yaml[39m
[38;5;197mMODEL[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mSEM_SEG_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mNUM_CLASSES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m171[39m
[38;5;15m  [39m[38;5;197mMETA_ARCHITECTURE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mZeroShotMaskFormerClipfeatUpsample[39m[38;5;186m"[39m
[38;5;15m  [39m[38;5;197mBACKBONE[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mbuild_resnet_deeplab_backbone[39m[38;5;186m"[39m
[38;5;15m  [39m
[38;5;15m  [39m[38;5;197mWEIGHTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mdetectron2://DeepLab/R-103.pkl[39m[38;5;186m"[39m
[38;5;15m  [39m[38;5;197mRESNETS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mDEPTH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m101[39m
[38;5;15m    [39m[38;5;197mSTEM_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mdeeplab[39m[38;5;186m"[39m
[38;5;15m    [39m[38;5;197mSTEM_OUT_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m128[39m
[38;5;15m    [39m[38;5;197mSTRIDE_IN_1X1[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mFalse[39m
[38;5;15m    [39m[38;5;197mOUT_FEATURES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;186m"[39m[38;5;186mres2[39m[38;5;186m"[39m[38;5;15m,[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mres3[39m[38;5;186m"[39m[38;5;15m,[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mres4[39m[38;5;186m"[39m[38;5;15m,[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mres5[39m[38;5;186m"[39m[38;5;15m][39m
[38;5;15m    [39m[38;5;242m# NORM: "SyncBN"[39m
[38;5;15m    [39m[38;5;197mRES5_MULTI_GRID[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m1[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15m2[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15m4[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;197mCLIP_ADAPTER[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCLIP_ENSEMBLE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mTrue[39m
[38;5;15m    [39m[38;5;197mCLIP_ENSEMBLE_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.8[39m
[38;5;197mINPUT[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mMIN_SIZE_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;81m!!python/object/apply:eval[39m[38;5;15m [39m[38;5;15m[[39m[38;5;186m"[39m[38;5;186m[int(x[39m[38;5;15m [39m[38;5;186m*[39m[38;5;15m [39m[38;5;186m0.1[39m[38;5;15m [39m[38;5;186m*[39m[38;5;15m [39m[38;5;186m512)[39m[38;5;15m [39m[38;5;186mfor[39m[38;5;15m [39m[38;5;186mx[39m[38;5;15m [39m[38;5;186min[39m[38;5;15m [39m[38;5;186mrange(5,[39m[38;5;15m [39m[38;5;186m16)][39m[38;5;186m"[39m[38;5;15m][39m
[38;5;197mOUTPUT_DIR[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mdeop/train_log[39m[38;5;186m"[39m
[38;5;197mSOLVER[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mIMS_PER_BATCH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m8[39m
[38;5;15m  [39m[38;5;197mTEST_IMS_PER_BATCH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m8[39m
[38;5;197mTEST[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mEVAL_PERIOD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m5000[39m

[01/13 16:29:58] detectron2.utils.env INFO: Using a generated random seed 58293385
[01/13 16:29:58] mask_former.modeling.clip_adapter INFO: Prompt Learner training params: ['prefix_prompt']
[01/13 16:30:01] detectron2.engine.defaults INFO: Model:
ZeroShotClipfeatUpsample_addnorm_vit16Query2D_maskvit(
  (backbone): ResNet(
    (stem): DeepLabStem(
      (conv1): Conv2d(
        3, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
      )
      (conv3): Conv2d(
        64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
      )
    )
    (res2): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv1): Conv2d(
          128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
      )
    )
    (res3): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv1): Conv2d(
          256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
      (3): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
    )
    (res4): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
        (conv1): Conv2d(
          512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (3): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (4): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (5): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (6): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (7): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (8): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (9): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (10): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (11): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (12): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (13): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (14): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (15): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (16): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (17): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (18): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (19): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (20): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (21): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (22): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
    )
    (res5): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
        (conv1): Conv2d(
          1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
      )
    )
  )
  (sem_seg_head): ZeroShotMaskFormerHead(
    (pixel_decoder): BasePixelDecoder(
      (adapter_1): Conv2d(
        256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (layer_1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (adapter_2): Conv2d(
        512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (layer_2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (adapter_3): Conv2d(
        1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (layer_3): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (layer_4): Conv2d(
        2048, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (mask_features): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
    (predictor): ZeroShotTransformerPredictor(
      (pe_layer): PositionEmbeddingSine()
      (transformer): Transformer(
        (encoder): TransformerEncoder(
          (layers): ModuleList()
        )
        (decoder): TransformerDecoder(
          (layers): ModuleList(
            (0-5): 6 x TransformerDecoderLayer(
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (multihead_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (linear1): Linear(in_features=256, out_features=2048, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (linear2): Linear(in_features=2048, out_features=256, bias=True)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (dropout1): Dropout(p=0.1, inplace=False)
              (dropout2): Dropout(p=0.1, inplace=False)
              (dropout3): Dropout(p=0.1, inplace=False)
            )
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (query_embed): Embedding(100, 256)
      (input_proj): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
      (mask_embed): MLP(
        (layers): ModuleList(
          (0-2): 3 x Linear(in_features=256, out_features=256, bias=True)
        )
      )
      (class_embed): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=1024, bias=True)
          (1): Linear(in_features=1024, out_features=512, bias=True)
        )
      )
    )
  )
  (criterion): SetCriterion(
    (matcher): Matcher HungarianMatcher
        cost_class: 1
        cost_mask: 20.0
        cost_dice: 1.0
  )
  (clip_adapter): MaskFormerClipFeatureAdapter(
    (clip_model): CLIP(
      (visual): VisionTransformerLearnPosition(
        (conv1): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False)
        (ln_pre): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (transformer): TransformerVitPosition(
          (resblocks): Sequential(
            (0): ResidualAttentionBlockVitPosition(
              (attn): MultiheadAttentionLoRA(
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (1): ResidualAttentionBlockVitPosition(
              (attn): MultiheadAttentionLoRA(
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (2): ResidualAttentionBlockVitPosition(
              (attn): MultiheadAttentionLoRA(
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (3): ResidualAttentionBlockVitPosition(
              (attn): MultiheadAttentionLoRA(
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (4): ResidualAttentionBlockVitPosition(
              (attn): MultiheadAttentionLoRA(
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (5): ResidualAttentionBlockVitPosition(
              (attn): MultiheadAttentionLoRA(
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (6): ResidualAttentionBlockVitPosition(
              (attn): MultiheadAttentionLoRA(
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (7): ResidualAttentionBlockVitPosition(
              (attn): MultiheadAttentionLoRA(
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (8): ResidualAttentionBlockVitPosition(
              (attn): MultiheadAttentionLoRA(
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (9): ResidualAttentionBlockVitPosition(
              (attn): MultiheadAttentionLoRA(
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (10): ResidualAttentionBlockVitPosition(
              (attn): MultiheadAttentionLoRA(
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (11): ResidualAttentionBlockVitPosition(
              (attn): MultiheadAttentionVit(
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
        (ln_post): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (transformer): Transformer(
        (resblocks): Sequential(
          (0): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (1): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (2): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (3): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (4): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (5): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (6): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (7): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (8): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (9): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (10): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (11): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (token_embedding): Embedding(49408, 512)
      (ln_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (prompt_learner): LearnablePromptExtractor(
      prefix_prompt:16,suffix_prompt:0,dimension:512
      [Normal_Init(mu=0,std=0.02)]
    )
  )
  (decoder): TransformerDecoderLinear(
    (layers): ModuleList(
      (0): TransformerDecoderLayerLinear(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
        )
        (multihead_attn_my): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
        )
        (linear1): Linear(in_features=512, out_features=1024, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=1024, out_features=512, bias=True)
        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
    )
    (outlayer): TransformerDecoderOutLayerLinear(
      (self_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
      )
      (multihead_attn_my): MultiHeadAttentionMy(
        (dropout_F): Dropout(p=0.1, inplace=False)
      )
      (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (dropout1): Dropout(p=0.1, inplace=False)
      (dropout2): Dropout(p=0.1, inplace=False)
      (dropout3): Dropout(p=0.1, inplace=False)
    )
    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  )
  (pe_layer): PositionEmbeddingSine()
  (query_embedding): Embedding(100, 512)
)
[01/13 16:30:01] mask_former.data.dataset_mappers.mask_former_semantic_dataset_mapper INFO: [MaskFormerSemanticDatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=(256, 307, 358, 409, 460, 512, 563, 614, 665, 716, 768), max_size=2560, sample_style='choice'), RandomCrop_CategoryAreaConstraint(crop_type='absolute', crop_size=[640, 640], single_category_max_area=1.0, ignored_category=255), <detectron2.projects.point_rend.color_augmentation.ColorAugSSDTransform object at 0x7f093e05b0d0>, RandomFlip()]
[01/13 16:30:04] detectron2.data.datasets.coco INFO: Loaded 118287 images with semantic segmentation from /scratch/local/ssd/anjun/datasets/coco-stuff/images/train2017
[01/13 16:30:04] mask_former.data.build INFO: Using training sampler TrainingSampler
[01/13 16:30:04] detectron2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[01/13 16:30:04] detectron2.data.common INFO: Serializing 118287 elements to byte tensors and concatenating them all ...
[01/13 16:30:04] detectron2.data.common INFO: Serialized dataset takes 32.38 MiB
[01/13 16:30:04] detectron2.data.build INFO: Making batched data loader with batch_size=4
[01/13 16:30:05] detectron2.checkpoint.detection_checkpoint INFO: [DetectionCheckpointer] Loading from pretrained_models/deop_model_final.pth ...
[01/13 16:30:05] fvcore.common.checkpoint INFO: [Checkpointer] Loading from pretrained_models/deop_model_final.pth ...
[01/13 16:30:05] fvcore.common.checkpoint WARNING: Skip loading parameter 'criterion.empty_weight' to the model due to incompatible shapes: (157,) in the checkpoint but (172,) in the model! You might want to double check if this is expected.
[01/13 16:30:05] fvcore.common.checkpoint WARNING: Some model parameters or buffers are not found in the checkpoint:
[34mclip_adapter.clip_model.token_embedding.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.10.learned_position[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.11.attn.out_proj.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.11.learned_position[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.6.learned_position[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.7.learned_position[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.8.learned_position[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.9.learned_position[0m
[34mcriterion.empty_weight[0m
[01/13 16:30:05] detectron2.engine.train_loop INFO: Starting training from iteration 0
[01/13 16:30:22] detectron2.engine.train_loop ERROR: Exception during training:
Traceback (most recent call last):
  File "/homes/55/anjun/.local/lib/python3.10/site-packages/detectron2/engine/train_loop.py", line 155, in train
    self.run_step()
  File "/scratch/local/ssd/anjun/ovseg/DeOP/train_net.py", line 361, in run_step
    loss_dict = self.model(data)
  File "/scratch/local/ssd/anjun/anaconda3/envs/ldm/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/local/ssd/anjun/anaconda3/envs/ldm/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/local/ssd/anjun/anaconda3/envs/ldm/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1515, in forward
    inputs, kwargs = self._pre_forward(*inputs, **kwargs)
  File "/scratch/local/ssd/anjun/anaconda3/envs/ldm/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1409, in _pre_forward
    if torch.is_grad_enabled() and self.reducer._rebuild_buckets():
RuntimeError: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by 
making sure all `forward` function outputs participate in calculating loss. 
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameter indices which did not receive grad for rank 2: 13 14 15 16
 In addition, you can set the environment variable TORCH_DISTRIBUTED_DEBUG to either INFO or DETAIL to print out information about which particular parameters did not receive gradient on this rank as part of this error
[01/13 16:30:22] detectron2.engine.hooks INFO: Total training time: 0:00:00 (0:00:00 on hooks)
[01/13 16:39:24] detectron2 INFO: Rank of current process: 2. World size: 4
[01/13 16:39:28] detectron2 INFO: Environment info:
-------------------------------  --------------------------------------------------------------------------------------------------
sys.platform                     linux
Python                           3.10.11 (main, May 16 2023, 00:28:57) [GCC 11.2.0]
numpy                            1.26.2
detectron2                       0.6 @/homes/55/anjun/.local/lib/python3.10/site-packages/detectron2
Compiler                         GCC 8.3
CUDA compiler                    CUDA 12.1
detectron2 arch flags            8.6
DETECTRON2_ENV_MODULE            <not set>
PyTorch                          2.1.1+cu121 @/scratch/local/ssd/anjun/anaconda3/envs/ldm/lib/python3.10/site-packages/torch
PyTorch debug build              False
torch._C._GLIBCXX_USE_CXX11_ABI  False
GPU available                    Yes
GPU 0,1,2,3                      NVIDIA A40 (arch=8.6)
Driver version                   530.30.02
CUDA_HOME                        /usr/local/cuda-12.1/
Pillow                           9.5.0
torchvision                      0.16.1+cu121 @/scratch/local/ssd/anjun/anaconda3/envs/ldm/lib/python3.10/site-packages/torchvision
torchvision arch flags           5.0, 6.0, 7.0, 7.5, 8.0, 8.6, 9.0
fvcore                           0.1.5.post20221221
iopath                           0.1.9
cv2                              4.5.4-dev
-------------------------------  --------------------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v3.1.1 (Git Hash 64f6bcbcbab628e96f33a62c3e975f8535a7bde4)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 12.1
  - NVCC architecture flags: -gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.9.2
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=12.1, CUDNN_VERSION=8.9.2, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=old-style-cast -Wno-invalid-partial-specialization -Wno-unused-private-field -Wno-aligned-allocation-unavailable -Wno-missing-braces -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.1.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[01/13 16:39:28] detectron2 INFO: Command line arguments: Namespace(config_file='configs/coco-stuff-164k-156/zero_shot_maskformer_R101c_bs32_60k_proposalmask_featupsample_img512.yaml', resume=True, eval_only=False, num_gpus=4, num_machines=1, machine_rank=0, dist_url='tcp://0.0.0.0:12237', opts=['MODEL.CLIP_ADAPTER.CLIP_ENSEMBLE', 'False', 'MODEL.CLIP_ADAPTER.CLIP_ENSEMBLE_WEIGHT', '-1.0', 'SOLVER.TEST_IMS_PER_BATCH', '1', 'OUTPUT_DIR', 'deop/train_log', 'MODEL.CLIP_ADAPTER.PROMPT_LEARNER', 'learnable', 'SOLVER.IMS_PER_BATCH', '16', 'SOLVER.BASE_LR', '0.00005', 'SOLVER.CHECKPOINT_PERIOD', '1000', 'MODEL.CLIP_ADAPTER.CLIP_MODEL_NAME', 'ViT-B/16', 'MODEL.WEIGHTS', 'pretrained_models/deop_model_final.pth', 'MODEL.META_ARCHITECTURE', 'ZeroShotClipfeatUpsample_addnorm_vit16Query2D_maskvit', 'MODEL.NUM_DECODER_LAYER', '1', 'ORACLE', 'False', 'INPUT.MIN_SIZE_TEST', '512', 'TEST.EVAL_PERIOD', '1000', 'INPUT.SIZE_DIVISIBILITY', '512', 'MODEL.MASK_FORMER.DECODER_DICE_WEIGHT', '0.8', 'MODEL.MASK_FORMER.DECODER_CE_WEIGHT', '2.0', 'MODEL.CLIP_ADAPTER.LEARN_POSITION', 'True', 'MODEL.CLIP_ADAPTER.POSITION_LAYERS', '[1,2,3,4,5,6,7,8,9,10,11]', 'MODEL.CLIP_ADAPTER.LAYERMASKVIT', '[11]', 'MODEL.CLIP_ADAPTER.END2ENDTRAIN', 'False', 'MODEL.CLIP_ADAPTER.PS_SHORTCUT', '1.0', 'DATASETS.TRAIN', "('coco_2017_train_stuff_all_sem_seg',)", 'DATASETS.TEST', "('context_59_test_sem_seg',)", 'MODEL.SEM_SEG_HEAD.NUM_CLASSES', '171', 'MODEL.MASK_FORMER.LOSS_NUM_CLASS', '171'])
[01/13 16:39:28] detectron2 INFO: Contents of args.config_file=configs/coco-stuff-164k-156/zero_shot_maskformer_R101c_bs32_60k_proposalmask_featupsample_img512.yaml:
[38;5;197m_BASE_[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mzero_shot_maskformer_R50_bs32_60k.yaml[39m
[38;5;197mMODEL[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mSEM_SEG_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mNUM_CLASSES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m171[39m
[38;5;15m  [39m[38;5;197mMETA_ARCHITECTURE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mZeroShotMaskFormerClipfeatUpsample[39m[38;5;186m"[39m
[38;5;15m  [39m[38;5;197mBACKBONE[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mbuild_resnet_deeplab_backbone[39m[38;5;186m"[39m
[38;5;15m  [39m
[38;5;15m  [39m[38;5;197mWEIGHTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mdetectron2://DeepLab/R-103.pkl[39m[38;5;186m"[39m
[38;5;15m  [39m[38;5;197mRESNETS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mDEPTH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m101[39m
[38;5;15m    [39m[38;5;197mSTEM_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mdeeplab[39m[38;5;186m"[39m
[38;5;15m    [39m[38;5;197mSTEM_OUT_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m128[39m
[38;5;15m    [39m[38;5;197mSTRIDE_IN_1X1[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mFalse[39m
[38;5;15m    [39m[38;5;197mOUT_FEATURES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;186m"[39m[38;5;186mres2[39m[38;5;186m"[39m[38;5;15m,[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mres3[39m[38;5;186m"[39m[38;5;15m,[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mres4[39m[38;5;186m"[39m[38;5;15m,[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mres5[39m[38;5;186m"[39m[38;5;15m][39m
[38;5;15m    [39m[38;5;242m# NORM: "SyncBN"[39m
[38;5;15m    [39m[38;5;197mRES5_MULTI_GRID[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m1[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15m2[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15m4[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;197mCLIP_ADAPTER[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCLIP_ENSEMBLE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mTrue[39m
[38;5;15m    [39m[38;5;197mCLIP_ENSEMBLE_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.8[39m
[38;5;197mINPUT[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mMIN_SIZE_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;81m!!python/object/apply:eval[39m[38;5;15m [39m[38;5;15m[[39m[38;5;186m"[39m[38;5;186m[int(x[39m[38;5;15m [39m[38;5;186m*[39m[38;5;15m [39m[38;5;186m0.1[39m[38;5;15m [39m[38;5;186m*[39m[38;5;15m [39m[38;5;186m512)[39m[38;5;15m [39m[38;5;186mfor[39m[38;5;15m [39m[38;5;186mx[39m[38;5;15m [39m[38;5;186min[39m[38;5;15m [39m[38;5;186mrange(5,[39m[38;5;15m [39m[38;5;186m16)][39m[38;5;186m"[39m[38;5;15m][39m
[38;5;197mOUTPUT_DIR[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mdeop/train_log[39m[38;5;186m"[39m
[38;5;197mSOLVER[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mIMS_PER_BATCH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m8[39m
[38;5;15m  [39m[38;5;197mTEST_IMS_PER_BATCH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m8[39m
[38;5;197mTEST[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mEVAL_PERIOD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m5000[39m

[01/13 16:39:28] detectron2.utils.env INFO: Using a generated random seed 28547079
[01/13 16:39:29] mask_former.modeling.clip_adapter INFO: Prompt Learner training params: ['prefix_prompt']
[01/13 16:41:38] detectron2 INFO: Rank of current process: 2. World size: 4
[01/13 16:41:41] detectron2 INFO: Environment info:
-------------------------------  --------------------------------------------------------------------------------------------------
sys.platform                     linux
Python                           3.10.11 (main, May 16 2023, 00:28:57) [GCC 11.2.0]
numpy                            1.26.2
detectron2                       0.6 @/homes/55/anjun/.local/lib/python3.10/site-packages/detectron2
Compiler                         GCC 8.3
CUDA compiler                    CUDA 12.1
detectron2 arch flags            8.6
DETECTRON2_ENV_MODULE            <not set>
PyTorch                          2.1.1+cu121 @/scratch/local/ssd/anjun/anaconda3/envs/ldm/lib/python3.10/site-packages/torch
PyTorch debug build              False
torch._C._GLIBCXX_USE_CXX11_ABI  False
GPU available                    Yes
GPU 0,1,2,3                      NVIDIA A40 (arch=8.6)
Driver version                   530.30.02
CUDA_HOME                        /usr/local/cuda-12.1/
Pillow                           9.5.0
torchvision                      0.16.1+cu121 @/scratch/local/ssd/anjun/anaconda3/envs/ldm/lib/python3.10/site-packages/torchvision
torchvision arch flags           5.0, 6.0, 7.0, 7.5, 8.0, 8.6, 9.0
fvcore                           0.1.5.post20221221
iopath                           0.1.9
cv2                              4.5.4-dev
-------------------------------  --------------------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v3.1.1 (Git Hash 64f6bcbcbab628e96f33a62c3e975f8535a7bde4)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 12.1
  - NVCC architecture flags: -gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.9.2
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=12.1, CUDNN_VERSION=8.9.2, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=old-style-cast -Wno-invalid-partial-specialization -Wno-unused-private-field -Wno-aligned-allocation-unavailable -Wno-missing-braces -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.1.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[01/13 16:41:41] detectron2 INFO: Command line arguments: Namespace(config_file='configs/coco-stuff-164k-156/zero_shot_maskformer_R101c_bs32_60k_proposalmask_featupsample_img512.yaml', resume=True, eval_only=False, num_gpus=4, num_machines=1, machine_rank=0, dist_url='tcp://0.0.0.0:12237', opts=['MODEL.CLIP_ADAPTER.CLIP_ENSEMBLE', 'False', 'MODEL.CLIP_ADAPTER.CLIP_ENSEMBLE_WEIGHT', '-1.0', 'SOLVER.TEST_IMS_PER_BATCH', '1', 'OUTPUT_DIR', 'deop/train_log', 'MODEL.CLIP_ADAPTER.PROMPT_LEARNER', 'learnable', 'SOLVER.IMS_PER_BATCH', '16', 'SOLVER.BASE_LR', '0.00005', 'SOLVER.CHECKPOINT_PERIOD', '1000', 'MODEL.CLIP_ADAPTER.CLIP_MODEL_NAME', 'ViT-B/16', 'MODEL.WEIGHTS', 'pretrained_models/deop_model_final.pth', 'MODEL.META_ARCHITECTURE', 'ZeroShotClipfeatUpsample_addnorm_vit16Query2D_maskvit', 'MODEL.NUM_DECODER_LAYER', '1', 'ORACLE', 'False', 'INPUT.MIN_SIZE_TEST', '512', 'TEST.EVAL_PERIOD', '1000', 'INPUT.SIZE_DIVISIBILITY', '512', 'MODEL.MASK_FORMER.DECODER_DICE_WEIGHT', '0.8', 'MODEL.MASK_FORMER.DECODER_CE_WEIGHT', '2.0', 'MODEL.CLIP_ADAPTER.LEARN_POSITION', 'True', 'MODEL.CLIP_ADAPTER.POSITION_LAYERS', '[1,2,3,4,5,6,7,8,9,10,11]', 'MODEL.CLIP_ADAPTER.LAYERMASKVIT', '[11]', 'MODEL.CLIP_ADAPTER.END2ENDTRAIN', 'False', 'MODEL.CLIP_ADAPTER.PS_SHORTCUT', '1.0', 'DATASETS.TRAIN', "('coco_2017_train_stuff_all_sem_seg',)", 'DATASETS.TEST', "('context_59_test_sem_seg',)", 'MODEL.SEM_SEG_HEAD.NUM_CLASSES', '171', 'MODEL.MASK_FORMER.LOSS_NUM_CLASS', '171'])
[01/13 16:41:41] detectron2 INFO: Contents of args.config_file=configs/coco-stuff-164k-156/zero_shot_maskformer_R101c_bs32_60k_proposalmask_featupsample_img512.yaml:
[38;5;197m_BASE_[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mzero_shot_maskformer_R50_bs32_60k.yaml[39m
[38;5;197mMODEL[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mSEM_SEG_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mNUM_CLASSES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m171[39m
[38;5;15m  [39m[38;5;197mMETA_ARCHITECTURE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mZeroShotMaskFormerClipfeatUpsample[39m[38;5;186m"[39m
[38;5;15m  [39m[38;5;197mBACKBONE[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mbuild_resnet_deeplab_backbone[39m[38;5;186m"[39m
[38;5;15m  [39m
[38;5;15m  [39m[38;5;197mWEIGHTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mdetectron2://DeepLab/R-103.pkl[39m[38;5;186m"[39m
[38;5;15m  [39m[38;5;197mRESNETS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mDEPTH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m101[39m
[38;5;15m    [39m[38;5;197mSTEM_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mdeeplab[39m[38;5;186m"[39m
[38;5;15m    [39m[38;5;197mSTEM_OUT_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m128[39m
[38;5;15m    [39m[38;5;197mSTRIDE_IN_1X1[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mFalse[39m
[38;5;15m    [39m[38;5;197mOUT_FEATURES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;186m"[39m[38;5;186mres2[39m[38;5;186m"[39m[38;5;15m,[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mres3[39m[38;5;186m"[39m[38;5;15m,[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mres4[39m[38;5;186m"[39m[38;5;15m,[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mres5[39m[38;5;186m"[39m[38;5;15m][39m
[38;5;15m    [39m[38;5;242m# NORM: "SyncBN"[39m
[38;5;15m    [39m[38;5;197mRES5_MULTI_GRID[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m1[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15m2[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15m4[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;197mCLIP_ADAPTER[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCLIP_ENSEMBLE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mTrue[39m
[38;5;15m    [39m[38;5;197mCLIP_ENSEMBLE_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.8[39m
[38;5;197mINPUT[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mMIN_SIZE_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;81m!!python/object/apply:eval[39m[38;5;15m [39m[38;5;15m[[39m[38;5;186m"[39m[38;5;186m[int(x[39m[38;5;15m [39m[38;5;186m*[39m[38;5;15m [39m[38;5;186m0.1[39m[38;5;15m [39m[38;5;186m*[39m[38;5;15m [39m[38;5;186m512)[39m[38;5;15m [39m[38;5;186mfor[39m[38;5;15m [39m[38;5;186mx[39m[38;5;15m [39m[38;5;186min[39m[38;5;15m [39m[38;5;186mrange(5,[39m[38;5;15m [39m[38;5;186m16)][39m[38;5;186m"[39m[38;5;15m][39m
[38;5;197mOUTPUT_DIR[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mdeop/train_log[39m[38;5;186m"[39m
[38;5;197mSOLVER[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mIMS_PER_BATCH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m8[39m
[38;5;15m  [39m[38;5;197mTEST_IMS_PER_BATCH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m8[39m
[38;5;197mTEST[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mEVAL_PERIOD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m5000[39m

[01/13 16:41:41] detectron2.utils.env INFO: Using a generated random seed 41274490
[01/13 16:41:41] mask_former.modeling.clip_adapter INFO: Prompt Learner training params: ['prefix_prompt']
[01/13 16:41:50] detectron2.engine.defaults INFO: Model:
ZeroShotClipfeatUpsample_addnorm_vit16Query2D_maskvit(
  (backbone): ResNet(
    (stem): DeepLabStem(
      (conv1): Conv2d(
        3, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
      )
      (conv3): Conv2d(
        64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
      )
    )
    (res2): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv1): Conv2d(
          128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
      )
    )
    (res3): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv1): Conv2d(
          256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
      (3): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
    )
    (res4): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
        (conv1): Conv2d(
          512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (3): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (4): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (5): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (6): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (7): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (8): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (9): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (10): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (11): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (12): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (13): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (14): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (15): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (16): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (17): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (18): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (19): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (20): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (21): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (22): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
    )
    (res5): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
        (conv1): Conv2d(
          1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
      )
    )
  )
  (sem_seg_head): ZeroShotMaskFormerHead(
    (pixel_decoder): BasePixelDecoder(
      (adapter_1): Conv2d(
        256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (layer_1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (adapter_2): Conv2d(
        512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (layer_2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (adapter_3): Conv2d(
        1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (layer_3): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (layer_4): Conv2d(
        2048, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (mask_features): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
    (predictor): ZeroShotTransformerPredictor(
      (pe_layer): PositionEmbeddingSine()
      (transformer): Transformer(
        (encoder): TransformerEncoder(
          (layers): ModuleList()
        )
        (decoder): TransformerDecoder(
          (layers): ModuleList(
            (0-5): 6 x TransformerDecoderLayer(
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (multihead_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (linear1): Linear(in_features=256, out_features=2048, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (linear2): Linear(in_features=2048, out_features=256, bias=True)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (dropout1): Dropout(p=0.1, inplace=False)
              (dropout2): Dropout(p=0.1, inplace=False)
              (dropout3): Dropout(p=0.1, inplace=False)
            )
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (query_embed): Embedding(100, 256)
      (input_proj): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
      (mask_embed): MLP(
        (layers): ModuleList(
          (0-2): 3 x Linear(in_features=256, out_features=256, bias=True)
        )
      )
      (class_embed): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=1024, bias=True)
          (1): Linear(in_features=1024, out_features=512, bias=True)
        )
      )
    )
  )
  (criterion): SetCriterion(
    (matcher): Matcher HungarianMatcher
        cost_class: 1
        cost_mask: 20.0
        cost_dice: 1.0
  )
  (clip_adapter): MaskFormerClipFeatureAdapter(
    (clip_model): CLIP(
      (visual): VisionTransformerLearnPosition(
        (conv1): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False)
        (ln_pre): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (transformer): TransformerVitPosition(
          (resblocks): Sequential(
            (0): ResidualAttentionBlockVitPosition(
              (attn): MultiheadAttentionLoRA(
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (1): ResidualAttentionBlockVitPosition(
              (attn): MultiheadAttentionLoRA(
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (2): ResidualAttentionBlockVitPosition(
              (attn): MultiheadAttentionLoRA(
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (3): ResidualAttentionBlockVitPosition(
              (attn): MultiheadAttentionLoRA(
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (4): ResidualAttentionBlockVitPosition(
              (attn): MultiheadAttentionLoRA(
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (5): ResidualAttentionBlockVitPosition(
              (attn): MultiheadAttentionLoRA(
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (6): ResidualAttentionBlockVitPosition(
              (attn): MultiheadAttentionLoRA(
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (7): ResidualAttentionBlockVitPosition(
              (attn): MultiheadAttentionLoRA(
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (8): ResidualAttentionBlockVitPosition(
              (attn): MultiheadAttentionLoRA(
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (9): ResidualAttentionBlockVitPosition(
              (attn): MultiheadAttentionLoRA(
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (10): ResidualAttentionBlockVitPosition(
              (attn): MultiheadAttentionLoRA(
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (11): ResidualAttentionBlockVitPosition(
              (attn): MultiheadAttentionVit(
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
        (ln_post): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (transformer): Transformer(
        (resblocks): Sequential(
          (0): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (1): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (2): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (3): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (4): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (5): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (6): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (7): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (8): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (9): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (10): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (11): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (token_embedding): Embedding(49408, 512)
      (ln_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (prompt_learner): LearnablePromptExtractor(
      prefix_prompt:16,suffix_prompt:0,dimension:512
      [Normal_Init(mu=0,std=0.02)]
    )
  )
  (decoder): TransformerDecoderLinear(
    (layers): ModuleList(
      (0): TransformerDecoderLayerLinear(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
        )
        (multihead_attn_my): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
        )
        (linear1): Linear(in_features=512, out_features=1024, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=1024, out_features=512, bias=True)
        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
    )
    (outlayer): TransformerDecoderOutLayerLinear(
      (self_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
      )
      (multihead_attn_my): MultiHeadAttentionMy(
        (dropout_F): Dropout(p=0.1, inplace=False)
      )
      (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (dropout1): Dropout(p=0.1, inplace=False)
      (dropout2): Dropout(p=0.1, inplace=False)
      (dropout3): Dropout(p=0.1, inplace=False)
    )
    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  )
  (pe_layer): PositionEmbeddingSine()
  (query_embedding): Embedding(100, 512)
)
[01/13 16:41:50] mask_former.data.dataset_mappers.mask_former_semantic_dataset_mapper INFO: [MaskFormerSemanticDatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=(256, 307, 358, 409, 460, 512, 563, 614, 665, 716, 768), max_size=2560, sample_style='choice'), RandomCrop_CategoryAreaConstraint(crop_type='absolute', crop_size=[640, 640], single_category_max_area=1.0, ignored_category=255), <detectron2.projects.point_rend.color_augmentation.ColorAugSSDTransform object at 0x7f4ad812f0d0>, RandomFlip()]
[01/13 16:41:52] detectron2.data.datasets.coco INFO: Loaded 118287 images with semantic segmentation from /scratch/local/ssd/anjun/datasets/coco-stuff/images/train2017
[01/13 16:41:52] mask_former.data.build INFO: Using training sampler TrainingSampler
[01/13 16:41:52] detectron2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[01/13 16:41:52] detectron2.data.common INFO: Serializing 118287 elements to byte tensors and concatenating them all ...
[01/13 16:41:53] detectron2.data.common INFO: Serialized dataset takes 32.38 MiB
[01/13 16:41:53] detectron2.data.build INFO: Making batched data loader with batch_size=4
[01/13 16:41:54] detectron2.checkpoint.detection_checkpoint INFO: [DetectionCheckpointer] Loading from pretrained_models/deop_model_final.pth ...
[01/13 16:41:54] fvcore.common.checkpoint INFO: [Checkpointer] Loading from pretrained_models/deop_model_final.pth ...
[01/13 16:41:54] fvcore.common.checkpoint WARNING: Skip loading parameter 'criterion.empty_weight' to the model due to incompatible shapes: (157,) in the checkpoint but (172,) in the model! You might want to double check if this is expected.
[01/13 16:41:54] fvcore.common.checkpoint WARNING: Some model parameters or buffers are not found in the checkpoint:
[34mclip_adapter.clip_model.token_embedding.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.0.attn.out_proj.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.0.mlp.c_fc.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.0.mlp.c_proj.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.1.attn.out_proj.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.1.mlp.c_fc.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.1.mlp.c_proj.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.10.attn.out_proj.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.10.learned_position[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.10.mlp.c_fc.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.10.mlp.c_proj.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.11.attn.out_proj.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.11.learned_position[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.11.mlp.c_fc.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.11.mlp.c_proj.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.2.attn.out_proj.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.2.mlp.c_fc.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.2.mlp.c_proj.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.3.attn.out_proj.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.3.mlp.c_fc.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.3.mlp.c_proj.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.4.attn.out_proj.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.4.mlp.c_fc.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.4.mlp.c_proj.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.5.attn.out_proj.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.5.mlp.c_fc.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.5.mlp.c_proj.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.6.attn.out_proj.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.6.learned_position[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.6.mlp.c_fc.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.6.mlp.c_proj.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.7.attn.out_proj.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.7.learned_position[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.7.mlp.c_fc.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.7.mlp.c_proj.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.8.attn.out_proj.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.8.learned_position[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.8.mlp.c_fc.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.8.mlp.c_proj.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.9.attn.out_proj.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.9.learned_position[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.9.mlp.c_fc.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.9.mlp.c_proj.{lora_A, lora_B}[0m
[34mcriterion.empty_weight[0m
[01/13 16:41:54] detectron2.engine.train_loop INFO: Starting training from iteration 0
[01/13 16:42:11] detectron2.engine.train_loop ERROR: Exception during training:
Traceback (most recent call last):
  File "/homes/55/anjun/.local/lib/python3.10/site-packages/detectron2/engine/train_loop.py", line 155, in train
    self.run_step()
  File "/scratch/local/ssd/anjun/ovseg/DeOP/train_net.py", line 361, in run_step
    loss_dict = self.model(data)
  File "/scratch/local/ssd/anjun/anaconda3/envs/ldm/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/local/ssd/anjun/anaconda3/envs/ldm/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/local/ssd/anjun/anaconda3/envs/ldm/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1515, in forward
    inputs, kwargs = self._pre_forward(*inputs, **kwargs)
  File "/scratch/local/ssd/anjun/anaconda3/envs/ldm/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1409, in _pre_forward
    if torch.is_grad_enabled() and self.reducer._rebuild_buckets():
RuntimeError: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by 
making sure all `forward` function outputs participate in calculating loss. 
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameter indices which did not receive grad for rank 2: 2 3 9 10 16 17 23 24 30 31 37 38 44 45 51 52 58 59 65 66 72 73 79 80 85 86
 In addition, you can set the environment variable TORCH_DISTRIBUTED_DEBUG to either INFO or DETAIL to print out information about which particular parameters did not receive gradient on this rank as part of this error
[01/13 16:42:11] detectron2.engine.hooks INFO: Total training time: 0:00:00 (0:00:00 on hooks)
[01/13 16:46:42] detectron2 INFO: Rank of current process: 2. World size: 4
[01/13 16:46:45] detectron2 INFO: Environment info:
-------------------------------  --------------------------------------------------------------------------------------------------
sys.platform                     linux
Python                           3.10.11 (main, May 16 2023, 00:28:57) [GCC 11.2.0]
numpy                            1.26.2
detectron2                       0.6 @/homes/55/anjun/.local/lib/python3.10/site-packages/detectron2
Compiler                         GCC 8.3
CUDA compiler                    CUDA 12.1
detectron2 arch flags            8.6
DETECTRON2_ENV_MODULE            <not set>
PyTorch                          2.1.1+cu121 @/scratch/local/ssd/anjun/anaconda3/envs/ldm/lib/python3.10/site-packages/torch
PyTorch debug build              False
torch._C._GLIBCXX_USE_CXX11_ABI  False
GPU available                    Yes
GPU 0,1,2,3                      NVIDIA A40 (arch=8.6)
Driver version                   530.30.02
CUDA_HOME                        /usr/local/cuda-12.1/
Pillow                           9.5.0
torchvision                      0.16.1+cu121 @/scratch/local/ssd/anjun/anaconda3/envs/ldm/lib/python3.10/site-packages/torchvision
torchvision arch flags           5.0, 6.0, 7.0, 7.5, 8.0, 8.6, 9.0
fvcore                           0.1.5.post20221221
iopath                           0.1.9
cv2                              4.5.4-dev
-------------------------------  --------------------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v3.1.1 (Git Hash 64f6bcbcbab628e96f33a62c3e975f8535a7bde4)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 12.1
  - NVCC architecture flags: -gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.9.2
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=12.1, CUDNN_VERSION=8.9.2, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=old-style-cast -Wno-invalid-partial-specialization -Wno-unused-private-field -Wno-aligned-allocation-unavailable -Wno-missing-braces -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.1.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[01/13 16:46:45] detectron2 INFO: Command line arguments: Namespace(config_file='configs/coco-stuff-164k-156/zero_shot_maskformer_R101c_bs32_60k_proposalmask_featupsample_img512.yaml', resume=True, eval_only=False, num_gpus=4, num_machines=1, machine_rank=0, dist_url='tcp://0.0.0.0:12237', opts=['MODEL.CLIP_ADAPTER.CLIP_ENSEMBLE', 'False', 'MODEL.CLIP_ADAPTER.CLIP_ENSEMBLE_WEIGHT', '-1.0', 'SOLVER.TEST_IMS_PER_BATCH', '1', 'OUTPUT_DIR', 'deop/train_log', 'MODEL.CLIP_ADAPTER.PROMPT_LEARNER', 'learnable', 'SOLVER.IMS_PER_BATCH', '16', 'SOLVER.BASE_LR', '0.00005', 'SOLVER.CHECKPOINT_PERIOD', '1000', 'MODEL.CLIP_ADAPTER.CLIP_MODEL_NAME', 'ViT-B/16', 'MODEL.WEIGHTS', 'pretrained_models/deop_model_final.pth', 'MODEL.META_ARCHITECTURE', 'ZeroShotClipfeatUpsample_addnorm_vit16Query2D_maskvit', 'MODEL.NUM_DECODER_LAYER', '1', 'ORACLE', 'False', 'INPUT.MIN_SIZE_TEST', '512', 'TEST.EVAL_PERIOD', '1000', 'INPUT.SIZE_DIVISIBILITY', '512', 'MODEL.MASK_FORMER.DECODER_DICE_WEIGHT', '0.8', 'MODEL.MASK_FORMER.DECODER_CE_WEIGHT', '2.0', 'MODEL.CLIP_ADAPTER.LEARN_POSITION', 'True', 'MODEL.CLIP_ADAPTER.POSITION_LAYERS', '[1,2,3,4,5,6,7,8,9,10,11]', 'MODEL.CLIP_ADAPTER.LAYERMASKVIT', '[11]', 'MODEL.CLIP_ADAPTER.END2ENDTRAIN', 'False', 'MODEL.CLIP_ADAPTER.PS_SHORTCUT', '1.0', 'DATASETS.TRAIN', "('coco_2017_train_stuff_all_sem_seg',)", 'DATASETS.TEST', "('context_59_test_sem_seg',)", 'MODEL.SEM_SEG_HEAD.NUM_CLASSES', '171', 'MODEL.MASK_FORMER.LOSS_NUM_CLASS', '171'])
[01/13 16:46:45] detectron2 INFO: Contents of args.config_file=configs/coco-stuff-164k-156/zero_shot_maskformer_R101c_bs32_60k_proposalmask_featupsample_img512.yaml:
[38;5;197m_BASE_[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mzero_shot_maskformer_R50_bs32_60k.yaml[39m
[38;5;197mMODEL[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mSEM_SEG_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mNUM_CLASSES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m171[39m
[38;5;15m  [39m[38;5;197mMETA_ARCHITECTURE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mZeroShotMaskFormerClipfeatUpsample[39m[38;5;186m"[39m
[38;5;15m  [39m[38;5;197mBACKBONE[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mbuild_resnet_deeplab_backbone[39m[38;5;186m"[39m
[38;5;15m  [39m
[38;5;15m  [39m[38;5;197mWEIGHTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mdetectron2://DeepLab/R-103.pkl[39m[38;5;186m"[39m
[38;5;15m  [39m[38;5;197mRESNETS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mDEPTH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m101[39m
[38;5;15m    [39m[38;5;197mSTEM_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mdeeplab[39m[38;5;186m"[39m
[38;5;15m    [39m[38;5;197mSTEM_OUT_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m128[39m
[38;5;15m    [39m[38;5;197mSTRIDE_IN_1X1[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mFalse[39m
[38;5;15m    [39m[38;5;197mOUT_FEATURES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;186m"[39m[38;5;186mres2[39m[38;5;186m"[39m[38;5;15m,[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mres3[39m[38;5;186m"[39m[38;5;15m,[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mres4[39m[38;5;186m"[39m[38;5;15m,[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mres5[39m[38;5;186m"[39m[38;5;15m][39m
[38;5;15m    [39m[38;5;242m# NORM: "SyncBN"[39m
[38;5;15m    [39m[38;5;197mRES5_MULTI_GRID[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m1[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15m2[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15m4[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;197mCLIP_ADAPTER[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCLIP_ENSEMBLE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mTrue[39m
[38;5;15m    [39m[38;5;197mCLIP_ENSEMBLE_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.8[39m
[38;5;197mINPUT[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mMIN_SIZE_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;81m!!python/object/apply:eval[39m[38;5;15m [39m[38;5;15m[[39m[38;5;186m"[39m[38;5;186m[int(x[39m[38;5;15m [39m[38;5;186m*[39m[38;5;15m [39m[38;5;186m0.1[39m[38;5;15m [39m[38;5;186m*[39m[38;5;15m [39m[38;5;186m512)[39m[38;5;15m [39m[38;5;186mfor[39m[38;5;15m [39m[38;5;186mx[39m[38;5;15m [39m[38;5;186min[39m[38;5;15m [39m[38;5;186mrange(5,[39m[38;5;15m [39m[38;5;186m16)][39m[38;5;186m"[39m[38;5;15m][39m
[38;5;197mOUTPUT_DIR[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mdeop/train_log[39m[38;5;186m"[39m
[38;5;197mSOLVER[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mIMS_PER_BATCH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m8[39m
[38;5;15m  [39m[38;5;197mTEST_IMS_PER_BATCH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m8[39m
[38;5;197mTEST[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mEVAL_PERIOD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m5000[39m

[01/13 16:46:45] detectron2.utils.env INFO: Using a generated random seed 45167607
[01/13 16:46:45] mask_former.modeling.clip_adapter INFO: Prompt Learner training params: ['prefix_prompt']
[01/13 16:47:27] detectron2 INFO: Rank of current process: 2. World size: 4
[01/13 16:47:29] detectron2 INFO: Environment info:
-------------------------------  --------------------------------------------------------------------------------------------------
sys.platform                     linux
Python                           3.10.11 (main, May 16 2023, 00:28:57) [GCC 11.2.0]
numpy                            1.26.2
detectron2                       0.6 @/homes/55/anjun/.local/lib/python3.10/site-packages/detectron2
Compiler                         GCC 8.3
CUDA compiler                    CUDA 12.1
detectron2 arch flags            8.6
DETECTRON2_ENV_MODULE            <not set>
PyTorch                          2.1.1+cu121 @/scratch/local/ssd/anjun/anaconda3/envs/ldm/lib/python3.10/site-packages/torch
PyTorch debug build              False
torch._C._GLIBCXX_USE_CXX11_ABI  False
GPU available                    Yes
GPU 0,1,2,3                      NVIDIA A40 (arch=8.6)
Driver version                   530.30.02
CUDA_HOME                        /usr/local/cuda-12.1/
Pillow                           9.5.0
torchvision                      0.16.1+cu121 @/scratch/local/ssd/anjun/anaconda3/envs/ldm/lib/python3.10/site-packages/torchvision
torchvision arch flags           5.0, 6.0, 7.0, 7.5, 8.0, 8.6, 9.0
fvcore                           0.1.5.post20221221
iopath                           0.1.9
cv2                              4.5.4-dev
-------------------------------  --------------------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v3.1.1 (Git Hash 64f6bcbcbab628e96f33a62c3e975f8535a7bde4)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 12.1
  - NVCC architecture flags: -gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.9.2
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=12.1, CUDNN_VERSION=8.9.2, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=old-style-cast -Wno-invalid-partial-specialization -Wno-unused-private-field -Wno-aligned-allocation-unavailable -Wno-missing-braces -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.1.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[01/13 16:47:29] detectron2 INFO: Command line arguments: Namespace(config_file='configs/coco-stuff-164k-156/zero_shot_maskformer_R101c_bs32_60k_proposalmask_featupsample_img512.yaml', resume=True, eval_only=False, num_gpus=4, num_machines=1, machine_rank=0, dist_url='tcp://0.0.0.0:12237', opts=['MODEL.CLIP_ADAPTER.CLIP_ENSEMBLE', 'False', 'MODEL.CLIP_ADAPTER.CLIP_ENSEMBLE_WEIGHT', '-1.0', 'SOLVER.TEST_IMS_PER_BATCH', '1', 'OUTPUT_DIR', 'deop/train_log', 'MODEL.CLIP_ADAPTER.PROMPT_LEARNER', 'learnable', 'SOLVER.IMS_PER_BATCH', '16', 'SOLVER.BASE_LR', '0.00005', 'SOLVER.CHECKPOINT_PERIOD', '1000', 'MODEL.CLIP_ADAPTER.CLIP_MODEL_NAME', 'ViT-B/16', 'MODEL.WEIGHTS', 'pretrained_models/deop_model_final.pth', 'MODEL.META_ARCHITECTURE', 'ZeroShotClipfeatUpsample_addnorm_vit16Query2D_maskvit', 'MODEL.NUM_DECODER_LAYER', '1', 'ORACLE', 'False', 'INPUT.MIN_SIZE_TEST', '512', 'TEST.EVAL_PERIOD', '1000', 'INPUT.SIZE_DIVISIBILITY', '512', 'MODEL.MASK_FORMER.DECODER_DICE_WEIGHT', '0.8', 'MODEL.MASK_FORMER.DECODER_CE_WEIGHT', '2.0', 'MODEL.CLIP_ADAPTER.LEARN_POSITION', 'True', 'MODEL.CLIP_ADAPTER.POSITION_LAYERS', '[1,2,3,4,5,6,7,8,9,10,11]', 'MODEL.CLIP_ADAPTER.LAYERMASKVIT', '[11]', 'MODEL.CLIP_ADAPTER.END2ENDTRAIN', 'False', 'MODEL.CLIP_ADAPTER.PS_SHORTCUT', '1.0', 'DATASETS.TRAIN', "('coco_2017_train_stuff_all_sem_seg',)", 'DATASETS.TEST', "('context_59_test_sem_seg',)", 'MODEL.SEM_SEG_HEAD.NUM_CLASSES', '171', 'MODEL.MASK_FORMER.LOSS_NUM_CLASS', '171'])
[01/13 16:47:29] detectron2 INFO: Contents of args.config_file=configs/coco-stuff-164k-156/zero_shot_maskformer_R101c_bs32_60k_proposalmask_featupsample_img512.yaml:
[38;5;197m_BASE_[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mzero_shot_maskformer_R50_bs32_60k.yaml[39m
[38;5;197mMODEL[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mSEM_SEG_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mNUM_CLASSES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m171[39m
[38;5;15m  [39m[38;5;197mMETA_ARCHITECTURE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mZeroShotMaskFormerClipfeatUpsample[39m[38;5;186m"[39m
[38;5;15m  [39m[38;5;197mBACKBONE[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mbuild_resnet_deeplab_backbone[39m[38;5;186m"[39m
[38;5;15m  [39m
[38;5;15m  [39m[38;5;197mWEIGHTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mdetectron2://DeepLab/R-103.pkl[39m[38;5;186m"[39m
[38;5;15m  [39m[38;5;197mRESNETS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mDEPTH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m101[39m
[38;5;15m    [39m[38;5;197mSTEM_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mdeeplab[39m[38;5;186m"[39m
[38;5;15m    [39m[38;5;197mSTEM_OUT_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m128[39m
[38;5;15m    [39m[38;5;197mSTRIDE_IN_1X1[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mFalse[39m
[38;5;15m    [39m[38;5;197mOUT_FEATURES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;186m"[39m[38;5;186mres2[39m[38;5;186m"[39m[38;5;15m,[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mres3[39m[38;5;186m"[39m[38;5;15m,[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mres4[39m[38;5;186m"[39m[38;5;15m,[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mres5[39m[38;5;186m"[39m[38;5;15m][39m
[38;5;15m    [39m[38;5;242m# NORM: "SyncBN"[39m
[38;5;15m    [39m[38;5;197mRES5_MULTI_GRID[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m1[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15m2[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15m4[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;197mCLIP_ADAPTER[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCLIP_ENSEMBLE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mTrue[39m
[38;5;15m    [39m[38;5;197mCLIP_ENSEMBLE_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.8[39m
[38;5;197mINPUT[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mMIN_SIZE_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;81m!!python/object/apply:eval[39m[38;5;15m [39m[38;5;15m[[39m[38;5;186m"[39m[38;5;186m[int(x[39m[38;5;15m [39m[38;5;186m*[39m[38;5;15m [39m[38;5;186m0.1[39m[38;5;15m [39m[38;5;186m*[39m[38;5;15m [39m[38;5;186m512)[39m[38;5;15m [39m[38;5;186mfor[39m[38;5;15m [39m[38;5;186mx[39m[38;5;15m [39m[38;5;186min[39m[38;5;15m [39m[38;5;186mrange(5,[39m[38;5;15m [39m[38;5;186m16)][39m[38;5;186m"[39m[38;5;15m][39m
[38;5;197mOUTPUT_DIR[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mdeop/train_log[39m[38;5;186m"[39m
[38;5;197mSOLVER[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mIMS_PER_BATCH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m8[39m
[38;5;15m  [39m[38;5;197mTEST_IMS_PER_BATCH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m8[39m
[38;5;197mTEST[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mEVAL_PERIOD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m5000[39m

[01/13 16:47:29] detectron2.utils.env INFO: Using a generated random seed 29492201
[01/13 16:47:30] mask_former.modeling.clip_adapter INFO: Prompt Learner training params: ['prefix_prompt']
[01/13 16:47:38] detectron2.engine.defaults INFO: Model:
ZeroShotClipfeatUpsample_addnorm_vit16Query2D_maskvit(
  (backbone): ResNet(
    (stem): DeepLabStem(
      (conv1): Conv2d(
        3, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
      )
      (conv3): Conv2d(
        64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
      )
    )
    (res2): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv1): Conv2d(
          128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
      )
    )
    (res3): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv1): Conv2d(
          256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
      (3): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
    )
    (res4): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
        (conv1): Conv2d(
          512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (3): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (4): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (5): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (6): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (7): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (8): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (9): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (10): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (11): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (12): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (13): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (14): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (15): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (16): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (17): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (18): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (19): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (20): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (21): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (22): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
    )
    (res5): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
        (conv1): Conv2d(
          1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
      )
    )
  )
  (sem_seg_head): ZeroShotMaskFormerHead(
    (pixel_decoder): BasePixelDecoder(
      (adapter_1): Conv2d(
        256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (layer_1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (adapter_2): Conv2d(
        512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (layer_2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (adapter_3): Conv2d(
        1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (layer_3): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (layer_4): Conv2d(
        2048, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (mask_features): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
    (predictor): ZeroShotTransformerPredictor(
      (pe_layer): PositionEmbeddingSine()
      (transformer): Transformer(
        (encoder): TransformerEncoder(
          (layers): ModuleList()
        )
        (decoder): TransformerDecoder(
          (layers): ModuleList(
            (0-5): 6 x TransformerDecoderLayer(
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (multihead_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (linear1): Linear(in_features=256, out_features=2048, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (linear2): Linear(in_features=2048, out_features=256, bias=True)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (dropout1): Dropout(p=0.1, inplace=False)
              (dropout2): Dropout(p=0.1, inplace=False)
              (dropout3): Dropout(p=0.1, inplace=False)
            )
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (query_embed): Embedding(100, 256)
      (input_proj): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
      (mask_embed): MLP(
        (layers): ModuleList(
          (0-2): 3 x Linear(in_features=256, out_features=256, bias=True)
        )
      )
      (class_embed): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=1024, bias=True)
          (1): Linear(in_features=1024, out_features=512, bias=True)
        )
      )
    )
  )
  (criterion): SetCriterion(
    (matcher): Matcher HungarianMatcher
        cost_class: 1
        cost_mask: 20.0
        cost_dice: 1.0
  )
  (clip_adapter): MaskFormerClipFeatureAdapter(
    (clip_model): CLIP(
      (visual): VisionTransformerLearnPosition(
        (conv1): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False)
        (ln_pre): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (transformer): TransformerVitPosition(
          (resblocks): Sequential(
            (0): ResidualAttentionBlockVitPosition(
              (attn): MultiheadAttentionLoRA(
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (1): ResidualAttentionBlockVitPosition(
              (attn): MultiheadAttentionLoRA(
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (2): ResidualAttentionBlockVitPosition(
              (attn): MultiheadAttentionLoRA(
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (3): ResidualAttentionBlockVitPosition(
              (attn): MultiheadAttentionLoRA(
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (4): ResidualAttentionBlockVitPosition(
              (attn): MultiheadAttentionLoRA(
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (5): ResidualAttentionBlockVitPosition(
              (attn): MultiheadAttentionLoRA(
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (6): ResidualAttentionBlockVitPosition(
              (attn): MultiheadAttentionLoRA(
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (7): ResidualAttentionBlockVitPosition(
              (attn): MultiheadAttentionLoRA(
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (8): ResidualAttentionBlockVitPosition(
              (attn): MultiheadAttentionLoRA(
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (9): ResidualAttentionBlockVitPosition(
              (attn): MultiheadAttentionLoRA(
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (10): ResidualAttentionBlockVitPosition(
              (attn): MultiheadAttentionLoRA(
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (11): ResidualAttentionBlockVitPosition(
              (attn): MultiheadAttentionVit(
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
        (ln_post): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (transformer): Transformer(
        (resblocks): Sequential(
          (0): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (1): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (2): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (3): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (4): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (5): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (6): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (7): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (8): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (9): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (10): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (11): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (token_embedding): Embedding(49408, 512)
      (ln_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (prompt_learner): LearnablePromptExtractor(
      prefix_prompt:16,suffix_prompt:0,dimension:512
      [Normal_Init(mu=0,std=0.02)]
    )
  )
  (decoder): TransformerDecoderLinear(
    (layers): ModuleList(
      (0): TransformerDecoderLayerLinear(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
        )
        (multihead_attn_my): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
        )
        (linear1): Linear(in_features=512, out_features=1024, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=1024, out_features=512, bias=True)
        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
    )
    (outlayer): TransformerDecoderOutLayerLinear(
      (self_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
      )
      (multihead_attn_my): MultiHeadAttentionMy(
        (dropout_F): Dropout(p=0.1, inplace=False)
      )
      (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (dropout1): Dropout(p=0.1, inplace=False)
      (dropout2): Dropout(p=0.1, inplace=False)
      (dropout3): Dropout(p=0.1, inplace=False)
    )
    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  )
  (pe_layer): PositionEmbeddingSine()
  (query_embedding): Embedding(100, 512)
)
[01/13 16:47:38] mask_former.data.dataset_mappers.mask_former_semantic_dataset_mapper INFO: [MaskFormerSemanticDatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=(256, 307, 358, 409, 460, 512, 563, 614, 665, 716, 768), max_size=2560, sample_style='choice'), RandomCrop_CategoryAreaConstraint(crop_type='absolute', crop_size=[640, 640], single_category_max_area=1.0, ignored_category=255), <detectron2.projects.point_rend.color_augmentation.ColorAugSSDTransform object at 0x7f754c0b3100>, RandomFlip()]
[01/13 16:47:40] detectron2.data.datasets.coco INFO: Loaded 118287 images with semantic segmentation from /scratch/local/ssd/anjun/datasets/coco-stuff/images/train2017
[01/13 16:47:40] mask_former.data.build INFO: Using training sampler TrainingSampler
[01/13 16:47:40] detectron2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[01/13 16:47:40] detectron2.data.common INFO: Serializing 118287 elements to byte tensors and concatenating them all ...
[01/13 16:47:40] detectron2.data.common INFO: Serialized dataset takes 32.38 MiB
[01/13 16:47:40] detectron2.data.build INFO: Making batched data loader with batch_size=4
[01/13 16:47:41] detectron2.checkpoint.detection_checkpoint INFO: [DetectionCheckpointer] Loading from pretrained_models/deop_model_final.pth ...
[01/13 16:47:41] fvcore.common.checkpoint INFO: [Checkpointer] Loading from pretrained_models/deop_model_final.pth ...
[01/13 16:47:42] fvcore.common.checkpoint WARNING: Skip loading parameter 'criterion.empty_weight' to the model due to incompatible shapes: (157,) in the checkpoint but (172,) in the model! You might want to double check if this is expected.
[01/13 16:47:42] fvcore.common.checkpoint WARNING: Some model parameters or buffers are not found in the checkpoint:
[34mclip_adapter.clip_model.token_embedding.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.0.mlp.c_fc.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.0.mlp.c_proj.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.1.mlp.c_fc.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.1.mlp.c_proj.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.10.learned_position[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.10.mlp.c_fc.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.10.mlp.c_proj.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.11.learned_position[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.11.mlp.c_fc.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.11.mlp.c_proj.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.2.mlp.c_fc.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.2.mlp.c_proj.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.3.mlp.c_fc.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.3.mlp.c_proj.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.4.mlp.c_fc.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.4.mlp.c_proj.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.5.mlp.c_fc.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.5.mlp.c_proj.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.6.learned_position[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.6.mlp.c_fc.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.6.mlp.c_proj.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.7.learned_position[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.7.mlp.c_fc.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.7.mlp.c_proj.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.8.learned_position[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.8.mlp.c_fc.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.8.mlp.c_proj.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.9.learned_position[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.9.mlp.c_fc.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.9.mlp.c_proj.{lora_A, lora_B}[0m
[34mcriterion.empty_weight[0m
[01/13 16:47:42] detectron2.engine.train_loop INFO: Starting training from iteration 0
[01/13 16:47:59] detectron2.engine.train_loop ERROR: Exception during training:
Traceback (most recent call last):
  File "/homes/55/anjun/.local/lib/python3.10/site-packages/detectron2/engine/train_loop.py", line 155, in train
    self.run_step()
  File "/scratch/local/ssd/anjun/ovseg/DeOP/train_net.py", line 361, in run_step
    loss_dict = self.model(data)
  File "/scratch/local/ssd/anjun/anaconda3/envs/ldm/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/local/ssd/anjun/anaconda3/envs/ldm/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/local/ssd/anjun/anaconda3/envs/ldm/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1515, in forward
    inputs, kwargs = self._pre_forward(*inputs, **kwargs)
  File "/scratch/local/ssd/anjun/anaconda3/envs/ldm/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1409, in _pre_forward
    if torch.is_grad_enabled() and self.reducer._rebuild_buckets():
RuntimeError: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by 
making sure all `forward` function outputs participate in calculating loss. 
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameter indices which did not receive grad for rank 2: 61 62
 In addition, you can set the environment variable TORCH_DISTRIBUTED_DEBUG to either INFO or DETAIL to print out information about which particular parameters did not receive gradient on this rank as part of this error
[01/13 16:47:59] detectron2.engine.hooks INFO: Total training time: 0:00:00 (0:00:00 on hooks)
[01/13 16:49:14] detectron2 INFO: Rank of current process: 2. World size: 4
[01/13 16:49:17] detectron2 INFO: Environment info:
-------------------------------  --------------------------------------------------------------------------------------------------
sys.platform                     linux
Python                           3.10.11 (main, May 16 2023, 00:28:57) [GCC 11.2.0]
numpy                            1.26.2
detectron2                       0.6 @/homes/55/anjun/.local/lib/python3.10/site-packages/detectron2
Compiler                         GCC 8.3
CUDA compiler                    CUDA 12.1
detectron2 arch flags            8.6
DETECTRON2_ENV_MODULE            <not set>
PyTorch                          2.1.1+cu121 @/scratch/local/ssd/anjun/anaconda3/envs/ldm/lib/python3.10/site-packages/torch
PyTorch debug build              False
torch._C._GLIBCXX_USE_CXX11_ABI  False
GPU available                    Yes
GPU 0,1,2,3                      NVIDIA A40 (arch=8.6)
Driver version                   530.30.02
CUDA_HOME                        /usr/local/cuda-12.1/
Pillow                           9.5.0
torchvision                      0.16.1+cu121 @/scratch/local/ssd/anjun/anaconda3/envs/ldm/lib/python3.10/site-packages/torchvision
torchvision arch flags           5.0, 6.0, 7.0, 7.5, 8.0, 8.6, 9.0
fvcore                           0.1.5.post20221221
iopath                           0.1.9
cv2                              4.5.4-dev
-------------------------------  --------------------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v3.1.1 (Git Hash 64f6bcbcbab628e96f33a62c3e975f8535a7bde4)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 12.1
  - NVCC architecture flags: -gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.9.2
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=12.1, CUDNN_VERSION=8.9.2, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=old-style-cast -Wno-invalid-partial-specialization -Wno-unused-private-field -Wno-aligned-allocation-unavailable -Wno-missing-braces -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.1.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[01/13 16:49:17] detectron2 INFO: Command line arguments: Namespace(config_file='configs/coco-stuff-164k-156/zero_shot_maskformer_R101c_bs32_60k_proposalmask_featupsample_img512.yaml', resume=True, eval_only=False, num_gpus=4, num_machines=1, machine_rank=0, dist_url='tcp://0.0.0.0:12237', opts=['MODEL.CLIP_ADAPTER.CLIP_ENSEMBLE', 'False', 'MODEL.CLIP_ADAPTER.CLIP_ENSEMBLE_WEIGHT', '-1.0', 'SOLVER.TEST_IMS_PER_BATCH', '1', 'OUTPUT_DIR', 'deop/train_log', 'MODEL.CLIP_ADAPTER.PROMPT_LEARNER', 'learnable', 'SOLVER.IMS_PER_BATCH', '16', 'SOLVER.BASE_LR', '0.00005', 'SOLVER.CHECKPOINT_PERIOD', '1000', 'MODEL.CLIP_ADAPTER.CLIP_MODEL_NAME', 'ViT-B/16', 'MODEL.WEIGHTS', 'pretrained_models/deop_model_final.pth', 'MODEL.META_ARCHITECTURE', 'ZeroShotClipfeatUpsample_addnorm_vit16Query2D_maskvit', 'MODEL.NUM_DECODER_LAYER', '1', 'ORACLE', 'False', 'INPUT.MIN_SIZE_TEST', '512', 'TEST.EVAL_PERIOD', '1000', 'INPUT.SIZE_DIVISIBILITY', '512', 'MODEL.MASK_FORMER.DECODER_DICE_WEIGHT', '0.8', 'MODEL.MASK_FORMER.DECODER_CE_WEIGHT', '2.0', 'MODEL.CLIP_ADAPTER.LEARN_POSITION', 'True', 'MODEL.CLIP_ADAPTER.POSITION_LAYERS', '[1,2,3,4,5,6,7,8,9,10,11]', 'MODEL.CLIP_ADAPTER.LAYERMASKVIT', '[11]', 'MODEL.CLIP_ADAPTER.END2ENDTRAIN', 'False', 'MODEL.CLIP_ADAPTER.PS_SHORTCUT', '1.0', 'DATASETS.TRAIN', "('coco_2017_train_stuff_all_sem_seg',)", 'DATASETS.TEST', "('context_59_test_sem_seg',)", 'MODEL.SEM_SEG_HEAD.NUM_CLASSES', '171', 'MODEL.MASK_FORMER.LOSS_NUM_CLASS', '171'])
[01/13 16:49:17] detectron2 INFO: Contents of args.config_file=configs/coco-stuff-164k-156/zero_shot_maskformer_R101c_bs32_60k_proposalmask_featupsample_img512.yaml:
[38;5;197m_BASE_[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mzero_shot_maskformer_R50_bs32_60k.yaml[39m
[38;5;197mMODEL[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mSEM_SEG_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mNUM_CLASSES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m171[39m
[38;5;15m  [39m[38;5;197mMETA_ARCHITECTURE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mZeroShotMaskFormerClipfeatUpsample[39m[38;5;186m"[39m
[38;5;15m  [39m[38;5;197mBACKBONE[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mbuild_resnet_deeplab_backbone[39m[38;5;186m"[39m
[38;5;15m  [39m
[38;5;15m  [39m[38;5;197mWEIGHTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mdetectron2://DeepLab/R-103.pkl[39m[38;5;186m"[39m
[38;5;15m  [39m[38;5;197mRESNETS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mDEPTH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m101[39m
[38;5;15m    [39m[38;5;197mSTEM_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mdeeplab[39m[38;5;186m"[39m
[38;5;15m    [39m[38;5;197mSTEM_OUT_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m128[39m
[38;5;15m    [39m[38;5;197mSTRIDE_IN_1X1[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mFalse[39m
[38;5;15m    [39m[38;5;197mOUT_FEATURES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;186m"[39m[38;5;186mres2[39m[38;5;186m"[39m[38;5;15m,[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mres3[39m[38;5;186m"[39m[38;5;15m,[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mres4[39m[38;5;186m"[39m[38;5;15m,[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mres5[39m[38;5;186m"[39m[38;5;15m][39m
[38;5;15m    [39m[38;5;242m# NORM: "SyncBN"[39m
[38;5;15m    [39m[38;5;197mRES5_MULTI_GRID[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m1[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15m2[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15m4[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;197mCLIP_ADAPTER[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCLIP_ENSEMBLE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mTrue[39m
[38;5;15m    [39m[38;5;197mCLIP_ENSEMBLE_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.8[39m
[38;5;197mINPUT[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mMIN_SIZE_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;81m!!python/object/apply:eval[39m[38;5;15m [39m[38;5;15m[[39m[38;5;186m"[39m[38;5;186m[int(x[39m[38;5;15m [39m[38;5;186m*[39m[38;5;15m [39m[38;5;186m0.1[39m[38;5;15m [39m[38;5;186m*[39m[38;5;15m [39m[38;5;186m512)[39m[38;5;15m [39m[38;5;186mfor[39m[38;5;15m [39m[38;5;186mx[39m[38;5;15m [39m[38;5;186min[39m[38;5;15m [39m[38;5;186mrange(5,[39m[38;5;15m [39m[38;5;186m16)][39m[38;5;186m"[39m[38;5;15m][39m
[38;5;197mOUTPUT_DIR[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mdeop/train_log[39m[38;5;186m"[39m
[38;5;197mSOLVER[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mIMS_PER_BATCH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m8[39m
[38;5;15m  [39m[38;5;197mTEST_IMS_PER_BATCH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m8[39m
[38;5;197mTEST[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mEVAL_PERIOD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m5000[39m

[01/13 16:49:17] detectron2.utils.env INFO: Using a generated random seed 17400844
[01/13 16:49:17] mask_former.modeling.clip_adapter INFO: Prompt Learner training params: ['prefix_prompt']
[01/13 16:49:26] detectron2.engine.defaults INFO: Model:
ZeroShotClipfeatUpsample_addnorm_vit16Query2D_maskvit(
  (backbone): ResNet(
    (stem): DeepLabStem(
      (conv1): Conv2d(
        3, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
      )
      (conv3): Conv2d(
        64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
      )
    )
    (res2): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv1): Conv2d(
          128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
      )
    )
    (res3): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv1): Conv2d(
          256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
      (3): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
    )
    (res4): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
        (conv1): Conv2d(
          512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (3): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (4): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (5): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (6): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (7): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (8): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (9): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (10): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (11): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (12): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (13): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (14): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (15): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (16): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (17): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (18): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (19): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (20): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (21): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (22): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
    )
    (res5): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
        (conv1): Conv2d(
          1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
      )
    )
  )
  (sem_seg_head): ZeroShotMaskFormerHead(
    (pixel_decoder): BasePixelDecoder(
      (adapter_1): Conv2d(
        256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (layer_1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (adapter_2): Conv2d(
        512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (layer_2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (adapter_3): Conv2d(
        1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (layer_3): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (layer_4): Conv2d(
        2048, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (mask_features): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
    (predictor): ZeroShotTransformerPredictor(
      (pe_layer): PositionEmbeddingSine()
      (transformer): Transformer(
        (encoder): TransformerEncoder(
          (layers): ModuleList()
        )
        (decoder): TransformerDecoder(
          (layers): ModuleList(
            (0-5): 6 x TransformerDecoderLayer(
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (multihead_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (linear1): Linear(in_features=256, out_features=2048, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (linear2): Linear(in_features=2048, out_features=256, bias=True)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (dropout1): Dropout(p=0.1, inplace=False)
              (dropout2): Dropout(p=0.1, inplace=False)
              (dropout3): Dropout(p=0.1, inplace=False)
            )
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (query_embed): Embedding(100, 256)
      (input_proj): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
      (mask_embed): MLP(
        (layers): ModuleList(
          (0-2): 3 x Linear(in_features=256, out_features=256, bias=True)
        )
      )
      (class_embed): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=1024, bias=True)
          (1): Linear(in_features=1024, out_features=512, bias=True)
        )
      )
    )
  )
  (criterion): SetCriterion(
    (matcher): Matcher HungarianMatcher
        cost_class: 1
        cost_mask: 20.0
        cost_dice: 1.0
  )
  (clip_adapter): MaskFormerClipFeatureAdapter(
    (clip_model): CLIP(
      (visual): VisionTransformerLearnPosition(
        (conv1): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False)
        (ln_pre): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (transformer): TransformerVitPosition(
          (resblocks): Sequential(
            (0): ResidualAttentionBlockVitPosition(
              (attn): MultiheadAttentionLoRA(
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (1): ResidualAttentionBlockVitPosition(
              (attn): MultiheadAttentionLoRA(
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (2): ResidualAttentionBlockVitPosition(
              (attn): MultiheadAttentionLoRA(
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (3): ResidualAttentionBlockVitPosition(
              (attn): MultiheadAttentionLoRA(
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (4): ResidualAttentionBlockVitPosition(
              (attn): MultiheadAttentionLoRA(
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (5): ResidualAttentionBlockVitPosition(
              (attn): MultiheadAttentionLoRA(
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (6): ResidualAttentionBlockVitPosition(
              (attn): MultiheadAttentionLoRA(
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (7): ResidualAttentionBlockVitPosition(
              (attn): MultiheadAttentionLoRA(
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (8): ResidualAttentionBlockVitPosition(
              (attn): MultiheadAttentionLoRA(
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (9): ResidualAttentionBlockVitPosition(
              (attn): MultiheadAttentionLoRA(
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (10): ResidualAttentionBlockVitPosition(
              (attn): MultiheadAttentionLoRA(
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (11): ResidualAttentionBlockVitPosition(
              (attn): MultiheadAttentionVit(
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
        (ln_post): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (transformer): Transformer(
        (resblocks): Sequential(
          (0): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (1): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (2): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (3): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (4): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (5): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (6): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (7): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (8): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (9): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (10): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (11): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (token_embedding): Embedding(49408, 512)
      (ln_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (prompt_learner): LearnablePromptExtractor(
      prefix_prompt:16,suffix_prompt:0,dimension:512
      [Normal_Init(mu=0,std=0.02)]
    )
  )
  (decoder): TransformerDecoderLinear(
    (layers): ModuleList(
      (0): TransformerDecoderLayerLinear(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
        )
        (multihead_attn_my): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
        )
        (linear1): Linear(in_features=512, out_features=1024, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=1024, out_features=512, bias=True)
        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
    )
    (outlayer): TransformerDecoderOutLayerLinear(
      (self_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
      )
      (multihead_attn_my): MultiHeadAttentionMy(
        (dropout_F): Dropout(p=0.1, inplace=False)
      )
      (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (dropout1): Dropout(p=0.1, inplace=False)
      (dropout2): Dropout(p=0.1, inplace=False)
      (dropout3): Dropout(p=0.1, inplace=False)
    )
    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  )
  (pe_layer): PositionEmbeddingSine()
  (query_embedding): Embedding(100, 512)
)
[01/13 16:49:26] mask_former.data.dataset_mappers.mask_former_semantic_dataset_mapper INFO: [MaskFormerSemanticDatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=(256, 307, 358, 409, 460, 512, 563, 614, 665, 716, 768), max_size=2560, sample_style='choice'), RandomCrop_CategoryAreaConstraint(crop_type='absolute', crop_size=[640, 640], single_category_max_area=1.0, ignored_category=255), <detectron2.projects.point_rend.color_augmentation.ColorAugSSDTransform object at 0x7fc3d460b220>, RandomFlip()]
[01/13 16:49:28] detectron2.data.datasets.coco INFO: Loaded 118287 images with semantic segmentation from /scratch/local/ssd/anjun/datasets/coco-stuff/images/train2017
[01/13 16:49:28] mask_former.data.build INFO: Using training sampler TrainingSampler
[01/13 16:49:28] detectron2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[01/13 16:49:28] detectron2.data.common INFO: Serializing 118287 elements to byte tensors and concatenating them all ...
[01/13 16:49:29] detectron2.data.common INFO: Serialized dataset takes 32.38 MiB
[01/13 16:49:29] detectron2.data.build INFO: Making batched data loader with batch_size=4
[01/13 16:49:30] detectron2.checkpoint.detection_checkpoint INFO: [DetectionCheckpointer] Loading from pretrained_models/deop_model_final.pth ...
[01/13 16:49:30] fvcore.common.checkpoint INFO: [Checkpointer] Loading from pretrained_models/deop_model_final.pth ...
[01/13 16:49:30] fvcore.common.checkpoint WARNING: Skip loading parameter 'criterion.empty_weight' to the model due to incompatible shapes: (157,) in the checkpoint but (172,) in the model! You might want to double check if this is expected.
[01/13 16:49:30] fvcore.common.checkpoint WARNING: Some model parameters or buffers are not found in the checkpoint:
[34mclip_adapter.clip_model.visual.transformer.resblocks.0.mlp.c_fc.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.0.mlp.c_proj.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.1.mlp.c_fc.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.1.mlp.c_proj.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.10.learned_position[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.10.mlp.c_fc.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.10.mlp.c_proj.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.11.learned_position[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.11.mlp.c_fc.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.11.mlp.c_proj.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.2.mlp.c_fc.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.2.mlp.c_proj.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.3.mlp.c_fc.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.3.mlp.c_proj.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.4.mlp.c_fc.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.4.mlp.c_proj.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.5.mlp.c_fc.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.5.mlp.c_proj.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.6.learned_position[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.6.mlp.c_fc.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.6.mlp.c_proj.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.7.learned_position[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.7.mlp.c_fc.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.7.mlp.c_proj.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.8.learned_position[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.8.mlp.c_fc.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.8.mlp.c_proj.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.9.learned_position[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.9.mlp.c_fc.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.9.mlp.c_proj.{lora_A, lora_B}[0m
[34mcriterion.empty_weight[0m
[01/13 16:49:30] detectron2.engine.train_loop INFO: Starting training from iteration 0
[01/13 17:08:38] mask_former.data.dataset_mappers.mask_former_semantic_dataset_mapper INFO: [MaskFormerSemanticDatasetMapper] Augmentations used in inference: []
[01/13 17:08:38] detectron2.data.datasets.coco WARNING: Directory /scratch/local/ssd/anjun/datasets/VOCdevkit/VOC2010/JPEGImages and /scratch/local/ssd/anjun/datasets/VOCdevkit/VOC2010/annotations_detectron2/pc59_val has 11321 and 5104 files, respectively.
[01/13 17:08:38] detectron2.data.datasets.coco WARNING: Will use their intersection of 5104 files.
[01/13 17:08:38] detectron2.data.datasets.coco INFO: Loaded 5104 images with semantic segmentation from /scratch/local/ssd/anjun/datasets/VOCdevkit/VOC2010/JPEGImages
[01/13 17:08:38] detectron2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[01/13 17:08:38] detectron2.data.common INFO: Serializing 5104 elements to byte tensors and concatenating them all ...
[01/13 17:08:38] detectron2.data.common INFO: Serialized dataset takes 1.37 MiB
[01/13 17:08:38] detectron2.data.datasets.coco WARNING: Directory /scratch/local/ssd/anjun/datasets/VOCdevkit/VOC2010/JPEGImages and /scratch/local/ssd/anjun/datasets/VOCdevkit/VOC2010/annotations_detectron2/pc59_val has 11321 and 5104 files, respectively.
[01/13 17:08:38] detectron2.data.datasets.coco WARNING: Will use their intersection of 5104 files.
[01/13 17:08:38] detectron2.data.datasets.coco INFO: Loaded 5104 images with semantic segmentation from /scratch/local/ssd/anjun/datasets/VOCdevkit/VOC2010/JPEGImages
[01/13 17:08:38] detectron2.evaluation.evaluator INFO: Start inference on 1276 batches
[01/13 17:08:45] detectron2.evaluation.evaluator INFO: Inference done 11/1276. Dataloading: 0.0007 s/iter. Inference: 0.1371 s/iter. Eval: 0.0061 s/iter. Total: 0.1439 s/iter. ETA=0:03:02
[01/13 17:08:50] detectron2.evaluation.evaluator INFO: Inference done 47/1276. Dataloading: 0.0014 s/iter. Inference: 0.1341 s/iter. Eval: 0.0066 s/iter. Total: 0.1422 s/iter. ETA=0:02:54
[01/13 17:08:56] detectron2.evaluation.evaluator INFO: Inference done 85/1276. Dataloading: 0.0015 s/iter. Inference: 0.1301 s/iter. Eval: 0.0064 s/iter. Total: 0.1381 s/iter. ETA=0:02:44
[01/13 17:09:01] detectron2.evaluation.evaluator INFO: Inference done 124/1276. Dataloading: 0.0015 s/iter. Inference: 0.1278 s/iter. Eval: 0.0064 s/iter. Total: 0.1357 s/iter. ETA=0:02:36
[01/13 17:09:06] detectron2.evaluation.evaluator INFO: Inference done 163/1276. Dataloading: 0.0015 s/iter. Inference: 0.1268 s/iter. Eval: 0.0063 s/iter. Total: 0.1346 s/iter. ETA=0:02:29
[01/13 17:09:11] detectron2.evaluation.evaluator INFO: Inference done 202/1276. Dataloading: 0.0015 s/iter. Inference: 0.1261 s/iter. Eval: 0.0062 s/iter. Total: 0.1339 s/iter. ETA=0:02:23
[01/13 17:09:16] detectron2.evaluation.evaluator INFO: Inference done 240/1276. Dataloading: 0.0015 s/iter. Inference: 0.1262 s/iter. Eval: 0.0062 s/iter. Total: 0.1340 s/iter. ETA=0:02:18
[01/13 17:09:21] detectron2.evaluation.evaluator INFO: Inference done 277/1276. Dataloading: 0.0015 s/iter. Inference: 0.1264 s/iter. Eval: 0.0062 s/iter. Total: 0.1341 s/iter. ETA=0:02:14
[01/13 17:09:26] detectron2.evaluation.evaluator INFO: Inference done 316/1276. Dataloading: 0.0015 s/iter. Inference: 0.1258 s/iter. Eval: 0.0062 s/iter. Total: 0.1335 s/iter. ETA=0:02:08
[01/13 17:09:31] detectron2.evaluation.evaluator INFO: Inference done 355/1276. Dataloading: 0.0015 s/iter. Inference: 0.1255 s/iter. Eval: 0.0061 s/iter. Total: 0.1331 s/iter. ETA=0:02:02
[01/13 17:09:36] detectron2.evaluation.evaluator INFO: Inference done 392/1276. Dataloading: 0.0015 s/iter. Inference: 0.1258 s/iter. Eval: 0.0061 s/iter. Total: 0.1335 s/iter. ETA=0:01:57
[01/13 17:09:41] detectron2.evaluation.evaluator INFO: Inference done 430/1276. Dataloading: 0.0015 s/iter. Inference: 0.1257 s/iter. Eval: 0.0062 s/iter. Total: 0.1334 s/iter. ETA=0:01:52
[01/13 17:09:46] detectron2.evaluation.evaluator INFO: Inference done 468/1276. Dataloading: 0.0015 s/iter. Inference: 0.1256 s/iter. Eval: 0.0062 s/iter. Total: 0.1334 s/iter. ETA=0:01:47
[01/13 17:09:51] detectron2.evaluation.evaluator INFO: Inference done 507/1276. Dataloading: 0.0015 s/iter. Inference: 0.1253 s/iter. Eval: 0.0062 s/iter. Total: 0.1330 s/iter. ETA=0:01:42
[01/13 17:09:56] detectron2.evaluation.evaluator INFO: Inference done 546/1276. Dataloading: 0.0015 s/iter. Inference: 0.1251 s/iter. Eval: 0.0062 s/iter. Total: 0.1329 s/iter. ETA=0:01:37
[01/13 17:10:02] detectron2.evaluation.evaluator INFO: Inference done 585/1276. Dataloading: 0.0015 s/iter. Inference: 0.1250 s/iter. Eval: 0.0062 s/iter. Total: 0.1327 s/iter. ETA=0:01:31
[01/13 17:10:07] detectron2.evaluation.evaluator INFO: Inference done 623/1276. Dataloading: 0.0015 s/iter. Inference: 0.1250 s/iter. Eval: 0.0062 s/iter. Total: 0.1327 s/iter. ETA=0:01:26
[01/13 17:10:12] detectron2.evaluation.evaluator INFO: Inference done 662/1276. Dataloading: 0.0015 s/iter. Inference: 0.1249 s/iter. Eval: 0.0062 s/iter. Total: 0.1326 s/iter. ETA=0:01:21
[01/13 17:10:17] detectron2.evaluation.evaluator INFO: Inference done 699/1276. Dataloading: 0.0015 s/iter. Inference: 0.1251 s/iter. Eval: 0.0062 s/iter. Total: 0.1328 s/iter. ETA=0:01:16
[01/13 17:10:22] detectron2.evaluation.evaluator INFO: Inference done 737/1276. Dataloading: 0.0015 s/iter. Inference: 0.1251 s/iter. Eval: 0.0062 s/iter. Total: 0.1327 s/iter. ETA=0:01:11
[01/13 17:10:27] detectron2.evaluation.evaluator INFO: Inference done 776/1276. Dataloading: 0.0015 s/iter. Inference: 0.1249 s/iter. Eval: 0.0061 s/iter. Total: 0.1326 s/iter. ETA=0:01:06
[01/13 17:10:32] detectron2.evaluation.evaluator INFO: Inference done 815/1276. Dataloading: 0.0015 s/iter. Inference: 0.1249 s/iter. Eval: 0.0061 s/iter. Total: 0.1325 s/iter. ETA=0:01:01
[01/13 17:10:37] detectron2.evaluation.evaluator INFO: Inference done 854/1276. Dataloading: 0.0015 s/iter. Inference: 0.1249 s/iter. Eval: 0.0061 s/iter. Total: 0.1325 s/iter. ETA=0:00:55
[01/13 17:10:42] detectron2.evaluation.evaluator INFO: Inference done 891/1276. Dataloading: 0.0015 s/iter. Inference: 0.1251 s/iter. Eval: 0.0061 s/iter. Total: 0.1328 s/iter. ETA=0:00:51
[01/13 17:10:47] detectron2.evaluation.evaluator INFO: Inference done 929/1276. Dataloading: 0.0015 s/iter. Inference: 0.1252 s/iter. Eval: 0.0061 s/iter. Total: 0.1328 s/iter. ETA=0:00:46
[01/13 17:10:52] detectron2.evaluation.evaluator INFO: Inference done 967/1276. Dataloading: 0.0015 s/iter. Inference: 0.1252 s/iter. Eval: 0.0061 s/iter. Total: 0.1328 s/iter. ETA=0:00:41
[01/13 17:10:57] detectron2.evaluation.evaluator INFO: Inference done 1003/1276. Dataloading: 0.0015 s/iter. Inference: 0.1254 s/iter. Eval: 0.0061 s/iter. Total: 0.1330 s/iter. ETA=0:00:36
[01/13 17:11:02] detectron2.evaluation.evaluator INFO: Inference done 1040/1276. Dataloading: 0.0015 s/iter. Inference: 0.1255 s/iter. Eval: 0.0061 s/iter. Total: 0.1332 s/iter. ETA=0:00:31
[01/13 17:11:07] detectron2.evaluation.evaluator INFO: Inference done 1077/1276. Dataloading: 0.0015 s/iter. Inference: 0.1257 s/iter. Eval: 0.0061 s/iter. Total: 0.1334 s/iter. ETA=0:00:26
[01/13 17:11:13] detectron2.evaluation.evaluator INFO: Inference done 1114/1276. Dataloading: 0.0015 s/iter. Inference: 0.1258 s/iter. Eval: 0.0061 s/iter. Total: 0.1334 s/iter. ETA=0:00:21
[01/13 17:11:18] detectron2.evaluation.evaluator INFO: Inference done 1151/1276. Dataloading: 0.0015 s/iter. Inference: 0.1258 s/iter. Eval: 0.0061 s/iter. Total: 0.1335 s/iter. ETA=0:00:16
[01/13 17:11:23] detectron2.evaluation.evaluator INFO: Inference done 1188/1276. Dataloading: 0.0015 s/iter. Inference: 0.1260 s/iter. Eval: 0.0061 s/iter. Total: 0.1336 s/iter. ETA=0:00:11
[01/13 17:11:28] detectron2.evaluation.evaluator INFO: Inference done 1225/1276. Dataloading: 0.0015 s/iter. Inference: 0.1261 s/iter. Eval: 0.0061 s/iter. Total: 0.1338 s/iter. ETA=0:00:06
[01/13 17:11:33] detectron2.evaluation.evaluator INFO: Inference done 1262/1276. Dataloading: 0.0015 s/iter. Inference: 0.1262 s/iter. Eval: 0.0061 s/iter. Total: 0.1339 s/iter. ETA=0:00:01
[01/13 17:11:36] detectron2.evaluation.evaluator INFO: Total inference time: 0:02:51.024260 (0.134559 s / iter per device, on 4 devices)
[01/13 17:11:36] detectron2.evaluation.evaluator INFO: Total inference pure compute time: 0:02:40 (0.126225 s / iter per device, on 4 devices)
[01/13 17:30:34] mask_former.data.dataset_mappers.mask_former_semantic_dataset_mapper INFO: [MaskFormerSemanticDatasetMapper] Augmentations used in inference: []
[01/13 17:30:34] detectron2.data.datasets.coco WARNING: Directory /scratch/local/ssd/anjun/datasets/VOCdevkit/VOC2010/JPEGImages and /scratch/local/ssd/anjun/datasets/VOCdevkit/VOC2010/annotations_detectron2/pc59_val has 11321 and 5104 files, respectively.
[01/13 17:30:34] detectron2.data.datasets.coco WARNING: Will use their intersection of 5104 files.
[01/13 17:30:34] detectron2.data.datasets.coco INFO: Loaded 5104 images with semantic segmentation from /scratch/local/ssd/anjun/datasets/VOCdevkit/VOC2010/JPEGImages
[01/13 17:30:34] detectron2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[01/13 17:30:34] detectron2.data.common INFO: Serializing 5104 elements to byte tensors and concatenating them all ...
[01/13 17:30:34] detectron2.data.common INFO: Serialized dataset takes 1.37 MiB
[01/13 17:30:34] detectron2.data.datasets.coco WARNING: Directory /scratch/local/ssd/anjun/datasets/VOCdevkit/VOC2010/JPEGImages and /scratch/local/ssd/anjun/datasets/VOCdevkit/VOC2010/annotations_detectron2/pc59_val has 11321 and 5104 files, respectively.
[01/13 17:30:34] detectron2.data.datasets.coco WARNING: Will use their intersection of 5104 files.
[01/13 17:30:34] detectron2.data.datasets.coco INFO: Loaded 5104 images with semantic segmentation from /scratch/local/ssd/anjun/datasets/VOCdevkit/VOC2010/JPEGImages
[01/13 17:30:34] detectron2.evaluation.evaluator INFO: Start inference on 1276 batches
[01/13 17:30:42] detectron2.evaluation.evaluator INFO: Inference done 11/1276. Dataloading: 0.0008 s/iter. Inference: 0.1325 s/iter. Eval: 0.0060 s/iter. Total: 0.1393 s/iter. ETA=0:02:56
[01/13 17:30:47] detectron2.evaluation.evaluator INFO: Inference done 49/1276. Dataloading: 0.0014 s/iter. Inference: 0.1261 s/iter. Eval: 0.0061 s/iter. Total: 0.1336 s/iter. ETA=0:02:43
[01/13 17:30:52] detectron2.evaluation.evaluator INFO: Inference done 86/1276. Dataloading: 0.0015 s/iter. Inference: 0.1271 s/iter. Eval: 0.0061 s/iter. Total: 0.1348 s/iter. ETA=0:02:40
[01/13 17:30:57] detectron2.evaluation.evaluator INFO: Inference done 124/1276. Dataloading: 0.0015 s/iter. Inference: 0.1271 s/iter. Eval: 0.0062 s/iter. Total: 0.1348 s/iter. ETA=0:02:35
[01/13 17:31:02] detectron2.evaluation.evaluator INFO: Inference done 161/1276. Dataloading: 0.0016 s/iter. Inference: 0.1272 s/iter. Eval: 0.0061 s/iter. Total: 0.1350 s/iter. ETA=0:02:30
[01/13 17:31:08] detectron2.evaluation.evaluator INFO: Inference done 199/1276. Dataloading: 0.0015 s/iter. Inference: 0.1269 s/iter. Eval: 0.0060 s/iter. Total: 0.1345 s/iter. ETA=0:02:24
[01/13 17:31:13] detectron2.evaluation.evaluator INFO: Inference done 236/1276. Dataloading: 0.0015 s/iter. Inference: 0.1271 s/iter. Eval: 0.0061 s/iter. Total: 0.1347 s/iter. ETA=0:02:20
[01/13 17:31:18] detectron2.evaluation.evaluator INFO: Inference done 273/1276. Dataloading: 0.0015 s/iter. Inference: 0.1276 s/iter. Eval: 0.0060 s/iter. Total: 0.1351 s/iter. ETA=0:02:15
[01/13 17:31:23] detectron2.evaluation.evaluator INFO: Inference done 310/1276. Dataloading: 0.0015 s/iter. Inference: 0.1277 s/iter. Eval: 0.0060 s/iter. Total: 0.1352 s/iter. ETA=0:02:10
[01/13 17:31:28] detectron2.evaluation.evaluator INFO: Inference done 349/1276. Dataloading: 0.0015 s/iter. Inference: 0.1271 s/iter. Eval: 0.0060 s/iter. Total: 0.1347 s/iter. ETA=0:02:04
[01/13 17:31:33] detectron2.evaluation.evaluator INFO: Inference done 386/1276. Dataloading: 0.0015 s/iter. Inference: 0.1274 s/iter. Eval: 0.0060 s/iter. Total: 0.1350 s/iter. ETA=0:02:00
[01/13 17:31:38] detectron2.evaluation.evaluator INFO: Inference done 424/1276. Dataloading: 0.0015 s/iter. Inference: 0.1274 s/iter. Eval: 0.0060 s/iter. Total: 0.1350 s/iter. ETA=0:01:54
[01/13 17:31:43] detectron2.evaluation.evaluator INFO: Inference done 462/1276. Dataloading: 0.0015 s/iter. Inference: 0.1273 s/iter. Eval: 0.0060 s/iter. Total: 0.1349 s/iter. ETA=0:01:49
[01/13 17:31:48] detectron2.evaluation.evaluator INFO: Inference done 500/1276. Dataloading: 0.0015 s/iter. Inference: 0.1272 s/iter. Eval: 0.0060 s/iter. Total: 0.1347 s/iter. ETA=0:01:44
[01/13 17:31:53] detectron2.evaluation.evaluator INFO: Inference done 537/1276. Dataloading: 0.0015 s/iter. Inference: 0.1272 s/iter. Eval: 0.0060 s/iter. Total: 0.1348 s/iter. ETA=0:01:39
[01/13 17:31:58] detectron2.evaluation.evaluator INFO: Inference done 575/1276. Dataloading: 0.0015 s/iter. Inference: 0.1271 s/iter. Eval: 0.0060 s/iter. Total: 0.1347 s/iter. ETA=0:01:34
[01/13 17:32:03] detectron2.evaluation.evaluator INFO: Inference done 612/1276. Dataloading: 0.0015 s/iter. Inference: 0.1272 s/iter. Eval: 0.0061 s/iter. Total: 0.1349 s/iter. ETA=0:01:29
[01/13 17:32:08] detectron2.evaluation.evaluator INFO: Inference done 650/1276. Dataloading: 0.0015 s/iter. Inference: 0.1272 s/iter. Eval: 0.0061 s/iter. Total: 0.1348 s/iter. ETA=0:01:24
[01/13 17:32:13] detectron2.evaluation.evaluator INFO: Inference done 687/1276. Dataloading: 0.0015 s/iter. Inference: 0.1273 s/iter. Eval: 0.0061 s/iter. Total: 0.1350 s/iter. ETA=0:01:19
[01/13 17:32:19] detectron2.evaluation.evaluator INFO: Inference done 725/1276. Dataloading: 0.0015 s/iter. Inference: 0.1273 s/iter. Eval: 0.0061 s/iter. Total: 0.1350 s/iter. ETA=0:01:14
[01/13 17:32:24] detectron2.evaluation.evaluator INFO: Inference done 763/1276. Dataloading: 0.0015 s/iter. Inference: 0.1273 s/iter. Eval: 0.0060 s/iter. Total: 0.1349 s/iter. ETA=0:01:09
[01/13 17:32:29] detectron2.evaluation.evaluator INFO: Inference done 800/1276. Dataloading: 0.0015 s/iter. Inference: 0.1274 s/iter. Eval: 0.0061 s/iter. Total: 0.1350 s/iter. ETA=0:01:04
[01/13 17:32:34] detectron2.evaluation.evaluator INFO: Inference done 838/1276. Dataloading: 0.0015 s/iter. Inference: 0.1274 s/iter. Eval: 0.0061 s/iter. Total: 0.1350 s/iter. ETA=0:00:59
[01/13 17:32:39] detectron2.evaluation.evaluator INFO: Inference done 875/1276. Dataloading: 0.0015 s/iter. Inference: 0.1274 s/iter. Eval: 0.0061 s/iter. Total: 0.1350 s/iter. ETA=0:00:54
[01/13 17:32:44] detectron2.evaluation.evaluator INFO: Inference done 913/1276. Dataloading: 0.0015 s/iter. Inference: 0.1274 s/iter. Eval: 0.0061 s/iter. Total: 0.1351 s/iter. ETA=0:00:49
[01/13 17:32:49] detectron2.evaluation.evaluator INFO: Inference done 951/1276. Dataloading: 0.0015 s/iter. Inference: 0.1273 s/iter. Eval: 0.0061 s/iter. Total: 0.1349 s/iter. ETA=0:00:43
[01/13 17:32:54] detectron2.evaluation.evaluator INFO: Inference done 988/1276. Dataloading: 0.0015 s/iter. Inference: 0.1274 s/iter. Eval: 0.0061 s/iter. Total: 0.1350 s/iter. ETA=0:00:38
[01/13 17:32:59] detectron2.evaluation.evaluator INFO: Inference done 1026/1276. Dataloading: 0.0015 s/iter. Inference: 0.1274 s/iter. Eval: 0.0061 s/iter. Total: 0.1350 s/iter. ETA=0:00:33
[01/13 17:33:04] detectron2.evaluation.evaluator INFO: Inference done 1063/1276. Dataloading: 0.0015 s/iter. Inference: 0.1275 s/iter. Eval: 0.0061 s/iter. Total: 0.1351 s/iter. ETA=0:00:28
[01/13 17:33:09] detectron2.evaluation.evaluator INFO: Inference done 1101/1276. Dataloading: 0.0015 s/iter. Inference: 0.1275 s/iter. Eval: 0.0060 s/iter. Total: 0.1351 s/iter. ETA=0:00:23
[01/13 17:33:15] detectron2.evaluation.evaluator INFO: Inference done 1139/1276. Dataloading: 0.0015 s/iter. Inference: 0.1274 s/iter. Eval: 0.0060 s/iter. Total: 0.1350 s/iter. ETA=0:00:18
[01/13 17:33:20] detectron2.evaluation.evaluator INFO: Inference done 1176/1276. Dataloading: 0.0015 s/iter. Inference: 0.1274 s/iter. Eval: 0.0060 s/iter. Total: 0.1350 s/iter. ETA=0:00:13
[01/13 17:33:25] detectron2.evaluation.evaluator INFO: Inference done 1214/1276. Dataloading: 0.0015 s/iter. Inference: 0.1274 s/iter. Eval: 0.0060 s/iter. Total: 0.1350 s/iter. ETA=0:00:08
[01/13 17:33:30] detectron2.evaluation.evaluator INFO: Inference done 1251/1276. Dataloading: 0.0015 s/iter. Inference: 0.1274 s/iter. Eval: 0.0060 s/iter. Total: 0.1350 s/iter. ETA=0:00:03
[01/13 17:33:34] detectron2.evaluation.evaluator INFO: Total inference time: 0:02:52.406890 (0.135647 s / iter per device, on 4 devices)
[01/13 17:33:34] detectron2.evaluation.evaluator INFO: Total inference pure compute time: 0:02:41 (0.127453 s / iter per device, on 4 devices)
[01/13 17:52:33] mask_former.data.dataset_mappers.mask_former_semantic_dataset_mapper INFO: [MaskFormerSemanticDatasetMapper] Augmentations used in inference: []
[01/13 17:52:33] detectron2.data.datasets.coco WARNING: Directory /scratch/local/ssd/anjun/datasets/VOCdevkit/VOC2010/JPEGImages and /scratch/local/ssd/anjun/datasets/VOCdevkit/VOC2010/annotations_detectron2/pc59_val has 11321 and 5104 files, respectively.
[01/13 17:52:33] detectron2.data.datasets.coco WARNING: Will use their intersection of 5104 files.
[01/13 17:52:33] detectron2.data.datasets.coco INFO: Loaded 5104 images with semantic segmentation from /scratch/local/ssd/anjun/datasets/VOCdevkit/VOC2010/JPEGImages
[01/13 17:52:33] detectron2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[01/13 17:52:33] detectron2.data.common INFO: Serializing 5104 elements to byte tensors and concatenating them all ...
[01/13 17:52:33] detectron2.data.common INFO: Serialized dataset takes 1.37 MiB
[01/13 17:52:33] detectron2.data.datasets.coco WARNING: Directory /scratch/local/ssd/anjun/datasets/VOCdevkit/VOC2010/JPEGImages and /scratch/local/ssd/anjun/datasets/VOCdevkit/VOC2010/annotations_detectron2/pc59_val has 11321 and 5104 files, respectively.
[01/13 17:52:33] detectron2.data.datasets.coco WARNING: Will use their intersection of 5104 files.
[01/13 17:52:33] detectron2.data.datasets.coco INFO: Loaded 5104 images with semantic segmentation from /scratch/local/ssd/anjun/datasets/VOCdevkit/VOC2010/JPEGImages
[01/13 17:52:33] detectron2.evaluation.evaluator INFO: Start inference on 1276 batches
[01/13 17:52:41] detectron2.evaluation.evaluator INFO: Inference done 11/1276. Dataloading: 0.0014 s/iter. Inference: 0.1328 s/iter. Eval: 0.0068 s/iter. Total: 0.1411 s/iter. ETA=0:02:58
[01/13 17:52:46] detectron2.evaluation.evaluator INFO: Inference done 49/1276. Dataloading: 0.0017 s/iter. Inference: 0.1265 s/iter. Eval: 0.0064 s/iter. Total: 0.1346 s/iter. ETA=0:02:45
[01/13 17:52:52] detectron2.evaluation.evaluator INFO: Inference done 86/1276. Dataloading: 0.0021 s/iter. Inference: 0.1277 s/iter. Eval: 0.0064 s/iter. Total: 0.1363 s/iter. ETA=0:02:42
[01/13 17:52:57] detectron2.evaluation.evaluator INFO: Inference done 123/1276. Dataloading: 0.0020 s/iter. Inference: 0.1277 s/iter. Eval: 0.0064 s/iter. Total: 0.1361 s/iter. ETA=0:02:36
[01/13 17:53:02] detectron2.evaluation.evaluator INFO: Inference done 160/1276. Dataloading: 0.0020 s/iter. Inference: 0.1279 s/iter. Eval: 0.0063 s/iter. Total: 0.1363 s/iter. ETA=0:02:32
[01/13 17:53:07] detectron2.evaluation.evaluator INFO: Inference done 197/1276. Dataloading: 0.0020 s/iter. Inference: 0.1278 s/iter. Eval: 0.0063 s/iter. Total: 0.1362 s/iter. ETA=0:02:26
[01/13 17:53:12] detectron2.evaluation.evaluator INFO: Inference done 234/1276. Dataloading: 0.0020 s/iter. Inference: 0.1280 s/iter. Eval: 0.0063 s/iter. Total: 0.1364 s/iter. ETA=0:02:22
[01/13 17:53:17] detectron2.evaluation.evaluator INFO: Inference done 270/1276. Dataloading: 0.0020 s/iter. Inference: 0.1286 s/iter. Eval: 0.0063 s/iter. Total: 0.1369 s/iter. ETA=0:02:17
[01/13 17:53:22] detectron2.evaluation.evaluator INFO: Inference done 308/1276. Dataloading: 0.0019 s/iter. Inference: 0.1283 s/iter. Eval: 0.0063 s/iter. Total: 0.1366 s/iter. ETA=0:02:12
[01/13 17:53:27] detectron2.evaluation.evaluator INFO: Inference done 347/1276. Dataloading: 0.0019 s/iter. Inference: 0.1278 s/iter. Eval: 0.0062 s/iter. Total: 0.1360 s/iter. ETA=0:02:06
[01/13 17:53:32] detectron2.evaluation.evaluator INFO: Inference done 383/1276. Dataloading: 0.0019 s/iter. Inference: 0.1282 s/iter. Eval: 0.0062 s/iter. Total: 0.1364 s/iter. ETA=0:02:01
[01/13 17:53:37] detectron2.evaluation.evaluator INFO: Inference done 420/1276. Dataloading: 0.0019 s/iter. Inference: 0.1282 s/iter. Eval: 0.0062 s/iter. Total: 0.1364 s/iter. ETA=0:01:56
[01/13 17:53:42] detectron2.evaluation.evaluator INFO: Inference done 457/1276. Dataloading: 0.0019 s/iter. Inference: 0.1282 s/iter. Eval: 0.0062 s/iter. Total: 0.1363 s/iter. ETA=0:01:51
[01/13 17:53:47] detectron2.evaluation.evaluator INFO: Inference done 495/1276. Dataloading: 0.0019 s/iter. Inference: 0.1280 s/iter. Eval: 0.0062 s/iter. Total: 0.1362 s/iter. ETA=0:01:46
[01/13 17:53:52] detectron2.evaluation.evaluator INFO: Inference done 532/1276. Dataloading: 0.0019 s/iter. Inference: 0.1280 s/iter. Eval: 0.0063 s/iter. Total: 0.1362 s/iter. ETA=0:01:41
[01/13 17:53:57] detectron2.evaluation.evaluator INFO: Inference done 569/1276. Dataloading: 0.0019 s/iter. Inference: 0.1279 s/iter. Eval: 0.0063 s/iter. Total: 0.1362 s/iter. ETA=0:01:36
[01/13 17:54:02] detectron2.evaluation.evaluator INFO: Inference done 606/1276. Dataloading: 0.0019 s/iter. Inference: 0.1279 s/iter. Eval: 0.0063 s/iter. Total: 0.1361 s/iter. ETA=0:01:31
[01/13 17:54:07] detectron2.evaluation.evaluator INFO: Inference done 643/1276. Dataloading: 0.0019 s/iter. Inference: 0.1280 s/iter. Eval: 0.0063 s/iter. Total: 0.1362 s/iter. ETA=0:01:26
[01/13 17:54:12] detectron2.evaluation.evaluator INFO: Inference done 679/1276. Dataloading: 0.0019 s/iter. Inference: 0.1282 s/iter. Eval: 0.0063 s/iter. Total: 0.1364 s/iter. ETA=0:01:21
[01/13 17:54:18] detectron2.evaluation.evaluator INFO: Inference done 716/1276. Dataloading: 0.0019 s/iter. Inference: 0.1282 s/iter. Eval: 0.0063 s/iter. Total: 0.1364 s/iter. ETA=0:01:16
[01/13 17:54:23] detectron2.evaluation.evaluator INFO: Inference done 754/1276. Dataloading: 0.0019 s/iter. Inference: 0.1281 s/iter. Eval: 0.0063 s/iter. Total: 0.1363 s/iter. ETA=0:01:11
[01/13 17:54:28] detectron2.evaluation.evaluator INFO: Inference done 791/1276. Dataloading: 0.0019 s/iter. Inference: 0.1282 s/iter. Eval: 0.0063 s/iter. Total: 0.1364 s/iter. ETA=0:01:06
[01/13 17:54:33] detectron2.evaluation.evaluator INFO: Inference done 828/1276. Dataloading: 0.0019 s/iter. Inference: 0.1282 s/iter. Eval: 0.0063 s/iter. Total: 0.1364 s/iter. ETA=0:01:01
[01/13 17:54:38] detectron2.evaluation.evaluator INFO: Inference done 864/1276. Dataloading: 0.0019 s/iter. Inference: 0.1283 s/iter. Eval: 0.0063 s/iter. Total: 0.1365 s/iter. ETA=0:00:56
[01/13 17:54:43] detectron2.evaluation.evaluator INFO: Inference done 900/1276. Dataloading: 0.0019 s/iter. Inference: 0.1284 s/iter. Eval: 0.0063 s/iter. Total: 0.1366 s/iter. ETA=0:00:51
[01/13 17:54:48] detectron2.evaluation.evaluator INFO: Inference done 937/1276. Dataloading: 0.0019 s/iter. Inference: 0.1284 s/iter. Eval: 0.0063 s/iter. Total: 0.1366 s/iter. ETA=0:00:46
[01/13 17:54:53] detectron2.evaluation.evaluator INFO: Inference done 975/1276. Dataloading: 0.0019 s/iter. Inference: 0.1282 s/iter. Eval: 0.0063 s/iter. Total: 0.1365 s/iter. ETA=0:00:41
[01/13 17:54:58] detectron2.evaluation.evaluator INFO: Inference done 1012/1276. Dataloading: 0.0019 s/iter. Inference: 0.1283 s/iter. Eval: 0.0063 s/iter. Total: 0.1365 s/iter. ETA=0:00:36
[01/13 17:55:03] detectron2.evaluation.evaluator INFO: Inference done 1048/1276. Dataloading: 0.0019 s/iter. Inference: 0.1283 s/iter. Eval: 0.0063 s/iter. Total: 0.1366 s/iter. ETA=0:00:31
[01/13 17:55:08] detectron2.evaluation.evaluator INFO: Inference done 1084/1276. Dataloading: 0.0019 s/iter. Inference: 0.1284 s/iter. Eval: 0.0063 s/iter. Total: 0.1367 s/iter. ETA=0:00:26
[01/13 17:55:13] detectron2.evaluation.evaluator INFO: Inference done 1122/1276. Dataloading: 0.0019 s/iter. Inference: 0.1284 s/iter. Eval: 0.0063 s/iter. Total: 0.1366 s/iter. ETA=0:00:21
[01/13 17:55:18] detectron2.evaluation.evaluator INFO: Inference done 1159/1276. Dataloading: 0.0019 s/iter. Inference: 0.1284 s/iter. Eval: 0.0063 s/iter. Total: 0.1366 s/iter. ETA=0:00:15
[01/13 17:55:23] detectron2.evaluation.evaluator INFO: Inference done 1197/1276. Dataloading: 0.0019 s/iter. Inference: 0.1284 s/iter. Eval: 0.0063 s/iter. Total: 0.1365 s/iter. ETA=0:00:10
[01/13 17:55:28] detectron2.evaluation.evaluator INFO: Inference done 1234/1276. Dataloading: 0.0019 s/iter. Inference: 0.1283 s/iter. Eval: 0.0063 s/iter. Total: 0.1365 s/iter. ETA=0:00:05
[01/13 17:55:33] detectron2.evaluation.evaluator INFO: Inference done 1270/1276. Dataloading: 0.0019 s/iter. Inference: 0.1284 s/iter. Eval: 0.0063 s/iter. Total: 0.1366 s/iter. ETA=0:00:00
[01/13 17:55:35] detectron2.evaluation.evaluator INFO: Total inference time: 0:02:54.434080 (0.137242 s / iter per device, on 4 devices)
[01/13 17:55:35] detectron2.evaluation.evaluator INFO: Total inference pure compute time: 0:02:43 (0.128450 s / iter per device, on 4 devices)
[01/13 18:14:34] mask_former.data.dataset_mappers.mask_former_semantic_dataset_mapper INFO: [MaskFormerSemanticDatasetMapper] Augmentations used in inference: []
[01/13 18:14:34] detectron2.data.datasets.coco WARNING: Directory /scratch/local/ssd/anjun/datasets/VOCdevkit/VOC2010/JPEGImages and /scratch/local/ssd/anjun/datasets/VOCdevkit/VOC2010/annotations_detectron2/pc59_val has 11321 and 5104 files, respectively.
[01/13 18:14:34] detectron2.data.datasets.coco WARNING: Will use their intersection of 5104 files.
[01/13 18:14:34] detectron2.data.datasets.coco INFO: Loaded 5104 images with semantic segmentation from /scratch/local/ssd/anjun/datasets/VOCdevkit/VOC2010/JPEGImages
[01/13 18:14:34] detectron2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[01/13 18:14:34] detectron2.data.common INFO: Serializing 5104 elements to byte tensors and concatenating them all ...
[01/13 18:14:34] detectron2.data.common INFO: Serialized dataset takes 1.37 MiB
[01/13 18:14:34] detectron2.data.datasets.coco WARNING: Directory /scratch/local/ssd/anjun/datasets/VOCdevkit/VOC2010/JPEGImages and /scratch/local/ssd/anjun/datasets/VOCdevkit/VOC2010/annotations_detectron2/pc59_val has 11321 and 5104 files, respectively.
[01/13 18:14:34] detectron2.data.datasets.coco WARNING: Will use their intersection of 5104 files.
[01/13 18:14:34] detectron2.data.datasets.coco INFO: Loaded 5104 images with semantic segmentation from /scratch/local/ssd/anjun/datasets/VOCdevkit/VOC2010/JPEGImages
[01/13 18:14:34] detectron2.evaluation.evaluator INFO: Start inference on 1276 batches
[01/13 18:14:42] detectron2.evaluation.evaluator INFO: Inference done 11/1276. Dataloading: 0.0007 s/iter. Inference: 0.1315 s/iter. Eval: 0.0062 s/iter. Total: 0.1383 s/iter. ETA=0:02:55
[01/13 18:14:47] detectron2.evaluation.evaluator INFO: Inference done 49/1276. Dataloading: 0.0019 s/iter. Inference: 0.1265 s/iter. Eval: 0.0064 s/iter. Total: 0.1348 s/iter. ETA=0:02:45
[01/13 18:14:52] detectron2.evaluation.evaluator INFO: Inference done 86/1276. Dataloading: 0.0017 s/iter. Inference: 0.1274 s/iter. Eval: 0.0062 s/iter. Total: 0.1354 s/iter. ETA=0:02:41
[01/13 18:14:57] detectron2.evaluation.evaluator INFO: Inference done 124/1276. Dataloading: 0.0017 s/iter. Inference: 0.1271 s/iter. Eval: 0.0061 s/iter. Total: 0.1350 s/iter. ETA=0:02:35
[01/13 18:15:02] detectron2.evaluation.evaluator INFO: Inference done 161/1276. Dataloading: 0.0017 s/iter. Inference: 0.1274 s/iter. Eval: 0.0061 s/iter. Total: 0.1352 s/iter. ETA=0:02:30
[01/13 18:15:07] detectron2.evaluation.evaluator INFO: Inference done 198/1276. Dataloading: 0.0017 s/iter. Inference: 0.1275 s/iter. Eval: 0.0061 s/iter. Total: 0.1353 s/iter. ETA=0:02:25
[01/13 18:15:13] detectron2.evaluation.evaluator INFO: Inference done 235/1276. Dataloading: 0.0017 s/iter. Inference: 0.1278 s/iter. Eval: 0.0061 s/iter. Total: 0.1356 s/iter. ETA=0:02:21
[01/13 18:15:18] detectron2.evaluation.evaluator INFO: Inference done 271/1276. Dataloading: 0.0017 s/iter. Inference: 0.1283 s/iter. Eval: 0.0061 s/iter. Total: 0.1361 s/iter. ETA=0:02:16
[01/13 18:15:23] detectron2.evaluation.evaluator INFO: Inference done 308/1276. Dataloading: 0.0017 s/iter. Inference: 0.1282 s/iter. Eval: 0.0061 s/iter. Total: 0.1360 s/iter. ETA=0:02:11
[01/13 18:15:28] detectron2.evaluation.evaluator INFO: Inference done 347/1276. Dataloading: 0.0017 s/iter. Inference: 0.1277 s/iter. Eval: 0.0061 s/iter. Total: 0.1355 s/iter. ETA=0:02:05
[01/13 18:15:33] detectron2.evaluation.evaluator INFO: Inference done 383/1276. Dataloading: 0.0017 s/iter. Inference: 0.1280 s/iter. Eval: 0.0061 s/iter. Total: 0.1358 s/iter. ETA=0:02:01
[01/13 18:15:38] detectron2.evaluation.evaluator INFO: Inference done 420/1276. Dataloading: 0.0017 s/iter. Inference: 0.1280 s/iter. Eval: 0.0061 s/iter. Total: 0.1358 s/iter. ETA=0:01:56
[01/13 18:15:43] detectron2.evaluation.evaluator INFO: Inference done 457/1276. Dataloading: 0.0017 s/iter. Inference: 0.1280 s/iter. Eval: 0.0061 s/iter. Total: 0.1358 s/iter. ETA=0:01:51
[01/13 18:15:48] detectron2.evaluation.evaluator INFO: Inference done 494/1276. Dataloading: 0.0018 s/iter. Inference: 0.1279 s/iter. Eval: 0.0061 s/iter. Total: 0.1358 s/iter. ETA=0:01:46
[01/13 18:15:53] detectron2.evaluation.evaluator INFO: Inference done 532/1276. Dataloading: 0.0018 s/iter. Inference: 0.1278 s/iter. Eval: 0.0061 s/iter. Total: 0.1357 s/iter. ETA=0:01:40
[01/13 18:15:58] detectron2.evaluation.evaluator INFO: Inference done 570/1276. Dataloading: 0.0018 s/iter. Inference: 0.1278 s/iter. Eval: 0.0061 s/iter. Total: 0.1357 s/iter. ETA=0:01:35
[01/13 18:16:03] detectron2.evaluation.evaluator INFO: Inference done 607/1276. Dataloading: 0.0018 s/iter. Inference: 0.1278 s/iter. Eval: 0.0061 s/iter. Total: 0.1357 s/iter. ETA=0:01:30
[01/13 18:16:08] detectron2.evaluation.evaluator INFO: Inference done 644/1276. Dataloading: 0.0018 s/iter. Inference: 0.1278 s/iter. Eval: 0.0061 s/iter. Total: 0.1358 s/iter. ETA=0:01:25
[01/13 18:16:13] detectron2.evaluation.evaluator INFO: Inference done 680/1276. Dataloading: 0.0018 s/iter. Inference: 0.1280 s/iter. Eval: 0.0061 s/iter. Total: 0.1360 s/iter. ETA=0:01:21
[01/13 18:16:18] detectron2.evaluation.evaluator INFO: Inference done 717/1276. Dataloading: 0.0018 s/iter. Inference: 0.1280 s/iter. Eval: 0.0061 s/iter. Total: 0.1360 s/iter. ETA=0:01:15
[01/13 18:16:23] detectron2.evaluation.evaluator INFO: Inference done 755/1276. Dataloading: 0.0018 s/iter. Inference: 0.1279 s/iter. Eval: 0.0061 s/iter. Total: 0.1359 s/iter. ETA=0:01:10
[01/13 18:16:28] detectron2.evaluation.evaluator INFO: Inference done 792/1276. Dataloading: 0.0018 s/iter. Inference: 0.1280 s/iter. Eval: 0.0061 s/iter. Total: 0.1360 s/iter. ETA=0:01:05
[01/13 18:16:33] detectron2.evaluation.evaluator INFO: Inference done 829/1276. Dataloading: 0.0018 s/iter. Inference: 0.1280 s/iter. Eval: 0.0061 s/iter. Total: 0.1360 s/iter. ETA=0:01:00
[01/13 18:16:38] detectron2.evaluation.evaluator INFO: Inference done 865/1276. Dataloading: 0.0018 s/iter. Inference: 0.1281 s/iter. Eval: 0.0062 s/iter. Total: 0.1361 s/iter. ETA=0:00:55
[01/13 18:16:44] detectron2.evaluation.evaluator INFO: Inference done 902/1276. Dataloading: 0.0018 s/iter. Inference: 0.1282 s/iter. Eval: 0.0062 s/iter. Total: 0.1362 s/iter. ETA=0:00:50
[01/13 18:16:49] detectron2.evaluation.evaluator INFO: Inference done 940/1276. Dataloading: 0.0018 s/iter. Inference: 0.1282 s/iter. Eval: 0.0062 s/iter. Total: 0.1361 s/iter. ETA=0:00:45
[01/13 18:16:54] detectron2.evaluation.evaluator INFO: Inference done 978/1276. Dataloading: 0.0018 s/iter. Inference: 0.1280 s/iter. Eval: 0.0061 s/iter. Total: 0.1360 s/iter. ETA=0:00:40
[01/13 18:16:59] detectron2.evaluation.evaluator INFO: Inference done 1015/1276. Dataloading: 0.0018 s/iter. Inference: 0.1281 s/iter. Eval: 0.0061 s/iter. Total: 0.1361 s/iter. ETA=0:00:35
[01/13 18:17:04] detectron2.evaluation.evaluator INFO: Inference done 1052/1276. Dataloading: 0.0018 s/iter. Inference: 0.1282 s/iter. Eval: 0.0061 s/iter. Total: 0.1361 s/iter. ETA=0:00:30
[01/13 18:17:09] detectron2.evaluation.evaluator INFO: Inference done 1089/1276. Dataloading: 0.0018 s/iter. Inference: 0.1282 s/iter. Eval: 0.0061 s/iter. Total: 0.1362 s/iter. ETA=0:00:25
[01/13 18:17:14] detectron2.evaluation.evaluator INFO: Inference done 1127/1276. Dataloading: 0.0018 s/iter. Inference: 0.1282 s/iter. Eval: 0.0061 s/iter. Total: 0.1362 s/iter. ETA=0:00:20
[01/13 18:17:19] detectron2.evaluation.evaluator INFO: Inference done 1164/1276. Dataloading: 0.0018 s/iter. Inference: 0.1282 s/iter. Eval: 0.0061 s/iter. Total: 0.1362 s/iter. ETA=0:00:15
[01/13 18:17:24] detectron2.evaluation.evaluator INFO: Inference done 1202/1276. Dataloading: 0.0018 s/iter. Inference: 0.1281 s/iter. Eval: 0.0061 s/iter. Total: 0.1361 s/iter. ETA=0:00:10
[01/13 18:17:29] detectron2.evaluation.evaluator INFO: Inference done 1239/1276. Dataloading: 0.0018 s/iter. Inference: 0.1281 s/iter. Eval: 0.0061 s/iter. Total: 0.1360 s/iter. ETA=0:00:05
[01/13 18:17:34] detectron2.evaluation.evaluator INFO: Inference done 1275/1276. Dataloading: 0.0018 s/iter. Inference: 0.1282 s/iter. Eval: 0.0061 s/iter. Total: 0.1361 s/iter. ETA=0:00:00
[01/13 18:17:35] detectron2.evaluation.evaluator INFO: Total inference time: 0:02:53.847930 (0.136780 s / iter per device, on 4 devices)
[01/13 18:17:35] detectron2.evaluation.evaluator INFO: Total inference pure compute time: 0:02:42 (0.128236 s / iter per device, on 4 devices)
[01/13 18:36:36] mask_former.data.dataset_mappers.mask_former_semantic_dataset_mapper INFO: [MaskFormerSemanticDatasetMapper] Augmentations used in inference: []
[01/13 18:36:36] detectron2.data.datasets.coco WARNING: Directory /scratch/local/ssd/anjun/datasets/VOCdevkit/VOC2010/JPEGImages and /scratch/local/ssd/anjun/datasets/VOCdevkit/VOC2010/annotations_detectron2/pc59_val has 11321 and 5104 files, respectively.
[01/13 18:36:36] detectron2.data.datasets.coco WARNING: Will use their intersection of 5104 files.
[01/13 18:36:36] detectron2.data.datasets.coco INFO: Loaded 5104 images with semantic segmentation from /scratch/local/ssd/anjun/datasets/VOCdevkit/VOC2010/JPEGImages
[01/13 18:36:36] detectron2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[01/13 18:36:36] detectron2.data.common INFO: Serializing 5104 elements to byte tensors and concatenating them all ...
[01/13 18:36:36] detectron2.data.common INFO: Serialized dataset takes 1.37 MiB
[01/13 18:36:36] detectron2.data.datasets.coco WARNING: Directory /scratch/local/ssd/anjun/datasets/VOCdevkit/VOC2010/JPEGImages and /scratch/local/ssd/anjun/datasets/VOCdevkit/VOC2010/annotations_detectron2/pc59_val has 11321 and 5104 files, respectively.
[01/13 18:36:36] detectron2.data.datasets.coco WARNING: Will use their intersection of 5104 files.
[01/13 18:36:36] detectron2.data.datasets.coco INFO: Loaded 5104 images with semantic segmentation from /scratch/local/ssd/anjun/datasets/VOCdevkit/VOC2010/JPEGImages
[01/13 18:36:36] detectron2.evaluation.evaluator INFO: Start inference on 1276 batches
[01/13 18:36:44] detectron2.evaluation.evaluator INFO: Inference done 11/1276. Dataloading: 0.0007 s/iter. Inference: 0.1315 s/iter. Eval: 0.0060 s/iter. Total: 0.1383 s/iter. ETA=0:02:54
[01/13 18:36:49] detectron2.evaluation.evaluator INFO: Inference done 49/1276. Dataloading: 0.0013 s/iter. Inference: 0.1265 s/iter. Eval: 0.0064 s/iter. Total: 0.1341 s/iter. ETA=0:02:44
[01/13 18:36:54] detectron2.evaluation.evaluator INFO: Inference done 86/1276. Dataloading: 0.0013 s/iter. Inference: 0.1274 s/iter. Eval: 0.0062 s/iter. Total: 0.1350 s/iter. ETA=0:02:40
[01/13 18:36:59] detectron2.evaluation.evaluator INFO: Inference done 124/1276. Dataloading: 0.0013 s/iter. Inference: 0.1270 s/iter. Eval: 0.0063 s/iter. Total: 0.1346 s/iter. ETA=0:02:35
[01/13 18:37:04] detectron2.evaluation.evaluator INFO: Inference done 161/1276. Dataloading: 0.0014 s/iter. Inference: 0.1272 s/iter. Eval: 0.0064 s/iter. Total: 0.1349 s/iter. ETA=0:02:30
[01/13 18:37:09] detectron2.evaluation.evaluator INFO: Inference done 197/1276. Dataloading: 0.0014 s/iter. Inference: 0.1278 s/iter. Eval: 0.0066 s/iter. Total: 0.1359 s/iter. ETA=0:02:26
[01/13 18:37:14] detectron2.evaluation.evaluator INFO: Inference done 234/1276. Dataloading: 0.0014 s/iter. Inference: 0.1280 s/iter. Eval: 0.0066 s/iter. Total: 0.1361 s/iter. ETA=0:02:21
[01/13 18:37:19] detectron2.evaluation.evaluator INFO: Inference done 271/1276. Dataloading: 0.0014 s/iter. Inference: 0.1284 s/iter. Eval: 0.0065 s/iter. Total: 0.1364 s/iter. ETA=0:02:17
[01/13 18:37:24] detectron2.evaluation.evaluator INFO: Inference done 309/1276. Dataloading: 0.0014 s/iter. Inference: 0.1282 s/iter. Eval: 0.0065 s/iter. Total: 0.1361 s/iter. ETA=0:02:11
[01/13 18:37:29] detectron2.evaluation.evaluator INFO: Inference done 348/1276. Dataloading: 0.0014 s/iter. Inference: 0.1275 s/iter. Eval: 0.0063 s/iter. Total: 0.1353 s/iter. ETA=0:02:05
[01/13 18:37:34] detectron2.evaluation.evaluator INFO: Inference done 385/1276. Dataloading: 0.0014 s/iter. Inference: 0.1278 s/iter. Eval: 0.0063 s/iter. Total: 0.1355 s/iter. ETA=0:02:00
[01/13 18:37:39] detectron2.evaluation.evaluator INFO: Inference done 422/1276. Dataloading: 0.0014 s/iter. Inference: 0.1278 s/iter. Eval: 0.0063 s/iter. Total: 0.1355 s/iter. ETA=0:01:55
[01/13 18:37:44] detectron2.evaluation.evaluator INFO: Inference done 459/1276. Dataloading: 0.0014 s/iter. Inference: 0.1278 s/iter. Eval: 0.0063 s/iter. Total: 0.1355 s/iter. ETA=0:01:50
[01/13 18:37:50] detectron2.evaluation.evaluator INFO: Inference done 497/1276. Dataloading: 0.0014 s/iter. Inference: 0.1277 s/iter. Eval: 0.0063 s/iter. Total: 0.1354 s/iter. ETA=0:01:45
[01/13 18:37:55] detectron2.evaluation.evaluator INFO: Inference done 535/1276. Dataloading: 0.0014 s/iter. Inference: 0.1275 s/iter. Eval: 0.0063 s/iter. Total: 0.1353 s/iter. ETA=0:01:40
[01/13 18:38:00] detectron2.evaluation.evaluator INFO: Inference done 573/1276. Dataloading: 0.0014 s/iter. Inference: 0.1274 s/iter. Eval: 0.0062 s/iter. Total: 0.1351 s/iter. ETA=0:01:34
[01/13 18:38:05] detectron2.evaluation.evaluator INFO: Inference done 610/1276. Dataloading: 0.0014 s/iter. Inference: 0.1275 s/iter. Eval: 0.0062 s/iter. Total: 0.1351 s/iter. ETA=0:01:30
[01/13 18:38:10] detectron2.evaluation.evaluator INFO: Inference done 648/1276. Dataloading: 0.0014 s/iter. Inference: 0.1275 s/iter. Eval: 0.0062 s/iter. Total: 0.1352 s/iter. ETA=0:01:24
[01/13 18:38:15] detectron2.evaluation.evaluator INFO: Inference done 684/1276. Dataloading: 0.0014 s/iter. Inference: 0.1277 s/iter. Eval: 0.0062 s/iter. Total: 0.1354 s/iter. ETA=0:01:20
[01/13 18:38:20] detectron2.evaluation.evaluator INFO: Inference done 721/1276. Dataloading: 0.0014 s/iter. Inference: 0.1277 s/iter. Eval: 0.0062 s/iter. Total: 0.1354 s/iter. ETA=0:01:15
[01/13 18:38:25] detectron2.evaluation.evaluator INFO: Inference done 759/1276. Dataloading: 0.0014 s/iter. Inference: 0.1276 s/iter. Eval: 0.0062 s/iter. Total: 0.1352 s/iter. ETA=0:01:09
[01/13 18:38:30] detectron2.evaluation.evaluator INFO: Inference done 796/1276. Dataloading: 0.0014 s/iter. Inference: 0.1277 s/iter. Eval: 0.0062 s/iter. Total: 0.1354 s/iter. ETA=0:01:04
[01/13 18:38:35] detectron2.evaluation.evaluator INFO: Inference done 834/1276. Dataloading: 0.0014 s/iter. Inference: 0.1277 s/iter. Eval: 0.0062 s/iter. Total: 0.1354 s/iter. ETA=0:00:59
[01/13 18:38:40] detectron2.evaluation.evaluator INFO: Inference done 871/1276. Dataloading: 0.0014 s/iter. Inference: 0.1278 s/iter. Eval: 0.0062 s/iter. Total: 0.1354 s/iter. ETA=0:00:54
[01/13 18:38:45] detectron2.evaluation.evaluator INFO: Inference done 908/1276. Dataloading: 0.0014 s/iter. Inference: 0.1278 s/iter. Eval: 0.0062 s/iter. Total: 0.1355 s/iter. ETA=0:00:49
[01/13 18:38:50] detectron2.evaluation.evaluator INFO: Inference done 946/1276. Dataloading: 0.0014 s/iter. Inference: 0.1277 s/iter. Eval: 0.0062 s/iter. Total: 0.1353 s/iter. ETA=0:00:44
[01/13 18:38:55] detectron2.evaluation.evaluator INFO: Inference done 984/1276. Dataloading: 0.0014 s/iter. Inference: 0.1277 s/iter. Eval: 0.0062 s/iter. Total: 0.1353 s/iter. ETA=0:00:39
[01/13 18:39:01] detectron2.evaluation.evaluator INFO: Inference done 1022/1276. Dataloading: 0.0014 s/iter. Inference: 0.1277 s/iter. Eval: 0.0062 s/iter. Total: 0.1353 s/iter. ETA=0:00:34
[01/13 18:39:06] detectron2.evaluation.evaluator INFO: Inference done 1059/1276. Dataloading: 0.0014 s/iter. Inference: 0.1277 s/iter. Eval: 0.0062 s/iter. Total: 0.1354 s/iter. ETA=0:00:29
[01/13 18:39:11] detectron2.evaluation.evaluator INFO: Inference done 1097/1276. Dataloading: 0.0014 s/iter. Inference: 0.1277 s/iter. Eval: 0.0061 s/iter. Total: 0.1353 s/iter. ETA=0:00:24
[01/13 18:39:16] detectron2.evaluation.evaluator INFO: Inference done 1135/1276. Dataloading: 0.0014 s/iter. Inference: 0.1277 s/iter. Eval: 0.0061 s/iter. Total: 0.1353 s/iter. ETA=0:00:19
[01/13 18:39:21] detectron2.evaluation.evaluator INFO: Inference done 1173/1276. Dataloading: 0.0014 s/iter. Inference: 0.1277 s/iter. Eval: 0.0061 s/iter. Total: 0.1353 s/iter. ETA=0:00:13
[01/13 18:39:26] detectron2.evaluation.evaluator INFO: Inference done 1212/1276. Dataloading: 0.0014 s/iter. Inference: 0.1276 s/iter. Eval: 0.0061 s/iter. Total: 0.1352 s/iter. ETA=0:00:08
[01/13 18:39:31] detectron2.evaluation.evaluator INFO: Inference done 1249/1276. Dataloading: 0.0014 s/iter. Inference: 0.1276 s/iter. Eval: 0.0061 s/iter. Total: 0.1352 s/iter. ETA=0:00:03
[01/13 18:39:36] detectron2.evaluation.evaluator INFO: Total inference time: 0:02:52.722051 (0.135895 s / iter per device, on 4 devices)
[01/13 18:39:36] detectron2.evaluation.evaluator INFO: Total inference pure compute time: 0:02:42 (0.127637 s / iter per device, on 4 devices)
[01/13 18:58:35] mask_former.data.dataset_mappers.mask_former_semantic_dataset_mapper INFO: [MaskFormerSemanticDatasetMapper] Augmentations used in inference: []
[01/13 18:58:35] detectron2.data.datasets.coco WARNING: Directory /scratch/local/ssd/anjun/datasets/VOCdevkit/VOC2010/JPEGImages and /scratch/local/ssd/anjun/datasets/VOCdevkit/VOC2010/annotations_detectron2/pc59_val has 11321 and 5104 files, respectively.
[01/13 18:58:35] detectron2.data.datasets.coco WARNING: Will use their intersection of 5104 files.
[01/13 18:58:35] detectron2.data.datasets.coco INFO: Loaded 5104 images with semantic segmentation from /scratch/local/ssd/anjun/datasets/VOCdevkit/VOC2010/JPEGImages
[01/13 18:58:35] detectron2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[01/13 18:58:35] detectron2.data.common INFO: Serializing 5104 elements to byte tensors and concatenating them all ...
[01/13 18:58:35] detectron2.data.common INFO: Serialized dataset takes 1.37 MiB
[01/13 18:58:35] detectron2.data.datasets.coco WARNING: Directory /scratch/local/ssd/anjun/datasets/VOCdevkit/VOC2010/JPEGImages and /scratch/local/ssd/anjun/datasets/VOCdevkit/VOC2010/annotations_detectron2/pc59_val has 11321 and 5104 files, respectively.
[01/13 18:58:35] detectron2.data.datasets.coco WARNING: Will use their intersection of 5104 files.
[01/13 18:58:35] detectron2.data.datasets.coco INFO: Loaded 5104 images with semantic segmentation from /scratch/local/ssd/anjun/datasets/VOCdevkit/VOC2010/JPEGImages
[01/13 18:58:35] detectron2.evaluation.evaluator INFO: Start inference on 1276 batches
[01/13 18:58:43] detectron2.evaluation.evaluator INFO: Inference done 11/1276. Dataloading: 0.0008 s/iter. Inference: 0.1339 s/iter. Eval: 0.0069 s/iter. Total: 0.1416 s/iter. ETA=0:02:59
[01/13 18:58:48] detectron2.evaluation.evaluator INFO: Inference done 49/1276. Dataloading: 0.0015 s/iter. Inference: 0.1278 s/iter. Eval: 0.0067 s/iter. Total: 0.1360 s/iter. ETA=0:02:46
[01/13 18:58:54] detectron2.evaluation.evaluator INFO: Inference done 86/1276. Dataloading: 0.0015 s/iter. Inference: 0.1288 s/iter. Eval: 0.0064 s/iter. Total: 0.1367 s/iter. ETA=0:02:42
[01/13 18:58:59] detectron2.evaluation.evaluator INFO: Inference done 124/1276. Dataloading: 0.0015 s/iter. Inference: 0.1282 s/iter. Eval: 0.0063 s/iter. Total: 0.1361 s/iter. ETA=0:02:36
[01/13 18:59:04] detectron2.evaluation.evaluator INFO: Inference done 161/1276. Dataloading: 0.0015 s/iter. Inference: 0.1283 s/iter. Eval: 0.0063 s/iter. Total: 0.1361 s/iter. ETA=0:02:31
[01/13 18:59:09] detectron2.evaluation.evaluator INFO: Inference done 199/1276. Dataloading: 0.0015 s/iter. Inference: 0.1279 s/iter. Eval: 0.0062 s/iter. Total: 0.1357 s/iter. ETA=0:02:26
[01/13 18:59:14] detectron2.evaluation.evaluator INFO: Inference done 236/1276. Dataloading: 0.0015 s/iter. Inference: 0.1281 s/iter. Eval: 0.0063 s/iter. Total: 0.1359 s/iter. ETA=0:02:21
[01/13 18:59:19] detectron2.evaluation.evaluator INFO: Inference done 272/1276. Dataloading: 0.0015 s/iter. Inference: 0.1286 s/iter. Eval: 0.0062 s/iter. Total: 0.1363 s/iter. ETA=0:02:16
[01/13 18:59:24] detectron2.evaluation.evaluator INFO: Inference done 310/1276. Dataloading: 0.0015 s/iter. Inference: 0.1283 s/iter. Eval: 0.0062 s/iter. Total: 0.1360 s/iter. ETA=0:02:11
[01/13 18:59:29] detectron2.evaluation.evaluator INFO: Inference done 349/1276. Dataloading: 0.0015 s/iter. Inference: 0.1277 s/iter. Eval: 0.0061 s/iter. Total: 0.1353 s/iter. ETA=0:02:05
[01/13 18:59:34] detectron2.evaluation.evaluator INFO: Inference done 386/1276. Dataloading: 0.0015 s/iter. Inference: 0.1279 s/iter. Eval: 0.0061 s/iter. Total: 0.1356 s/iter. ETA=0:02:00
[01/13 18:59:39] detectron2.evaluation.evaluator INFO: Inference done 423/1276. Dataloading: 0.0015 s/iter. Inference: 0.1281 s/iter. Eval: 0.0061 s/iter. Total: 0.1357 s/iter. ETA=0:01:55
[01/13 18:59:44] detectron2.evaluation.evaluator INFO: Inference done 461/1276. Dataloading: 0.0015 s/iter. Inference: 0.1280 s/iter. Eval: 0.0061 s/iter. Total: 0.1356 s/iter. ETA=0:01:50
[01/13 18:59:49] detectron2.evaluation.evaluator INFO: Inference done 499/1276. Dataloading: 0.0015 s/iter. Inference: 0.1277 s/iter. Eval: 0.0061 s/iter. Total: 0.1354 s/iter. ETA=0:01:45
[01/13 18:59:54] detectron2.evaluation.evaluator INFO: Inference done 536/1276. Dataloading: 0.0015 s/iter. Inference: 0.1277 s/iter. Eval: 0.0062 s/iter. Total: 0.1354 s/iter. ETA=0:01:40
[01/13 18:59:59] detectron2.evaluation.evaluator INFO: Inference done 574/1276. Dataloading: 0.0015 s/iter. Inference: 0.1275 s/iter. Eval: 0.0062 s/iter. Total: 0.1352 s/iter. ETA=0:01:34
[01/13 19:00:04] detectron2.evaluation.evaluator INFO: Inference done 611/1276. Dataloading: 0.0015 s/iter. Inference: 0.1275 s/iter. Eval: 0.0062 s/iter. Total: 0.1352 s/iter. ETA=0:01:29
[01/13 19:00:09] detectron2.evaluation.evaluator INFO: Inference done 648/1276. Dataloading: 0.0015 s/iter. Inference: 0.1276 s/iter. Eval: 0.0062 s/iter. Total: 0.1353 s/iter. ETA=0:01:24
[01/13 19:00:15] detectron2.evaluation.evaluator INFO: Inference done 685/1276. Dataloading: 0.0015 s/iter. Inference: 0.1277 s/iter. Eval: 0.0062 s/iter. Total: 0.1354 s/iter. ETA=0:01:20
[01/13 19:00:20] detectron2.evaluation.evaluator INFO: Inference done 723/1276. Dataloading: 0.0015 s/iter. Inference: 0.1277 s/iter. Eval: 0.0062 s/iter. Total: 0.1353 s/iter. ETA=0:01:14
[01/13 19:00:25] detectron2.evaluation.evaluator INFO: Inference done 761/1276. Dataloading: 0.0014 s/iter. Inference: 0.1276 s/iter. Eval: 0.0061 s/iter. Total: 0.1352 s/iter. ETA=0:01:09
[01/13 19:00:30] detectron2.evaluation.evaluator INFO: Inference done 798/1276. Dataloading: 0.0014 s/iter. Inference: 0.1276 s/iter. Eval: 0.0061 s/iter. Total: 0.1352 s/iter. ETA=0:01:04
[01/13 19:00:35] detectron2.evaluation.evaluator INFO: Inference done 836/1276. Dataloading: 0.0014 s/iter. Inference: 0.1276 s/iter. Eval: 0.0061 s/iter. Total: 0.1352 s/iter. ETA=0:00:59
[01/13 19:00:40] detectron2.evaluation.evaluator INFO: Inference done 873/1276. Dataloading: 0.0014 s/iter. Inference: 0.1277 s/iter. Eval: 0.0061 s/iter. Total: 0.1353 s/iter. ETA=0:00:54
[01/13 19:00:45] detectron2.evaluation.evaluator INFO: Inference done 910/1276. Dataloading: 0.0014 s/iter. Inference: 0.1278 s/iter. Eval: 0.0061 s/iter. Total: 0.1354 s/iter. ETA=0:00:49
[01/13 19:00:50] detectron2.evaluation.evaluator INFO: Inference done 948/1276. Dataloading: 0.0015 s/iter. Inference: 0.1276 s/iter. Eval: 0.0061 s/iter. Total: 0.1352 s/iter. ETA=0:00:44
[01/13 19:00:55] detectron2.evaluation.evaluator INFO: Inference done 986/1276. Dataloading: 0.0015 s/iter. Inference: 0.1277 s/iter. Eval: 0.0061 s/iter. Total: 0.1352 s/iter. ETA=0:00:39
[01/13 19:01:00] detectron2.evaluation.evaluator INFO: Inference done 1023/1276. Dataloading: 0.0015 s/iter. Inference: 0.1277 s/iter. Eval: 0.0061 s/iter. Total: 0.1353 s/iter. ETA=0:00:34
[01/13 19:01:05] detectron2.evaluation.evaluator INFO: Inference done 1059/1276. Dataloading: 0.0015 s/iter. Inference: 0.1278 s/iter. Eval: 0.0061 s/iter. Total: 0.1354 s/iter. ETA=0:00:29
[01/13 19:01:10] detectron2.evaluation.evaluator INFO: Inference done 1096/1276. Dataloading: 0.0015 s/iter. Inference: 0.1279 s/iter. Eval: 0.0061 s/iter. Total: 0.1354 s/iter. ETA=0:00:24
[01/13 19:01:15] detectron2.evaluation.evaluator INFO: Inference done 1134/1276. Dataloading: 0.0015 s/iter. Inference: 0.1278 s/iter. Eval: 0.0061 s/iter. Total: 0.1354 s/iter. ETA=0:00:19
[01/13 19:01:20] detectron2.evaluation.evaluator INFO: Inference done 1171/1276. Dataloading: 0.0015 s/iter. Inference: 0.1279 s/iter. Eval: 0.0061 s/iter. Total: 0.1355 s/iter. ETA=0:00:14
[01/13 19:01:26] detectron2.evaluation.evaluator INFO: Inference done 1210/1276. Dataloading: 0.0015 s/iter. Inference: 0.1277 s/iter. Eval: 0.0061 s/iter. Total: 0.1353 s/iter. ETA=0:00:08
[01/13 19:01:31] detectron2.evaluation.evaluator INFO: Inference done 1247/1276. Dataloading: 0.0015 s/iter. Inference: 0.1278 s/iter. Eval: 0.0061 s/iter. Total: 0.1353 s/iter. ETA=0:00:03
[01/13 19:01:35] detectron2.evaluation.evaluator INFO: Total inference time: 0:02:52.891535 (0.136028 s / iter per device, on 4 devices)
[01/13 19:01:35] detectron2.evaluation.evaluator INFO: Total inference pure compute time: 0:02:42 (0.127832 s / iter per device, on 4 devices)
[01/13 19:20:35] mask_former.data.dataset_mappers.mask_former_semantic_dataset_mapper INFO: [MaskFormerSemanticDatasetMapper] Augmentations used in inference: []
[01/13 19:20:35] detectron2.data.datasets.coco WARNING: Directory /scratch/local/ssd/anjun/datasets/VOCdevkit/VOC2010/JPEGImages and /scratch/local/ssd/anjun/datasets/VOCdevkit/VOC2010/annotations_detectron2/pc59_val has 11321 and 5104 files, respectively.
[01/13 19:20:35] detectron2.data.datasets.coco WARNING: Will use their intersection of 5104 files.
[01/13 19:20:35] detectron2.data.datasets.coco INFO: Loaded 5104 images with semantic segmentation from /scratch/local/ssd/anjun/datasets/VOCdevkit/VOC2010/JPEGImages
[01/13 19:20:35] detectron2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[01/13 19:20:35] detectron2.data.common INFO: Serializing 5104 elements to byte tensors and concatenating them all ...
[01/13 19:20:35] detectron2.data.common INFO: Serialized dataset takes 1.37 MiB
[01/13 19:20:35] detectron2.data.datasets.coco WARNING: Directory /scratch/local/ssd/anjun/datasets/VOCdevkit/VOC2010/JPEGImages and /scratch/local/ssd/anjun/datasets/VOCdevkit/VOC2010/annotations_detectron2/pc59_val has 11321 and 5104 files, respectively.
[01/13 19:20:35] detectron2.data.datasets.coco WARNING: Will use their intersection of 5104 files.
[01/13 19:20:35] detectron2.data.datasets.coco INFO: Loaded 5104 images with semantic segmentation from /scratch/local/ssd/anjun/datasets/VOCdevkit/VOC2010/JPEGImages
[01/13 19:20:35] detectron2.evaluation.evaluator INFO: Start inference on 1276 batches
[01/13 19:20:44] detectron2.evaluation.evaluator INFO: Inference done 11/1276. Dataloading: 0.0007 s/iter. Inference: 0.1317 s/iter. Eval: 0.0061 s/iter. Total: 0.1386 s/iter. ETA=0:02:55
[01/13 19:20:49] detectron2.evaluation.evaluator INFO: Inference done 49/1276. Dataloading: 0.0018 s/iter. Inference: 0.1266 s/iter. Eval: 0.0064 s/iter. Total: 0.1348 s/iter. ETA=0:02:45
[01/13 19:20:54] detectron2.evaluation.evaluator INFO: Inference done 86/1276. Dataloading: 0.0019 s/iter. Inference: 0.1275 s/iter. Eval: 0.0063 s/iter. Total: 0.1357 s/iter. ETA=0:02:41
[01/13 19:20:59] detectron2.evaluation.evaluator INFO: Inference done 124/1276. Dataloading: 0.0018 s/iter. Inference: 0.1272 s/iter. Eval: 0.0061 s/iter. Total: 0.1351 s/iter. ETA=0:02:35
[01/13 19:21:04] detectron2.evaluation.evaluator INFO: Inference done 162/1276. Dataloading: 0.0017 s/iter. Inference: 0.1273 s/iter. Eval: 0.0061 s/iter. Total: 0.1352 s/iter. ETA=0:02:30
[01/13 19:21:09] detectron2.evaluation.evaluator INFO: Inference done 200/1276. Dataloading: 0.0017 s/iter. Inference: 0.1269 s/iter. Eval: 0.0061 s/iter. Total: 0.1348 s/iter. ETA=0:02:25
[01/13 19:21:14] detectron2.evaluation.evaluator INFO: Inference done 237/1276. Dataloading: 0.0017 s/iter. Inference: 0.1272 s/iter. Eval: 0.0061 s/iter. Total: 0.1351 s/iter. ETA=0:02:20
[01/13 19:21:19] detectron2.evaluation.evaluator INFO: Inference done 273/1276. Dataloading: 0.0017 s/iter. Inference: 0.1278 s/iter. Eval: 0.0062 s/iter. Total: 0.1357 s/iter. ETA=0:02:16
[01/13 19:21:24] detectron2.evaluation.evaluator INFO: Inference done 311/1276. Dataloading: 0.0017 s/iter. Inference: 0.1276 s/iter. Eval: 0.0061 s/iter. Total: 0.1355 s/iter. ETA=0:02:10
[01/13 19:21:29] detectron2.evaluation.evaluator INFO: Inference done 350/1276. Dataloading: 0.0017 s/iter. Inference: 0.1272 s/iter. Eval: 0.0061 s/iter. Total: 0.1351 s/iter. ETA=0:02:05
[01/13 19:21:34] detectron2.evaluation.evaluator INFO: Inference done 387/1276. Dataloading: 0.0017 s/iter. Inference: 0.1274 s/iter. Eval: 0.0061 s/iter. Total: 0.1353 s/iter. ETA=0:02:00
[01/13 19:21:40] detectron2.evaluation.evaluator INFO: Inference done 425/1276. Dataloading: 0.0018 s/iter. Inference: 0.1274 s/iter. Eval: 0.0061 s/iter. Total: 0.1353 s/iter. ETA=0:01:55
[01/13 19:21:45] detectron2.evaluation.evaluator INFO: Inference done 463/1276. Dataloading: 0.0018 s/iter. Inference: 0.1273 s/iter. Eval: 0.0061 s/iter. Total: 0.1351 s/iter. ETA=0:01:49
[01/13 19:21:50] detectron2.evaluation.evaluator INFO: Inference done 501/1276. Dataloading: 0.0018 s/iter. Inference: 0.1271 s/iter. Eval: 0.0061 s/iter. Total: 0.1350 s/iter. ETA=0:01:44
[01/13 19:21:55] detectron2.evaluation.evaluator INFO: Inference done 538/1276. Dataloading: 0.0018 s/iter. Inference: 0.1271 s/iter. Eval: 0.0061 s/iter. Total: 0.1350 s/iter. ETA=0:01:39
[01/13 19:22:00] detectron2.evaluation.evaluator INFO: Inference done 576/1276. Dataloading: 0.0018 s/iter. Inference: 0.1270 s/iter. Eval: 0.0061 s/iter. Total: 0.1349 s/iter. ETA=0:01:34
[01/13 19:22:05] detectron2.evaluation.evaluator INFO: Inference done 614/1276. Dataloading: 0.0018 s/iter. Inference: 0.1270 s/iter. Eval: 0.0061 s/iter. Total: 0.1349 s/iter. ETA=0:01:29
[01/13 19:22:10] detectron2.evaluation.evaluator INFO: Inference done 652/1276. Dataloading: 0.0017 s/iter. Inference: 0.1270 s/iter. Eval: 0.0061 s/iter. Total: 0.1349 s/iter. ETA=0:01:24
[01/13 19:22:15] detectron2.evaluation.evaluator INFO: Inference done 689/1276. Dataloading: 0.0017 s/iter. Inference: 0.1271 s/iter. Eval: 0.0061 s/iter. Total: 0.1350 s/iter. ETA=0:01:19
[01/13 19:22:20] detectron2.evaluation.evaluator INFO: Inference done 726/1276. Dataloading: 0.0017 s/iter. Inference: 0.1272 s/iter. Eval: 0.0061 s/iter. Total: 0.1351 s/iter. ETA=0:01:14
[01/13 19:22:25] detectron2.evaluation.evaluator INFO: Inference done 764/1276. Dataloading: 0.0017 s/iter. Inference: 0.1271 s/iter. Eval: 0.0061 s/iter. Total: 0.1350 s/iter. ETA=0:01:09
[01/13 19:22:30] detectron2.evaluation.evaluator INFO: Inference done 801/1276. Dataloading: 0.0017 s/iter. Inference: 0.1272 s/iter. Eval: 0.0061 s/iter. Total: 0.1351 s/iter. ETA=0:01:04
[01/13 19:22:35] detectron2.evaluation.evaluator INFO: Inference done 838/1276. Dataloading: 0.0018 s/iter. Inference: 0.1272 s/iter. Eval: 0.0061 s/iter. Total: 0.1351 s/iter. ETA=0:00:59
[01/13 19:22:40] detectron2.evaluation.evaluator INFO: Inference done 875/1276. Dataloading: 0.0018 s/iter. Inference: 0.1272 s/iter. Eval: 0.0061 s/iter. Total: 0.1351 s/iter. ETA=0:00:54
[01/13 19:22:45] detectron2.evaluation.evaluator INFO: Inference done 912/1276. Dataloading: 0.0018 s/iter. Inference: 0.1273 s/iter. Eval: 0.0061 s/iter. Total: 0.1351 s/iter. ETA=0:00:49
[01/13 19:22:50] detectron2.evaluation.evaluator INFO: Inference done 949/1276. Dataloading: 0.0018 s/iter. Inference: 0.1272 s/iter. Eval: 0.0061 s/iter. Total: 0.1351 s/iter. ETA=0:00:44
[01/13 19:22:55] detectron2.evaluation.evaluator INFO: Inference done 987/1276. Dataloading: 0.0018 s/iter. Inference: 0.1273 s/iter. Eval: 0.0061 s/iter. Total: 0.1351 s/iter. ETA=0:00:39
[01/13 19:23:00] detectron2.evaluation.evaluator INFO: Inference done 1024/1276. Dataloading: 0.0018 s/iter. Inference: 0.1273 s/iter. Eval: 0.0061 s/iter. Total: 0.1351 s/iter. ETA=0:00:34
[01/13 19:23:06] detectron2.evaluation.evaluator INFO: Inference done 1060/1276. Dataloading: 0.0018 s/iter. Inference: 0.1274 s/iter. Eval: 0.0061 s/iter. Total: 0.1353 s/iter. ETA=0:00:29
[01/13 19:23:11] detectron2.evaluation.evaluator INFO: Inference done 1097/1276. Dataloading: 0.0018 s/iter. Inference: 0.1274 s/iter. Eval: 0.0061 s/iter. Total: 0.1353 s/iter. ETA=0:00:24
[01/13 19:23:16] detectron2.evaluation.evaluator INFO: Inference done 1135/1276. Dataloading: 0.0018 s/iter. Inference: 0.1273 s/iter. Eval: 0.0061 s/iter. Total: 0.1352 s/iter. ETA=0:00:19
[01/13 19:23:21] detectron2.evaluation.evaluator INFO: Inference done 1173/1276. Dataloading: 0.0018 s/iter. Inference: 0.1274 s/iter. Eval: 0.0060 s/iter. Total: 0.1352 s/iter. ETA=0:00:13
[01/13 19:23:26] detectron2.evaluation.evaluator INFO: Inference done 1211/1276. Dataloading: 0.0018 s/iter. Inference: 0.1273 s/iter. Eval: 0.0060 s/iter. Total: 0.1351 s/iter. ETA=0:00:08
[01/13 19:23:31] detectron2.evaluation.evaluator INFO: Inference done 1248/1276. Dataloading: 0.0018 s/iter. Inference: 0.1273 s/iter. Eval: 0.0060 s/iter. Total: 0.1352 s/iter. ETA=0:00:03
[01/13 19:23:35] detectron2.evaluation.evaluator INFO: Total inference time: 0:02:52.601484 (0.135800 s / iter per device, on 4 devices)
[01/13 19:23:35] detectron2.evaluation.evaluator INFO: Total inference pure compute time: 0:02:41 (0.127366 s / iter per device, on 4 devices)
[01/13 19:42:34] mask_former.data.dataset_mappers.mask_former_semantic_dataset_mapper INFO: [MaskFormerSemanticDatasetMapper] Augmentations used in inference: []
[01/13 19:42:34] detectron2.data.datasets.coco WARNING: Directory /scratch/local/ssd/anjun/datasets/VOCdevkit/VOC2010/JPEGImages and /scratch/local/ssd/anjun/datasets/VOCdevkit/VOC2010/annotations_detectron2/pc59_val has 11321 and 5104 files, respectively.
[01/13 19:42:34] detectron2.data.datasets.coco WARNING: Will use their intersection of 5104 files.
[01/13 19:42:34] detectron2.data.datasets.coco INFO: Loaded 5104 images with semantic segmentation from /scratch/local/ssd/anjun/datasets/VOCdevkit/VOC2010/JPEGImages
[01/13 19:42:34] detectron2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[01/13 19:42:34] detectron2.data.common INFO: Serializing 5104 elements to byte tensors and concatenating them all ...
[01/13 19:42:34] detectron2.data.common INFO: Serialized dataset takes 1.37 MiB
[01/13 19:42:34] detectron2.data.datasets.coco WARNING: Directory /scratch/local/ssd/anjun/datasets/VOCdevkit/VOC2010/JPEGImages and /scratch/local/ssd/anjun/datasets/VOCdevkit/VOC2010/annotations_detectron2/pc59_val has 11321 and 5104 files, respectively.
[01/13 19:42:34] detectron2.data.datasets.coco WARNING: Will use their intersection of 5104 files.
[01/13 19:42:34] detectron2.data.datasets.coco INFO: Loaded 5104 images with semantic segmentation from /scratch/local/ssd/anjun/datasets/VOCdevkit/VOC2010/JPEGImages
[01/13 19:42:34] detectron2.evaluation.evaluator INFO: Start inference on 1276 batches
[01/13 19:42:42] detectron2.evaluation.evaluator INFO: Inference done 11/1276. Dataloading: 0.0007 s/iter. Inference: 0.1321 s/iter. Eval: 0.0061 s/iter. Total: 0.1389 s/iter. ETA=0:02:55
[01/13 19:42:47] detectron2.evaluation.evaluator INFO: Inference done 49/1276. Dataloading: 0.0015 s/iter. Inference: 0.1271 s/iter. Eval: 0.0063 s/iter. Total: 0.1349 s/iter. ETA=0:02:45
[01/13 19:42:52] detectron2.evaluation.evaluator INFO: Inference done 86/1276. Dataloading: 0.0016 s/iter. Inference: 0.1278 s/iter. Eval: 0.0062 s/iter. Total: 0.1356 s/iter. ETA=0:02:41
[01/13 19:42:57] detectron2.evaluation.evaluator INFO: Inference done 124/1276. Dataloading: 0.0016 s/iter. Inference: 0.1271 s/iter. Eval: 0.0061 s/iter. Total: 0.1348 s/iter. ETA=0:02:35
[01/13 19:43:02] detectron2.evaluation.evaluator INFO: Inference done 162/1276. Dataloading: 0.0015 s/iter. Inference: 0.1273 s/iter. Eval: 0.0061 s/iter. Total: 0.1350 s/iter. ETA=0:02:30
[01/13 19:43:07] detectron2.evaluation.evaluator INFO: Inference done 200/1276. Dataloading: 0.0016 s/iter. Inference: 0.1271 s/iter. Eval: 0.0061 s/iter. Total: 0.1348 s/iter. ETA=0:02:25
[01/13 19:43:13] detectron2.evaluation.evaluator INFO: Inference done 237/1276. Dataloading: 0.0016 s/iter. Inference: 0.1272 s/iter. Eval: 0.0061 s/iter. Total: 0.1350 s/iter. ETA=0:02:20
[01/13 19:43:18] detectron2.evaluation.evaluator INFO: Inference done 273/1276. Dataloading: 0.0016 s/iter. Inference: 0.1278 s/iter. Eval: 0.0061 s/iter. Total: 0.1356 s/iter. ETA=0:02:15
[01/13 19:43:23] detectron2.evaluation.evaluator INFO: Inference done 311/1276. Dataloading: 0.0016 s/iter. Inference: 0.1276 s/iter. Eval: 0.0061 s/iter. Total: 0.1354 s/iter. ETA=0:02:10
[01/13 19:43:28] detectron2.evaluation.evaluator INFO: Inference done 350/1276. Dataloading: 0.0016 s/iter. Inference: 0.1271 s/iter. Eval: 0.0061 s/iter. Total: 0.1349 s/iter. ETA=0:02:04
[01/13 19:43:33] detectron2.evaluation.evaluator INFO: Inference done 387/1276. Dataloading: 0.0016 s/iter. Inference: 0.1273 s/iter. Eval: 0.0062 s/iter. Total: 0.1351 s/iter. ETA=0:02:00
[01/13 19:43:38] detectron2.evaluation.evaluator INFO: Inference done 425/1276. Dataloading: 0.0016 s/iter. Inference: 0.1273 s/iter. Eval: 0.0061 s/iter. Total: 0.1351 s/iter. ETA=0:01:54
[01/13 19:43:43] detectron2.evaluation.evaluator INFO: Inference done 463/1276. Dataloading: 0.0016 s/iter. Inference: 0.1272 s/iter. Eval: 0.0061 s/iter. Total: 0.1350 s/iter. ETA=0:01:49
[01/13 19:43:48] detectron2.evaluation.evaluator INFO: Inference done 501/1276. Dataloading: 0.0016 s/iter. Inference: 0.1271 s/iter. Eval: 0.0061 s/iter. Total: 0.1349 s/iter. ETA=0:01:44
[01/13 19:43:53] detectron2.evaluation.evaluator INFO: Inference done 538/1276. Dataloading: 0.0017 s/iter. Inference: 0.1271 s/iter. Eval: 0.0061 s/iter. Total: 0.1350 s/iter. ETA=0:01:39
[01/13 19:43:58] detectron2.evaluation.evaluator INFO: Inference done 576/1276. Dataloading: 0.0017 s/iter. Inference: 0.1270 s/iter. Eval: 0.0062 s/iter. Total: 0.1349 s/iter. ETA=0:01:34
[01/13 19:44:03] detectron2.evaluation.evaluator INFO: Inference done 613/1276. Dataloading: 0.0017 s/iter. Inference: 0.1271 s/iter. Eval: 0.0062 s/iter. Total: 0.1350 s/iter. ETA=0:01:29
[01/13 19:44:08] detectron2.evaluation.evaluator INFO: Inference done 650/1276. Dataloading: 0.0017 s/iter. Inference: 0.1271 s/iter. Eval: 0.0062 s/iter. Total: 0.1350 s/iter. ETA=0:01:24
[01/13 19:44:13] detectron2.evaluation.evaluator INFO: Inference done 687/1276. Dataloading: 0.0017 s/iter. Inference: 0.1273 s/iter. Eval: 0.0062 s/iter. Total: 0.1352 s/iter. ETA=0:01:19
[01/13 19:44:19] detectron2.evaluation.evaluator INFO: Inference done 723/1276. Dataloading: 0.0017 s/iter. Inference: 0.1276 s/iter. Eval: 0.0062 s/iter. Total: 0.1356 s/iter. ETA=0:01:14
[01/13 19:44:24] detectron2.evaluation.evaluator INFO: Inference done 760/1276. Dataloading: 0.0017 s/iter. Inference: 0.1276 s/iter. Eval: 0.0062 s/iter. Total: 0.1356 s/iter. ETA=0:01:09
[01/13 19:44:29] detectron2.evaluation.evaluator INFO: Inference done 797/1276. Dataloading: 0.0017 s/iter. Inference: 0.1277 s/iter. Eval: 0.0062 s/iter. Total: 0.1357 s/iter. ETA=0:01:05
[01/13 19:44:34] detectron2.evaluation.evaluator INFO: Inference done 835/1276. Dataloading: 0.0017 s/iter. Inference: 0.1277 s/iter. Eval: 0.0062 s/iter. Total: 0.1357 s/iter. ETA=0:00:59
[01/13 19:44:39] detectron2.evaluation.evaluator INFO: Inference done 867/1276. Dataloading: 0.0021 s/iter. Inference: 0.1282 s/iter. Eval: 0.0063 s/iter. Total: 0.1366 s/iter. ETA=0:00:55
[01/13 19:44:44] detectron2.evaluation.evaluator INFO: Inference done 900/1276. Dataloading: 0.0022 s/iter. Inference: 0.1287 s/iter. Eval: 0.0063 s/iter. Total: 0.1373 s/iter. ETA=0:00:51
[01/13 19:44:49] detectron2.evaluation.evaluator INFO: Inference done 934/1276. Dataloading: 0.0022 s/iter. Inference: 0.1293 s/iter. Eval: 0.0064 s/iter. Total: 0.1379 s/iter. ETA=0:00:47
[01/13 19:44:55] detectron2.evaluation.evaluator INFO: Inference done 958/1276. Dataloading: 0.0022 s/iter. Inference: 0.1313 s/iter. Eval: 0.0064 s/iter. Total: 0.1399 s/iter. ETA=0:00:44
[01/13 19:45:00] detectron2.evaluation.evaluator INFO: Inference done 985/1276. Dataloading: 0.0022 s/iter. Inference: 0.1326 s/iter. Eval: 0.0064 s/iter. Total: 0.1411 s/iter. ETA=0:00:41
[01/13 19:45:05] detectron2.evaluation.evaluator INFO: Inference done 1022/1276. Dataloading: 0.0021 s/iter. Inference: 0.1324 s/iter. Eval: 0.0064 s/iter. Total: 0.1410 s/iter. ETA=0:00:35
[01/13 19:45:10] detectron2.evaluation.evaluator INFO: Inference done 1059/1276. Dataloading: 0.0021 s/iter. Inference: 0.1323 s/iter. Eval: 0.0064 s/iter. Total: 0.1408 s/iter. ETA=0:00:30
[01/13 19:45:15] detectron2.evaluation.evaluator INFO: Inference done 1097/1276. Dataloading: 0.0021 s/iter. Inference: 0.1322 s/iter. Eval: 0.0063 s/iter. Total: 0.1406 s/iter. ETA=0:00:25
[01/13 19:45:20] detectron2.evaluation.evaluator INFO: Inference done 1135/1276. Dataloading: 0.0021 s/iter. Inference: 0.1320 s/iter. Eval: 0.0063 s/iter. Total: 0.1404 s/iter. ETA=0:00:19
[01/13 19:45:25] detectron2.evaluation.evaluator INFO: Inference done 1173/1276. Dataloading: 0.0021 s/iter. Inference: 0.1318 s/iter. Eval: 0.0063 s/iter. Total: 0.1402 s/iter. ETA=0:00:14
[01/13 19:45:30] detectron2.evaluation.evaluator INFO: Inference done 1212/1276. Dataloading: 0.0021 s/iter. Inference: 0.1315 s/iter. Eval: 0.0063 s/iter. Total: 0.1399 s/iter. ETA=0:00:08
[01/13 19:45:35] detectron2.evaluation.evaluator INFO: Inference done 1249/1276. Dataloading: 0.0021 s/iter. Inference: 0.1314 s/iter. Eval: 0.0063 s/iter. Total: 0.1398 s/iter. ETA=0:00:03
[01/13 19:45:40] detectron2.evaluation.evaluator INFO: Total inference time: 0:02:58.479717 (0.140425 s / iter per device, on 4 devices)
[01/13 19:45:40] detectron2.evaluation.evaluator INFO: Total inference pure compute time: 0:02:47 (0.131423 s / iter per device, on 4 devices)
[01/13 19:55:46] detectron2.engine.hooks INFO: Overall training speed: 8522 iterations in 2:40:50 (1.1324 s / it)
[01/13 19:55:46] detectron2.engine.hooks INFO: Total training time: 3:05:58 (0:25:08 on hooks)
