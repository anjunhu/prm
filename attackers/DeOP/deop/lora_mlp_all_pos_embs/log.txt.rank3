[01/13 16:23:07] detectron2 INFO: Rank of current process: 3. World size: 4
[01/13 16:23:13] detectron2 INFO: Environment info:
-------------------------------  --------------------------------------------------------------------------------------------------
sys.platform                     linux
Python                           3.10.11 (main, May 16 2023, 00:28:57) [GCC 11.2.0]
numpy                            1.26.2
detectron2                       0.6 @/homes/55/anjun/.local/lib/python3.10/site-packages/detectron2
Compiler                         GCC 8.3
CUDA compiler                    CUDA 12.1
detectron2 arch flags            8.6
DETECTRON2_ENV_MODULE            <not set>
PyTorch                          2.1.1+cu121 @/scratch/local/ssd/anjun/anaconda3/envs/ldm/lib/python3.10/site-packages/torch
PyTorch debug build              False
torch._C._GLIBCXX_USE_CXX11_ABI  False
GPU available                    Yes
GPU 0,1,2,3                      NVIDIA A40 (arch=8.6)
Driver version                   530.30.02
CUDA_HOME                        /usr/local/cuda-12.1/
Pillow                           9.5.0
torchvision                      0.16.1+cu121 @/scratch/local/ssd/anjun/anaconda3/envs/ldm/lib/python3.10/site-packages/torchvision
torchvision arch flags           5.0, 6.0, 7.0, 7.5, 8.0, 8.6, 9.0
fvcore                           0.1.5.post20221221
iopath                           0.1.9
cv2                              4.5.4-dev
-------------------------------  --------------------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v3.1.1 (Git Hash 64f6bcbcbab628e96f33a62c3e975f8535a7bde4)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 12.1
  - NVCC architecture flags: -gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.9.2
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=12.1, CUDNN_VERSION=8.9.2, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=old-style-cast -Wno-invalid-partial-specialization -Wno-unused-private-field -Wno-aligned-allocation-unavailable -Wno-missing-braces -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.1.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[01/13 16:23:13] detectron2 INFO: Command line arguments: Namespace(config_file='configs/coco-stuff-164k-156/zero_shot_maskformer_R101c_bs32_60k_proposalmask_featupsample_img512.yaml', resume=True, eval_only=False, num_gpus=4, num_machines=1, machine_rank=0, dist_url='tcp://0.0.0.0:12237', opts=['MODEL.CLIP_ADAPTER.CLIP_ENSEMBLE', 'False', 'MODEL.CLIP_ADAPTER.CLIP_ENSEMBLE_WEIGHT', '-1.0', 'SOLVER.TEST_IMS_PER_BATCH', '1', 'OUTPUT_DIR', 'deop/train_log', 'MODEL.CLIP_ADAPTER.PROMPT_LEARNER', 'learnable', 'SOLVER.IMS_PER_BATCH', '16', 'SOLVER.BASE_LR', '0.00005', 'SOLVER.CHECKPOINT_PERIOD', '1000', 'MODEL.CLIP_ADAPTER.CLIP_MODEL_NAME', 'ViT-B/16', 'MODEL.WEIGHTS', 'pretrained_models/deop_model_final.pth', 'MODEL.META_ARCHITECTURE', 'ZeroShotClipfeatUpsample_addnorm_vit16Query2D_maskvit', 'MODEL.NUM_DECODER_LAYER', '1', 'ORACLE', 'False', 'INPUT.MIN_SIZE_TEST', '512', 'TEST.EVAL_PERIOD', '1000', 'INPUT.SIZE_DIVISIBILITY', '512', 'MODEL.MASK_FORMER.DECODER_DICE_WEIGHT', '0.8', 'MODEL.MASK_FORMER.DECODER_CE_WEIGHT', '2.0', 'MODEL.CLIP_ADAPTER.LEARN_POSITION', 'True', 'MODEL.CLIP_ADAPTER.POSITION_LAYERS', '[1,2,3,4,5,6,7,8,9,10,11]', 'MODEL.CLIP_ADAPTER.LAYERMASKVIT', '[11]', 'MODEL.CLIP_ADAPTER.END2ENDTRAIN', 'False', 'MODEL.CLIP_ADAPTER.PS_SHORTCUT', '1.0', 'DATASETS.TRAIN', "('coco_2017_train_stuff_all_sem_seg',)", 'DATASETS.TEST', "('context_59_test_sem_seg',)", 'MODEL.SEM_SEG_HEAD.NUM_CLASSES', '171', 'MODEL.MASK_FORMER.LOSS_NUM_CLASS', '171'])
[01/13 16:23:13] detectron2 INFO: Contents of args.config_file=configs/coco-stuff-164k-156/zero_shot_maskformer_R101c_bs32_60k_proposalmask_featupsample_img512.yaml:
[38;5;197m_BASE_[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mzero_shot_maskformer_R50_bs32_60k.yaml[39m
[38;5;197mMODEL[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mSEM_SEG_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mNUM_CLASSES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m171[39m
[38;5;15m  [39m[38;5;197mMETA_ARCHITECTURE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mZeroShotMaskFormerClipfeatUpsample[39m[38;5;186m"[39m
[38;5;15m  [39m[38;5;197mBACKBONE[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mbuild_resnet_deeplab_backbone[39m[38;5;186m"[39m
[38;5;15m  [39m
[38;5;15m  [39m[38;5;197mWEIGHTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mdetectron2://DeepLab/R-103.pkl[39m[38;5;186m"[39m
[38;5;15m  [39m[38;5;197mRESNETS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mDEPTH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m101[39m
[38;5;15m    [39m[38;5;197mSTEM_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mdeeplab[39m[38;5;186m"[39m
[38;5;15m    [39m[38;5;197mSTEM_OUT_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m128[39m
[38;5;15m    [39m[38;5;197mSTRIDE_IN_1X1[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mFalse[39m
[38;5;15m    [39m[38;5;197mOUT_FEATURES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;186m"[39m[38;5;186mres2[39m[38;5;186m"[39m[38;5;15m,[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mres3[39m[38;5;186m"[39m[38;5;15m,[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mres4[39m[38;5;186m"[39m[38;5;15m,[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mres5[39m[38;5;186m"[39m[38;5;15m][39m
[38;5;15m    [39m[38;5;242m# NORM: "SyncBN"[39m
[38;5;15m    [39m[38;5;197mRES5_MULTI_GRID[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m1[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15m2[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15m4[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;197mCLIP_ADAPTER[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCLIP_ENSEMBLE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mTrue[39m
[38;5;15m    [39m[38;5;197mCLIP_ENSEMBLE_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.8[39m
[38;5;197mINPUT[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mMIN_SIZE_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;81m!!python/object/apply:eval[39m[38;5;15m [39m[38;5;15m[[39m[38;5;186m"[39m[38;5;186m[int(x[39m[38;5;15m [39m[38;5;186m*[39m[38;5;15m [39m[38;5;186m0.1[39m[38;5;15m [39m[38;5;186m*[39m[38;5;15m [39m[38;5;186m512)[39m[38;5;15m [39m[38;5;186mfor[39m[38;5;15m [39m[38;5;186mx[39m[38;5;15m [39m[38;5;186min[39m[38;5;15m [39m[38;5;186mrange(5,[39m[38;5;15m [39m[38;5;186m16)][39m[38;5;186m"[39m[38;5;15m][39m
[38;5;197mOUTPUT_DIR[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mdeop/train_log[39m[38;5;186m"[39m
[38;5;197mSOLVER[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mIMS_PER_BATCH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m8[39m
[38;5;15m  [39m[38;5;197mTEST_IMS_PER_BATCH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m8[39m
[38;5;197mTEST[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mEVAL_PERIOD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m5000[39m

[01/13 16:23:13] detectron2.utils.env INFO: Using a generated random seed 13463330
[01/13 16:23:14] mask_former.modeling.clip_adapter INFO: Prompt Learner training params: ['prefix_prompt']
[01/13 16:23:18] detectron2.engine.defaults INFO: Model:
ZeroShotClipfeatUpsample_addnorm_vit16Query2D_maskvit(
  (backbone): ResNet(
    (stem): DeepLabStem(
      (conv1): Conv2d(
        3, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
      )
      (conv3): Conv2d(
        64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
      )
    )
    (res2): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv1): Conv2d(
          128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
      )
    )
    (res3): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv1): Conv2d(
          256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
      (3): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
    )
    (res4): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
        (conv1): Conv2d(
          512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (3): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (4): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (5): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (6): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (7): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (8): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (9): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (10): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (11): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (12): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (13): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (14): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (15): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (16): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (17): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (18): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (19): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (20): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (21): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (22): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
    )
    (res5): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
        (conv1): Conv2d(
          1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
      )
    )
  )
  (sem_seg_head): ZeroShotMaskFormerHead(
    (pixel_decoder): BasePixelDecoder(
      (adapter_1): Conv2d(
        256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (layer_1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (adapter_2): Conv2d(
        512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (layer_2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (adapter_3): Conv2d(
        1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (layer_3): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (layer_4): Conv2d(
        2048, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (mask_features): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
    (predictor): ZeroShotTransformerPredictor(
      (pe_layer): PositionEmbeddingSine()
      (transformer): Transformer(
        (encoder): TransformerEncoder(
          (layers): ModuleList()
        )
        (decoder): TransformerDecoder(
          (layers): ModuleList(
            (0-5): 6 x TransformerDecoderLayer(
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (multihead_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (linear1): Linear(in_features=256, out_features=2048, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (linear2): Linear(in_features=2048, out_features=256, bias=True)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (dropout1): Dropout(p=0.1, inplace=False)
              (dropout2): Dropout(p=0.1, inplace=False)
              (dropout3): Dropout(p=0.1, inplace=False)
            )
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (query_embed): Embedding(100, 256)
      (input_proj): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
      (mask_embed): MLP(
        (layers): ModuleList(
          (0-2): 3 x Linear(in_features=256, out_features=256, bias=True)
        )
      )
      (class_embed): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=1024, bias=True)
          (1): Linear(in_features=1024, out_features=512, bias=True)
        )
      )
    )
  )
  (criterion): SetCriterion(
    (matcher): Matcher HungarianMatcher
        cost_class: 1
        cost_mask: 20.0
        cost_dice: 1.0
  )
  (clip_adapter): MaskFormerClipFeatureAdapter(
    (clip_model): CLIP(
      (visual): VisionTransformerLearnPosition(
        (conv1): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False)
        (ln_pre): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (transformer): TransformerVitPosition(
          (resblocks): Sequential(
            (0): ResidualAttentionBlockVitPosition(
              (attn): MultiheadAttentionLoRA(
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (1): ResidualAttentionBlockVitPosition(
              (attn): MultiheadAttentionLoRA(
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (2): ResidualAttentionBlockVitPosition(
              (attn): MultiheadAttentionLoRA(
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (3): ResidualAttentionBlockVitPosition(
              (attn): MultiheadAttentionLoRA(
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (4): ResidualAttentionBlockVitPosition(
              (attn): MultiheadAttentionLoRA(
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (5): ResidualAttentionBlockVitPosition(
              (attn): MultiheadAttentionLoRA(
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (6): ResidualAttentionBlockVitPosition(
              (attn): MultiheadAttentionLoRA(
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (7): ResidualAttentionBlockVitPosition(
              (attn): MultiheadAttentionLoRA(
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (8): ResidualAttentionBlockVitPosition(
              (attn): MultiheadAttentionLoRA(
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (9): ResidualAttentionBlockVitPosition(
              (attn): MultiheadAttentionLoRA(
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (10): ResidualAttentionBlockVitPosition(
              (attn): MultiheadAttentionLoRA(
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (11): ResidualAttentionBlockVitPosition(
              (attn): MultiheadAttentionVit(
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
        (ln_post): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (transformer): Transformer(
        (resblocks): Sequential(
          (0): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (1): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (2): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (3): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (4): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (5): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (6): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (7): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (8): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (9): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (10): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (11): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (token_embedding): Embedding(49408, 512)
      (ln_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (prompt_learner): LearnablePromptExtractor(
      prefix_prompt:16,suffix_prompt:0,dimension:512
      [Normal_Init(mu=0,std=0.02)]
    )
  )
  (decoder): TransformerDecoderLinear(
    (layers): ModuleList(
      (0): TransformerDecoderLayerLinear(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
        )
        (multihead_attn_my): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
        )
        (linear1): Linear(in_features=512, out_features=1024, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=1024, out_features=512, bias=True)
        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
    )
    (outlayer): TransformerDecoderOutLayerLinear(
      (self_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
      )
      (multihead_attn_my): MultiHeadAttentionMy(
        (dropout_F): Dropout(p=0.1, inplace=False)
      )
      (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (dropout1): Dropout(p=0.1, inplace=False)
      (dropout2): Dropout(p=0.1, inplace=False)
      (dropout3): Dropout(p=0.1, inplace=False)
    )
    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  )
  (pe_layer): PositionEmbeddingSine()
  (query_embedding): Embedding(100, 512)
)
[01/13 16:23:18] mask_former.data.dataset_mappers.mask_former_semantic_dataset_mapper INFO: [MaskFormerSemanticDatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=(256, 307, 358, 409, 460, 512, 563, 614, 665, 716, 768), max_size=2560, sample_style='choice'), RandomCrop_CategoryAreaConstraint(crop_type='absolute', crop_size=[640, 640], single_category_max_area=1.0, ignored_category=255), <detectron2.projects.point_rend.color_augmentation.ColorAugSSDTransform object at 0x7fdc0610aef0>, RandomFlip()]
[01/13 16:23:20] detectron2.data.datasets.coco INFO: Loaded 118287 images with semantic segmentation from /scratch/local/ssd/anjun/datasets/coco-stuff/images/train2017
[01/13 16:23:21] mask_former.data.build INFO: Using training sampler TrainingSampler
[01/13 16:23:21] detectron2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[01/13 16:23:21] detectron2.data.common INFO: Serializing 118287 elements to byte tensors and concatenating them all ...
[01/13 16:23:21] detectron2.data.common INFO: Serialized dataset takes 32.38 MiB
[01/13 16:23:21] detectron2.data.build INFO: Making batched data loader with batch_size=4
[01/13 16:23:22] detectron2.checkpoint.detection_checkpoint INFO: [DetectionCheckpointer] Loading from pretrained_models/deop_model_final.pth ...
[01/13 16:23:22] fvcore.common.checkpoint INFO: [Checkpointer] Loading from pretrained_models/deop_model_final.pth ...
[01/13 16:23:22] fvcore.common.checkpoint WARNING: Skip loading parameter 'criterion.empty_weight' to the model due to incompatible shapes: (157,) in the checkpoint but (172,) in the model! You might want to double check if this is expected.
[01/13 16:23:22] fvcore.common.checkpoint WARNING: Some model parameters or buffers are not found in the checkpoint:
[34mclip_adapter.clip_model.token_embedding.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.10.learned_position[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.11.attn.out_proj.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.11.learned_position[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.6.learned_position[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.7.learned_position[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.8.learned_position[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.9.learned_position[0m
[34mcriterion.empty_weight[0m
[01/13 16:23:22] detectron2.engine.train_loop INFO: Starting training from iteration 0
[01/13 16:23:40] detectron2.engine.hooks INFO: Total training time: 0:00:00 (0:00:00 on hooks)
[01/13 16:28:36] detectron2 INFO: Rank of current process: 3. World size: 4
[01/13 16:28:38] detectron2 INFO: Environment info:
-------------------------------  --------------------------------------------------------------------------------------------------
sys.platform                     linux
Python                           3.10.11 (main, May 16 2023, 00:28:57) [GCC 11.2.0]
numpy                            1.26.2
detectron2                       0.6 @/homes/55/anjun/.local/lib/python3.10/site-packages/detectron2
Compiler                         GCC 8.3
CUDA compiler                    CUDA 12.1
detectron2 arch flags            8.6
DETECTRON2_ENV_MODULE            <not set>
PyTorch                          2.1.1+cu121 @/scratch/local/ssd/anjun/anaconda3/envs/ldm/lib/python3.10/site-packages/torch
PyTorch debug build              False
torch._C._GLIBCXX_USE_CXX11_ABI  False
GPU available                    Yes
GPU 0,1,2,3                      NVIDIA A40 (arch=8.6)
Driver version                   530.30.02
CUDA_HOME                        /usr/local/cuda-12.1/
Pillow                           9.5.0
torchvision                      0.16.1+cu121 @/scratch/local/ssd/anjun/anaconda3/envs/ldm/lib/python3.10/site-packages/torchvision
torchvision arch flags           5.0, 6.0, 7.0, 7.5, 8.0, 8.6, 9.0
fvcore                           0.1.5.post20221221
iopath                           0.1.9
cv2                              4.5.4-dev
-------------------------------  --------------------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v3.1.1 (Git Hash 64f6bcbcbab628e96f33a62c3e975f8535a7bde4)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 12.1
  - NVCC architecture flags: -gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.9.2
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=12.1, CUDNN_VERSION=8.9.2, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=old-style-cast -Wno-invalid-partial-specialization -Wno-unused-private-field -Wno-aligned-allocation-unavailable -Wno-missing-braces -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.1.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[01/13 16:28:38] detectron2 INFO: Command line arguments: Namespace(config_file='configs/coco-stuff-164k-156/zero_shot_maskformer_R101c_bs32_60k_proposalmask_featupsample_img512.yaml', resume=True, eval_only=False, num_gpus=4, num_machines=1, machine_rank=0, dist_url='tcp://0.0.0.0:12237', opts=['MODEL.CLIP_ADAPTER.CLIP_ENSEMBLE', 'False', 'MODEL.CLIP_ADAPTER.CLIP_ENSEMBLE_WEIGHT', '-1.0', 'SOLVER.TEST_IMS_PER_BATCH', '1', 'OUTPUT_DIR', 'deop/train_log', 'MODEL.CLIP_ADAPTER.PROMPT_LEARNER', 'learnable', 'SOLVER.IMS_PER_BATCH', '16', 'SOLVER.BASE_LR', '0.00005', 'SOLVER.CHECKPOINT_PERIOD', '1000', 'MODEL.CLIP_ADAPTER.CLIP_MODEL_NAME', 'ViT-B/16', 'MODEL.WEIGHTS', 'pretrained_models/deop_model_final.pth', 'MODEL.META_ARCHITECTURE', 'ZeroShotClipfeatUpsample_addnorm_vit16Query2D_maskvit', 'MODEL.NUM_DECODER_LAYER', '1', 'ORACLE', 'False', 'INPUT.MIN_SIZE_TEST', '512', 'TEST.EVAL_PERIOD', '1000', 'INPUT.SIZE_DIVISIBILITY', '512', 'MODEL.MASK_FORMER.DECODER_DICE_WEIGHT', '0.8', 'MODEL.MASK_FORMER.DECODER_CE_WEIGHT', '2.0', 'MODEL.CLIP_ADAPTER.LEARN_POSITION', 'True', 'MODEL.CLIP_ADAPTER.POSITION_LAYERS', '[1,2,3,4,5,6,7,8,9,10,11]', 'MODEL.CLIP_ADAPTER.LAYERMASKVIT', '[11]', 'MODEL.CLIP_ADAPTER.END2ENDTRAIN', 'False', 'MODEL.CLIP_ADAPTER.PS_SHORTCUT', '1.0', 'DATASETS.TRAIN', "('coco_2017_train_stuff_all_sem_seg',)", 'DATASETS.TEST', "('context_59_test_sem_seg',)", 'MODEL.SEM_SEG_HEAD.NUM_CLASSES', '171', 'MODEL.MASK_FORMER.LOSS_NUM_CLASS', '171'])
[01/13 16:28:38] detectron2 INFO: Contents of args.config_file=configs/coco-stuff-164k-156/zero_shot_maskformer_R101c_bs32_60k_proposalmask_featupsample_img512.yaml:
[38;5;197m_BASE_[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mzero_shot_maskformer_R50_bs32_60k.yaml[39m
[38;5;197mMODEL[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mSEM_SEG_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mNUM_CLASSES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m171[39m
[38;5;15m  [39m[38;5;197mMETA_ARCHITECTURE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mZeroShotMaskFormerClipfeatUpsample[39m[38;5;186m"[39m
[38;5;15m  [39m[38;5;197mBACKBONE[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mbuild_resnet_deeplab_backbone[39m[38;5;186m"[39m
[38;5;15m  [39m
[38;5;15m  [39m[38;5;197mWEIGHTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mdetectron2://DeepLab/R-103.pkl[39m[38;5;186m"[39m
[38;5;15m  [39m[38;5;197mRESNETS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mDEPTH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m101[39m
[38;5;15m    [39m[38;5;197mSTEM_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mdeeplab[39m[38;5;186m"[39m
[38;5;15m    [39m[38;5;197mSTEM_OUT_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m128[39m
[38;5;15m    [39m[38;5;197mSTRIDE_IN_1X1[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mFalse[39m
[38;5;15m    [39m[38;5;197mOUT_FEATURES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;186m"[39m[38;5;186mres2[39m[38;5;186m"[39m[38;5;15m,[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mres3[39m[38;5;186m"[39m[38;5;15m,[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mres4[39m[38;5;186m"[39m[38;5;15m,[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mres5[39m[38;5;186m"[39m[38;5;15m][39m
[38;5;15m    [39m[38;5;242m# NORM: "SyncBN"[39m
[38;5;15m    [39m[38;5;197mRES5_MULTI_GRID[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m1[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15m2[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15m4[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;197mCLIP_ADAPTER[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCLIP_ENSEMBLE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mTrue[39m
[38;5;15m    [39m[38;5;197mCLIP_ENSEMBLE_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.8[39m
[38;5;197mINPUT[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mMIN_SIZE_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;81m!!python/object/apply:eval[39m[38;5;15m [39m[38;5;15m[[39m[38;5;186m"[39m[38;5;186m[int(x[39m[38;5;15m [39m[38;5;186m*[39m[38;5;15m [39m[38;5;186m0.1[39m[38;5;15m [39m[38;5;186m*[39m[38;5;15m [39m[38;5;186m512)[39m[38;5;15m [39m[38;5;186mfor[39m[38;5;15m [39m[38;5;186mx[39m[38;5;15m [39m[38;5;186min[39m[38;5;15m [39m[38;5;186mrange(5,[39m[38;5;15m [39m[38;5;186m16)][39m[38;5;186m"[39m[38;5;15m][39m
[38;5;197mOUTPUT_DIR[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mdeop/train_log[39m[38;5;186m"[39m
[38;5;197mSOLVER[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mIMS_PER_BATCH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m8[39m
[38;5;15m  [39m[38;5;197mTEST_IMS_PER_BATCH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m8[39m
[38;5;197mTEST[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mEVAL_PERIOD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m5000[39m

[01/13 16:28:38] detectron2.utils.env INFO: Using a generated random seed 38695889
[01/13 16:28:39] mask_former.modeling.clip_adapter INFO: Prompt Learner training params: ['prefix_prompt']
[01/13 16:28:47] detectron2.engine.defaults INFO: Model:
ZeroShotClipfeatUpsample_addnorm_vit16Query2D_maskvit(
  (backbone): ResNet(
    (stem): DeepLabStem(
      (conv1): Conv2d(
        3, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
      )
      (conv3): Conv2d(
        64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
      )
    )
    (res2): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv1): Conv2d(
          128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
      )
    )
    (res3): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv1): Conv2d(
          256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
      (3): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
    )
    (res4): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
        (conv1): Conv2d(
          512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (3): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (4): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (5): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (6): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (7): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (8): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (9): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (10): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (11): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (12): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (13): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (14): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (15): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (16): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (17): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (18): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (19): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (20): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (21): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (22): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
    )
    (res5): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
        (conv1): Conv2d(
          1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
      )
    )
  )
  (sem_seg_head): ZeroShotMaskFormerHead(
    (pixel_decoder): BasePixelDecoder(
      (adapter_1): Conv2d(
        256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (layer_1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (adapter_2): Conv2d(
        512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (layer_2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (adapter_3): Conv2d(
        1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (layer_3): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (layer_4): Conv2d(
        2048, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (mask_features): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
    (predictor): ZeroShotTransformerPredictor(
      (pe_layer): PositionEmbeddingSine()
      (transformer): Transformer(
        (encoder): TransformerEncoder(
          (layers): ModuleList()
        )
        (decoder): TransformerDecoder(
          (layers): ModuleList(
            (0-5): 6 x TransformerDecoderLayer(
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (multihead_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (linear1): Linear(in_features=256, out_features=2048, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (linear2): Linear(in_features=2048, out_features=256, bias=True)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (dropout1): Dropout(p=0.1, inplace=False)
              (dropout2): Dropout(p=0.1, inplace=False)
              (dropout3): Dropout(p=0.1, inplace=False)
            )
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (query_embed): Embedding(100, 256)
      (input_proj): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
      (mask_embed): MLP(
        (layers): ModuleList(
          (0-2): 3 x Linear(in_features=256, out_features=256, bias=True)
        )
      )
      (class_embed): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=1024, bias=True)
          (1): Linear(in_features=1024, out_features=512, bias=True)
        )
      )
    )
  )
  (criterion): SetCriterion(
    (matcher): Matcher HungarianMatcher
        cost_class: 1
        cost_mask: 20.0
        cost_dice: 1.0
  )
  (clip_adapter): MaskFormerClipFeatureAdapter(
    (clip_model): CLIP(
      (visual): VisionTransformerLearnPosition(
        (conv1): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False)
        (ln_pre): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (transformer): TransformerVitPosition(
          (resblocks): Sequential(
            (0): ResidualAttentionBlockVitPosition(
              (attn): MultiheadAttentionLoRA(
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (1): ResidualAttentionBlockVitPosition(
              (attn): MultiheadAttentionLoRA(
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (2): ResidualAttentionBlockVitPosition(
              (attn): MultiheadAttentionLoRA(
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (3): ResidualAttentionBlockVitPosition(
              (attn): MultiheadAttentionLoRA(
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (4): ResidualAttentionBlockVitPosition(
              (attn): MultiheadAttentionLoRA(
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (5): ResidualAttentionBlockVitPosition(
              (attn): MultiheadAttentionLoRA(
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (6): ResidualAttentionBlockVitPosition(
              (attn): MultiheadAttentionLoRA(
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (7): ResidualAttentionBlockVitPosition(
              (attn): MultiheadAttentionLoRA(
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (8): ResidualAttentionBlockVitPosition(
              (attn): MultiheadAttentionLoRA(
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (9): ResidualAttentionBlockVitPosition(
              (attn): MultiheadAttentionLoRA(
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (10): ResidualAttentionBlockVitPosition(
              (attn): MultiheadAttentionLoRA(
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (11): ResidualAttentionBlockVitPosition(
              (attn): MultiheadAttentionVit(
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
        (ln_post): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (transformer): Transformer(
        (resblocks): Sequential(
          (0): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (1): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (2): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (3): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (4): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (5): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (6): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (7): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (8): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (9): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (10): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (11): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (token_embedding): Embedding(49408, 512)
      (ln_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (prompt_learner): LearnablePromptExtractor(
      prefix_prompt:16,suffix_prompt:0,dimension:512
      [Normal_Init(mu=0,std=0.02)]
    )
  )
  (decoder): TransformerDecoderLinear(
    (layers): ModuleList(
      (0): TransformerDecoderLayerLinear(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
        )
        (multihead_attn_my): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
        )
        (linear1): Linear(in_features=512, out_features=1024, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=1024, out_features=512, bias=True)
        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
    )
    (outlayer): TransformerDecoderOutLayerLinear(
      (self_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
      )
      (multihead_attn_my): MultiHeadAttentionMy(
        (dropout_F): Dropout(p=0.1, inplace=False)
      )
      (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (dropout1): Dropout(p=0.1, inplace=False)
      (dropout2): Dropout(p=0.1, inplace=False)
      (dropout3): Dropout(p=0.1, inplace=False)
    )
    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  )
  (pe_layer): PositionEmbeddingSine()
  (query_embedding): Embedding(100, 512)
)
[01/13 16:28:47] mask_former.data.dataset_mappers.mask_former_semantic_dataset_mapper INFO: [MaskFormerSemanticDatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=(256, 307, 358, 409, 460, 512, 563, 614, 665, 716, 768), max_size=2560, sample_style='choice'), RandomCrop_CategoryAreaConstraint(crop_type='absolute', crop_size=[640, 640], single_category_max_area=1.0, ignored_category=255), <detectron2.projects.point_rend.color_augmentation.ColorAugSSDTransform object at 0x7fef002a31c0>, RandomFlip()]
[01/13 16:28:49] detectron2.data.datasets.coco INFO: Loaded 118287 images with semantic segmentation from /scratch/local/ssd/anjun/datasets/coco-stuff/images/train2017
[01/13 16:28:49] mask_former.data.build INFO: Using training sampler TrainingSampler
[01/13 16:28:49] detectron2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[01/13 16:28:49] detectron2.data.common INFO: Serializing 118287 elements to byte tensors and concatenating them all ...
[01/13 16:28:50] detectron2.data.common INFO: Serialized dataset takes 32.38 MiB
[01/13 16:28:50] detectron2.data.build INFO: Making batched data loader with batch_size=4
[01/13 16:28:50] detectron2.checkpoint.detection_checkpoint INFO: [DetectionCheckpointer] Loading from pretrained_models/deop_model_final.pth ...
[01/13 16:28:50] fvcore.common.checkpoint INFO: [Checkpointer] Loading from pretrained_models/deop_model_final.pth ...
[01/13 16:29:51] detectron2 INFO: Rank of current process: 3. World size: 4
[01/13 16:29:56] detectron2 INFO: Environment info:
-------------------------------  --------------------------------------------------------------------------------------------------
sys.platform                     linux
Python                           3.10.11 (main, May 16 2023, 00:28:57) [GCC 11.2.0]
numpy                            1.26.2
detectron2                       0.6 @/homes/55/anjun/.local/lib/python3.10/site-packages/detectron2
Compiler                         GCC 8.3
CUDA compiler                    CUDA 12.1
detectron2 arch flags            8.6
DETECTRON2_ENV_MODULE            <not set>
PyTorch                          2.1.1+cu121 @/scratch/local/ssd/anjun/anaconda3/envs/ldm/lib/python3.10/site-packages/torch
PyTorch debug build              False
torch._C._GLIBCXX_USE_CXX11_ABI  False
GPU available                    Yes
GPU 0,1,2,3                      NVIDIA A40 (arch=8.6)
Driver version                   530.30.02
CUDA_HOME                        /usr/local/cuda-12.1/
Pillow                           9.5.0
torchvision                      0.16.1+cu121 @/scratch/local/ssd/anjun/anaconda3/envs/ldm/lib/python3.10/site-packages/torchvision
torchvision arch flags           5.0, 6.0, 7.0, 7.5, 8.0, 8.6, 9.0
fvcore                           0.1.5.post20221221
iopath                           0.1.9
cv2                              4.5.4-dev
-------------------------------  --------------------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v3.1.1 (Git Hash 64f6bcbcbab628e96f33a62c3e975f8535a7bde4)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 12.1
  - NVCC architecture flags: -gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.9.2
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=12.1, CUDNN_VERSION=8.9.2, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=old-style-cast -Wno-invalid-partial-specialization -Wno-unused-private-field -Wno-aligned-allocation-unavailable -Wno-missing-braces -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.1.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[01/13 16:29:56] detectron2 INFO: Command line arguments: Namespace(config_file='configs/coco-stuff-164k-156/zero_shot_maskformer_R101c_bs32_60k_proposalmask_featupsample_img512.yaml', resume=True, eval_only=False, num_gpus=4, num_machines=1, machine_rank=0, dist_url='tcp://0.0.0.0:12237', opts=['MODEL.CLIP_ADAPTER.CLIP_ENSEMBLE', 'False', 'MODEL.CLIP_ADAPTER.CLIP_ENSEMBLE_WEIGHT', '-1.0', 'SOLVER.TEST_IMS_PER_BATCH', '1', 'OUTPUT_DIR', 'deop/train_log', 'MODEL.CLIP_ADAPTER.PROMPT_LEARNER', 'learnable', 'SOLVER.IMS_PER_BATCH', '16', 'SOLVER.BASE_LR', '0.00005', 'SOLVER.CHECKPOINT_PERIOD', '1000', 'MODEL.CLIP_ADAPTER.CLIP_MODEL_NAME', 'ViT-B/16', 'MODEL.WEIGHTS', 'pretrained_models/deop_model_final.pth', 'MODEL.META_ARCHITECTURE', 'ZeroShotClipfeatUpsample_addnorm_vit16Query2D_maskvit', 'MODEL.NUM_DECODER_LAYER', '1', 'ORACLE', 'False', 'INPUT.MIN_SIZE_TEST', '512', 'TEST.EVAL_PERIOD', '1000', 'INPUT.SIZE_DIVISIBILITY', '512', 'MODEL.MASK_FORMER.DECODER_DICE_WEIGHT', '0.8', 'MODEL.MASK_FORMER.DECODER_CE_WEIGHT', '2.0', 'MODEL.CLIP_ADAPTER.LEARN_POSITION', 'True', 'MODEL.CLIP_ADAPTER.POSITION_LAYERS', '[1,2,3,4,5,6,7,8,9,10,11]', 'MODEL.CLIP_ADAPTER.LAYERMASKVIT', '[11]', 'MODEL.CLIP_ADAPTER.END2ENDTRAIN', 'False', 'MODEL.CLIP_ADAPTER.PS_SHORTCUT', '1.0', 'DATASETS.TRAIN', "('coco_2017_train_stuff_all_sem_seg',)", 'DATASETS.TEST', "('context_59_test_sem_seg',)", 'MODEL.SEM_SEG_HEAD.NUM_CLASSES', '171', 'MODEL.MASK_FORMER.LOSS_NUM_CLASS', '171'])
[01/13 16:29:56] detectron2 INFO: Contents of args.config_file=configs/coco-stuff-164k-156/zero_shot_maskformer_R101c_bs32_60k_proposalmask_featupsample_img512.yaml:
[38;5;197m_BASE_[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mzero_shot_maskformer_R50_bs32_60k.yaml[39m
[38;5;197mMODEL[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mSEM_SEG_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mNUM_CLASSES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m171[39m
[38;5;15m  [39m[38;5;197mMETA_ARCHITECTURE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mZeroShotMaskFormerClipfeatUpsample[39m[38;5;186m"[39m
[38;5;15m  [39m[38;5;197mBACKBONE[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mbuild_resnet_deeplab_backbone[39m[38;5;186m"[39m
[38;5;15m  [39m
[38;5;15m  [39m[38;5;197mWEIGHTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mdetectron2://DeepLab/R-103.pkl[39m[38;5;186m"[39m
[38;5;15m  [39m[38;5;197mRESNETS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mDEPTH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m101[39m
[38;5;15m    [39m[38;5;197mSTEM_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mdeeplab[39m[38;5;186m"[39m
[38;5;15m    [39m[38;5;197mSTEM_OUT_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m128[39m
[38;5;15m    [39m[38;5;197mSTRIDE_IN_1X1[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mFalse[39m
[38;5;15m    [39m[38;5;197mOUT_FEATURES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;186m"[39m[38;5;186mres2[39m[38;5;186m"[39m[38;5;15m,[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mres3[39m[38;5;186m"[39m[38;5;15m,[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mres4[39m[38;5;186m"[39m[38;5;15m,[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mres5[39m[38;5;186m"[39m[38;5;15m][39m
[38;5;15m    [39m[38;5;242m# NORM: "SyncBN"[39m
[38;5;15m    [39m[38;5;197mRES5_MULTI_GRID[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m1[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15m2[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15m4[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;197mCLIP_ADAPTER[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCLIP_ENSEMBLE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mTrue[39m
[38;5;15m    [39m[38;5;197mCLIP_ENSEMBLE_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.8[39m
[38;5;197mINPUT[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mMIN_SIZE_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;81m!!python/object/apply:eval[39m[38;5;15m [39m[38;5;15m[[39m[38;5;186m"[39m[38;5;186m[int(x[39m[38;5;15m [39m[38;5;186m*[39m[38;5;15m [39m[38;5;186m0.1[39m[38;5;15m [39m[38;5;186m*[39m[38;5;15m [39m[38;5;186m512)[39m[38;5;15m [39m[38;5;186mfor[39m[38;5;15m [39m[38;5;186mx[39m[38;5;15m [39m[38;5;186min[39m[38;5;15m [39m[38;5;186mrange(5,[39m[38;5;15m [39m[38;5;186m16)][39m[38;5;186m"[39m[38;5;15m][39m
[38;5;197mOUTPUT_DIR[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mdeop/train_log[39m[38;5;186m"[39m
[38;5;197mSOLVER[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mIMS_PER_BATCH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m8[39m
[38;5;15m  [39m[38;5;197mTEST_IMS_PER_BATCH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m8[39m
[38;5;197mTEST[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mEVAL_PERIOD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m5000[39m

[01/13 16:29:56] detectron2.utils.env INFO: Using a generated random seed 56501373
[01/13 16:29:57] mask_former.modeling.clip_adapter INFO: Prompt Learner training params: ['prefix_prompt']
[01/13 16:30:01] detectron2.engine.defaults INFO: Model:
ZeroShotClipfeatUpsample_addnorm_vit16Query2D_maskvit(
  (backbone): ResNet(
    (stem): DeepLabStem(
      (conv1): Conv2d(
        3, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
      )
      (conv3): Conv2d(
        64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
      )
    )
    (res2): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv1): Conv2d(
          128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
      )
    )
    (res3): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv1): Conv2d(
          256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
      (3): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
    )
    (res4): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
        (conv1): Conv2d(
          512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (3): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (4): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (5): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (6): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (7): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (8): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (9): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (10): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (11): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (12): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (13): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (14): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (15): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (16): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (17): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (18): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (19): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (20): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (21): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (22): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
    )
    (res5): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
        (conv1): Conv2d(
          1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
      )
    )
  )
  (sem_seg_head): ZeroShotMaskFormerHead(
    (pixel_decoder): BasePixelDecoder(
      (adapter_1): Conv2d(
        256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (layer_1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (adapter_2): Conv2d(
        512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (layer_2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (adapter_3): Conv2d(
        1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (layer_3): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (layer_4): Conv2d(
        2048, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (mask_features): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
    (predictor): ZeroShotTransformerPredictor(
      (pe_layer): PositionEmbeddingSine()
      (transformer): Transformer(
        (encoder): TransformerEncoder(
          (layers): ModuleList()
        )
        (decoder): TransformerDecoder(
          (layers): ModuleList(
            (0-5): 6 x TransformerDecoderLayer(
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (multihead_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (linear1): Linear(in_features=256, out_features=2048, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (linear2): Linear(in_features=2048, out_features=256, bias=True)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (dropout1): Dropout(p=0.1, inplace=False)
              (dropout2): Dropout(p=0.1, inplace=False)
              (dropout3): Dropout(p=0.1, inplace=False)
            )
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (query_embed): Embedding(100, 256)
      (input_proj): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
      (mask_embed): MLP(
        (layers): ModuleList(
          (0-2): 3 x Linear(in_features=256, out_features=256, bias=True)
        )
      )
      (class_embed): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=1024, bias=True)
          (1): Linear(in_features=1024, out_features=512, bias=True)
        )
      )
    )
  )
  (criterion): SetCriterion(
    (matcher): Matcher HungarianMatcher
        cost_class: 1
        cost_mask: 20.0
        cost_dice: 1.0
  )
  (clip_adapter): MaskFormerClipFeatureAdapter(
    (clip_model): CLIP(
      (visual): VisionTransformerLearnPosition(
        (conv1): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False)
        (ln_pre): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (transformer): TransformerVitPosition(
          (resblocks): Sequential(
            (0): ResidualAttentionBlockVitPosition(
              (attn): MultiheadAttentionLoRA(
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (1): ResidualAttentionBlockVitPosition(
              (attn): MultiheadAttentionLoRA(
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (2): ResidualAttentionBlockVitPosition(
              (attn): MultiheadAttentionLoRA(
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (3): ResidualAttentionBlockVitPosition(
              (attn): MultiheadAttentionLoRA(
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (4): ResidualAttentionBlockVitPosition(
              (attn): MultiheadAttentionLoRA(
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (5): ResidualAttentionBlockVitPosition(
              (attn): MultiheadAttentionLoRA(
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (6): ResidualAttentionBlockVitPosition(
              (attn): MultiheadAttentionLoRA(
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (7): ResidualAttentionBlockVitPosition(
              (attn): MultiheadAttentionLoRA(
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (8): ResidualAttentionBlockVitPosition(
              (attn): MultiheadAttentionLoRA(
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (9): ResidualAttentionBlockVitPosition(
              (attn): MultiheadAttentionLoRA(
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (10): ResidualAttentionBlockVitPosition(
              (attn): MultiheadAttentionLoRA(
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (11): ResidualAttentionBlockVitPosition(
              (attn): MultiheadAttentionVit(
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
        (ln_post): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (transformer): Transformer(
        (resblocks): Sequential(
          (0): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (1): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (2): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (3): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (4): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (5): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (6): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (7): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (8): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (9): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (10): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (11): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (token_embedding): Embedding(49408, 512)
      (ln_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (prompt_learner): LearnablePromptExtractor(
      prefix_prompt:16,suffix_prompt:0,dimension:512
      [Normal_Init(mu=0,std=0.02)]
    )
  )
  (decoder): TransformerDecoderLinear(
    (layers): ModuleList(
      (0): TransformerDecoderLayerLinear(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
        )
        (multihead_attn_my): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
        )
        (linear1): Linear(in_features=512, out_features=1024, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=1024, out_features=512, bias=True)
        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
    )
    (outlayer): TransformerDecoderOutLayerLinear(
      (self_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
      )
      (multihead_attn_my): MultiHeadAttentionMy(
        (dropout_F): Dropout(p=0.1, inplace=False)
      )
      (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (dropout1): Dropout(p=0.1, inplace=False)
      (dropout2): Dropout(p=0.1, inplace=False)
      (dropout3): Dropout(p=0.1, inplace=False)
    )
    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  )
  (pe_layer): PositionEmbeddingSine()
  (query_embedding): Embedding(100, 512)
)
[01/13 16:30:01] mask_former.data.dataset_mappers.mask_former_semantic_dataset_mapper INFO: [MaskFormerSemanticDatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=(256, 307, 358, 409, 460, 512, 563, 614, 665, 716, 768), max_size=2560, sample_style='choice'), RandomCrop_CategoryAreaConstraint(crop_type='absolute', crop_size=[640, 640], single_category_max_area=1.0, ignored_category=255), <detectron2.projects.point_rend.color_augmentation.ColorAugSSDTransform object at 0x7f38901130d0>, RandomFlip()]
[01/13 16:30:04] detectron2.data.datasets.coco INFO: Loaded 118287 images with semantic segmentation from /scratch/local/ssd/anjun/datasets/coco-stuff/images/train2017
[01/13 16:30:04] mask_former.data.build INFO: Using training sampler TrainingSampler
[01/13 16:30:04] detectron2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[01/13 16:30:04] detectron2.data.common INFO: Serializing 118287 elements to byte tensors and concatenating them all ...
[01/13 16:30:04] detectron2.data.common INFO: Serialized dataset takes 32.38 MiB
[01/13 16:30:04] detectron2.data.build INFO: Making batched data loader with batch_size=4
[01/13 16:30:05] detectron2.checkpoint.detection_checkpoint INFO: [DetectionCheckpointer] Loading from pretrained_models/deop_model_final.pth ...
[01/13 16:30:05] fvcore.common.checkpoint INFO: [Checkpointer] Loading from pretrained_models/deop_model_final.pth ...
[01/13 16:30:05] fvcore.common.checkpoint WARNING: Skip loading parameter 'criterion.empty_weight' to the model due to incompatible shapes: (157,) in the checkpoint but (172,) in the model! You might want to double check if this is expected.
[01/13 16:30:05] fvcore.common.checkpoint WARNING: Some model parameters or buffers are not found in the checkpoint:
[34mclip_adapter.clip_model.token_embedding.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.10.learned_position[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.11.attn.out_proj.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.11.learned_position[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.6.learned_position[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.7.learned_position[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.8.learned_position[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.9.learned_position[0m
[34mcriterion.empty_weight[0m
[01/13 16:30:05] detectron2.engine.train_loop INFO: Starting training from iteration 0
[01/13 16:30:22] detectron2.engine.train_loop ERROR: Exception during training:
Traceback (most recent call last):
  File "/homes/55/anjun/.local/lib/python3.10/site-packages/detectron2/engine/train_loop.py", line 155, in train
    self.run_step()
  File "/scratch/local/ssd/anjun/ovseg/DeOP/train_net.py", line 361, in run_step
    loss_dict = self.model(data)
  File "/scratch/local/ssd/anjun/anaconda3/envs/ldm/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/local/ssd/anjun/anaconda3/envs/ldm/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/local/ssd/anjun/anaconda3/envs/ldm/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1515, in forward
    inputs, kwargs = self._pre_forward(*inputs, **kwargs)
  File "/scratch/local/ssd/anjun/anaconda3/envs/ldm/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1409, in _pre_forward
    if torch.is_grad_enabled() and self.reducer._rebuild_buckets():
RuntimeError: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by 
making sure all `forward` function outputs participate in calculating loss. 
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameter indices which did not receive grad for rank 3: 13 14 15 16
 In addition, you can set the environment variable TORCH_DISTRIBUTED_DEBUG to either INFO or DETAIL to print out information about which particular parameters did not receive gradient on this rank as part of this error
[01/13 16:30:22] detectron2.engine.hooks INFO: Total training time: 0:00:00 (0:00:00 on hooks)
[01/13 16:39:24] detectron2 INFO: Rank of current process: 3. World size: 4
[01/13 16:39:26] detectron2 INFO: Environment info:
-------------------------------  --------------------------------------------------------------------------------------------------
sys.platform                     linux
Python                           3.10.11 (main, May 16 2023, 00:28:57) [GCC 11.2.0]
numpy                            1.26.2
detectron2                       0.6 @/homes/55/anjun/.local/lib/python3.10/site-packages/detectron2
Compiler                         GCC 8.3
CUDA compiler                    CUDA 12.1
detectron2 arch flags            8.6
DETECTRON2_ENV_MODULE            <not set>
PyTorch                          2.1.1+cu121 @/scratch/local/ssd/anjun/anaconda3/envs/ldm/lib/python3.10/site-packages/torch
PyTorch debug build              False
torch._C._GLIBCXX_USE_CXX11_ABI  False
GPU available                    Yes
GPU 0,1,2,3                      NVIDIA A40 (arch=8.6)
Driver version                   530.30.02
CUDA_HOME                        /usr/local/cuda-12.1/
Pillow                           9.5.0
torchvision                      0.16.1+cu121 @/scratch/local/ssd/anjun/anaconda3/envs/ldm/lib/python3.10/site-packages/torchvision
torchvision arch flags           5.0, 6.0, 7.0, 7.5, 8.0, 8.6, 9.0
fvcore                           0.1.5.post20221221
iopath                           0.1.9
cv2                              4.5.4-dev
-------------------------------  --------------------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v3.1.1 (Git Hash 64f6bcbcbab628e96f33a62c3e975f8535a7bde4)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 12.1
  - NVCC architecture flags: -gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.9.2
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=12.1, CUDNN_VERSION=8.9.2, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=old-style-cast -Wno-invalid-partial-specialization -Wno-unused-private-field -Wno-aligned-allocation-unavailable -Wno-missing-braces -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.1.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[01/13 16:39:26] detectron2 INFO: Command line arguments: Namespace(config_file='configs/coco-stuff-164k-156/zero_shot_maskformer_R101c_bs32_60k_proposalmask_featupsample_img512.yaml', resume=True, eval_only=False, num_gpus=4, num_machines=1, machine_rank=0, dist_url='tcp://0.0.0.0:12237', opts=['MODEL.CLIP_ADAPTER.CLIP_ENSEMBLE', 'False', 'MODEL.CLIP_ADAPTER.CLIP_ENSEMBLE_WEIGHT', '-1.0', 'SOLVER.TEST_IMS_PER_BATCH', '1', 'OUTPUT_DIR', 'deop/train_log', 'MODEL.CLIP_ADAPTER.PROMPT_LEARNER', 'learnable', 'SOLVER.IMS_PER_BATCH', '16', 'SOLVER.BASE_LR', '0.00005', 'SOLVER.CHECKPOINT_PERIOD', '1000', 'MODEL.CLIP_ADAPTER.CLIP_MODEL_NAME', 'ViT-B/16', 'MODEL.WEIGHTS', 'pretrained_models/deop_model_final.pth', 'MODEL.META_ARCHITECTURE', 'ZeroShotClipfeatUpsample_addnorm_vit16Query2D_maskvit', 'MODEL.NUM_DECODER_LAYER', '1', 'ORACLE', 'False', 'INPUT.MIN_SIZE_TEST', '512', 'TEST.EVAL_PERIOD', '1000', 'INPUT.SIZE_DIVISIBILITY', '512', 'MODEL.MASK_FORMER.DECODER_DICE_WEIGHT', '0.8', 'MODEL.MASK_FORMER.DECODER_CE_WEIGHT', '2.0', 'MODEL.CLIP_ADAPTER.LEARN_POSITION', 'True', 'MODEL.CLIP_ADAPTER.POSITION_LAYERS', '[1,2,3,4,5,6,7,8,9,10,11]', 'MODEL.CLIP_ADAPTER.LAYERMASKVIT', '[11]', 'MODEL.CLIP_ADAPTER.END2ENDTRAIN', 'False', 'MODEL.CLIP_ADAPTER.PS_SHORTCUT', '1.0', 'DATASETS.TRAIN', "('coco_2017_train_stuff_all_sem_seg',)", 'DATASETS.TEST', "('context_59_test_sem_seg',)", 'MODEL.SEM_SEG_HEAD.NUM_CLASSES', '171', 'MODEL.MASK_FORMER.LOSS_NUM_CLASS', '171'])
[01/13 16:39:26] detectron2 INFO: Contents of args.config_file=configs/coco-stuff-164k-156/zero_shot_maskformer_R101c_bs32_60k_proposalmask_featupsample_img512.yaml:
[38;5;197m_BASE_[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mzero_shot_maskformer_R50_bs32_60k.yaml[39m
[38;5;197mMODEL[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mSEM_SEG_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mNUM_CLASSES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m171[39m
[38;5;15m  [39m[38;5;197mMETA_ARCHITECTURE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mZeroShotMaskFormerClipfeatUpsample[39m[38;5;186m"[39m
[38;5;15m  [39m[38;5;197mBACKBONE[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mbuild_resnet_deeplab_backbone[39m[38;5;186m"[39m
[38;5;15m  [39m
[38;5;15m  [39m[38;5;197mWEIGHTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mdetectron2://DeepLab/R-103.pkl[39m[38;5;186m"[39m
[38;5;15m  [39m[38;5;197mRESNETS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mDEPTH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m101[39m
[38;5;15m    [39m[38;5;197mSTEM_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mdeeplab[39m[38;5;186m"[39m
[38;5;15m    [39m[38;5;197mSTEM_OUT_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m128[39m
[38;5;15m    [39m[38;5;197mSTRIDE_IN_1X1[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mFalse[39m
[38;5;15m    [39m[38;5;197mOUT_FEATURES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;186m"[39m[38;5;186mres2[39m[38;5;186m"[39m[38;5;15m,[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mres3[39m[38;5;186m"[39m[38;5;15m,[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mres4[39m[38;5;186m"[39m[38;5;15m,[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mres5[39m[38;5;186m"[39m[38;5;15m][39m
[38;5;15m    [39m[38;5;242m# NORM: "SyncBN"[39m
[38;5;15m    [39m[38;5;197mRES5_MULTI_GRID[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m1[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15m2[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15m4[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;197mCLIP_ADAPTER[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCLIP_ENSEMBLE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mTrue[39m
[38;5;15m    [39m[38;5;197mCLIP_ENSEMBLE_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.8[39m
[38;5;197mINPUT[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mMIN_SIZE_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;81m!!python/object/apply:eval[39m[38;5;15m [39m[38;5;15m[[39m[38;5;186m"[39m[38;5;186m[int(x[39m[38;5;15m [39m[38;5;186m*[39m[38;5;15m [39m[38;5;186m0.1[39m[38;5;15m [39m[38;5;186m*[39m[38;5;15m [39m[38;5;186m512)[39m[38;5;15m [39m[38;5;186mfor[39m[38;5;15m [39m[38;5;186mx[39m[38;5;15m [39m[38;5;186min[39m[38;5;15m [39m[38;5;186mrange(5,[39m[38;5;15m [39m[38;5;186m16)][39m[38;5;186m"[39m[38;5;15m][39m
[38;5;197mOUTPUT_DIR[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mdeop/train_log[39m[38;5;186m"[39m
[38;5;197mSOLVER[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mIMS_PER_BATCH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m8[39m
[38;5;15m  [39m[38;5;197mTEST_IMS_PER_BATCH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m8[39m
[38;5;197mTEST[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mEVAL_PERIOD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m5000[39m

[01/13 16:39:27] detectron2.utils.env INFO: Using a generated random seed 27097281
[01/13 16:39:27] mask_former.modeling.clip_adapter INFO: Prompt Learner training params: ['prefix_prompt']
[01/13 16:41:38] detectron2 INFO: Rank of current process: 3. World size: 4
[01/13 16:41:43] detectron2 INFO: Environment info:
-------------------------------  --------------------------------------------------------------------------------------------------
sys.platform                     linux
Python                           3.10.11 (main, May 16 2023, 00:28:57) [GCC 11.2.0]
numpy                            1.26.2
detectron2                       0.6 @/homes/55/anjun/.local/lib/python3.10/site-packages/detectron2
Compiler                         GCC 8.3
CUDA compiler                    CUDA 12.1
detectron2 arch flags            8.6
DETECTRON2_ENV_MODULE            <not set>
PyTorch                          2.1.1+cu121 @/scratch/local/ssd/anjun/anaconda3/envs/ldm/lib/python3.10/site-packages/torch
PyTorch debug build              False
torch._C._GLIBCXX_USE_CXX11_ABI  False
GPU available                    Yes
GPU 0,1,2,3                      NVIDIA A40 (arch=8.6)
Driver version                   530.30.02
CUDA_HOME                        /usr/local/cuda-12.1/
Pillow                           9.5.0
torchvision                      0.16.1+cu121 @/scratch/local/ssd/anjun/anaconda3/envs/ldm/lib/python3.10/site-packages/torchvision
torchvision arch flags           5.0, 6.0, 7.0, 7.5, 8.0, 8.6, 9.0
fvcore                           0.1.5.post20221221
iopath                           0.1.9
cv2                              4.5.4-dev
-------------------------------  --------------------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v3.1.1 (Git Hash 64f6bcbcbab628e96f33a62c3e975f8535a7bde4)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 12.1
  - NVCC architecture flags: -gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.9.2
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=12.1, CUDNN_VERSION=8.9.2, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=old-style-cast -Wno-invalid-partial-specialization -Wno-unused-private-field -Wno-aligned-allocation-unavailable -Wno-missing-braces -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.1.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[01/13 16:41:43] detectron2 INFO: Command line arguments: Namespace(config_file='configs/coco-stuff-164k-156/zero_shot_maskformer_R101c_bs32_60k_proposalmask_featupsample_img512.yaml', resume=True, eval_only=False, num_gpus=4, num_machines=1, machine_rank=0, dist_url='tcp://0.0.0.0:12237', opts=['MODEL.CLIP_ADAPTER.CLIP_ENSEMBLE', 'False', 'MODEL.CLIP_ADAPTER.CLIP_ENSEMBLE_WEIGHT', '-1.0', 'SOLVER.TEST_IMS_PER_BATCH', '1', 'OUTPUT_DIR', 'deop/train_log', 'MODEL.CLIP_ADAPTER.PROMPT_LEARNER', 'learnable', 'SOLVER.IMS_PER_BATCH', '16', 'SOLVER.BASE_LR', '0.00005', 'SOLVER.CHECKPOINT_PERIOD', '1000', 'MODEL.CLIP_ADAPTER.CLIP_MODEL_NAME', 'ViT-B/16', 'MODEL.WEIGHTS', 'pretrained_models/deop_model_final.pth', 'MODEL.META_ARCHITECTURE', 'ZeroShotClipfeatUpsample_addnorm_vit16Query2D_maskvit', 'MODEL.NUM_DECODER_LAYER', '1', 'ORACLE', 'False', 'INPUT.MIN_SIZE_TEST', '512', 'TEST.EVAL_PERIOD', '1000', 'INPUT.SIZE_DIVISIBILITY', '512', 'MODEL.MASK_FORMER.DECODER_DICE_WEIGHT', '0.8', 'MODEL.MASK_FORMER.DECODER_CE_WEIGHT', '2.0', 'MODEL.CLIP_ADAPTER.LEARN_POSITION', 'True', 'MODEL.CLIP_ADAPTER.POSITION_LAYERS', '[1,2,3,4,5,6,7,8,9,10,11]', 'MODEL.CLIP_ADAPTER.LAYERMASKVIT', '[11]', 'MODEL.CLIP_ADAPTER.END2ENDTRAIN', 'False', 'MODEL.CLIP_ADAPTER.PS_SHORTCUT', '1.0', 'DATASETS.TRAIN', "('coco_2017_train_stuff_all_sem_seg',)", 'DATASETS.TEST', "('context_59_test_sem_seg',)", 'MODEL.SEM_SEG_HEAD.NUM_CLASSES', '171', 'MODEL.MASK_FORMER.LOSS_NUM_CLASS', '171'])
[01/13 16:41:43] detectron2 INFO: Contents of args.config_file=configs/coco-stuff-164k-156/zero_shot_maskformer_R101c_bs32_60k_proposalmask_featupsample_img512.yaml:
[38;5;197m_BASE_[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mzero_shot_maskformer_R50_bs32_60k.yaml[39m
[38;5;197mMODEL[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mSEM_SEG_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mNUM_CLASSES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m171[39m
[38;5;15m  [39m[38;5;197mMETA_ARCHITECTURE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mZeroShotMaskFormerClipfeatUpsample[39m[38;5;186m"[39m
[38;5;15m  [39m[38;5;197mBACKBONE[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mbuild_resnet_deeplab_backbone[39m[38;5;186m"[39m
[38;5;15m  [39m
[38;5;15m  [39m[38;5;197mWEIGHTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mdetectron2://DeepLab/R-103.pkl[39m[38;5;186m"[39m
[38;5;15m  [39m[38;5;197mRESNETS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mDEPTH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m101[39m
[38;5;15m    [39m[38;5;197mSTEM_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mdeeplab[39m[38;5;186m"[39m
[38;5;15m    [39m[38;5;197mSTEM_OUT_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m128[39m
[38;5;15m    [39m[38;5;197mSTRIDE_IN_1X1[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mFalse[39m
[38;5;15m    [39m[38;5;197mOUT_FEATURES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;186m"[39m[38;5;186mres2[39m[38;5;186m"[39m[38;5;15m,[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mres3[39m[38;5;186m"[39m[38;5;15m,[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mres4[39m[38;5;186m"[39m[38;5;15m,[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mres5[39m[38;5;186m"[39m[38;5;15m][39m
[38;5;15m    [39m[38;5;242m# NORM: "SyncBN"[39m
[38;5;15m    [39m[38;5;197mRES5_MULTI_GRID[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m1[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15m2[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15m4[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;197mCLIP_ADAPTER[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCLIP_ENSEMBLE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mTrue[39m
[38;5;15m    [39m[38;5;197mCLIP_ENSEMBLE_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.8[39m
[38;5;197mINPUT[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mMIN_SIZE_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;81m!!python/object/apply:eval[39m[38;5;15m [39m[38;5;15m[[39m[38;5;186m"[39m[38;5;186m[int(x[39m[38;5;15m [39m[38;5;186m*[39m[38;5;15m [39m[38;5;186m0.1[39m[38;5;15m [39m[38;5;186m*[39m[38;5;15m [39m[38;5;186m512)[39m[38;5;15m [39m[38;5;186mfor[39m[38;5;15m [39m[38;5;186mx[39m[38;5;15m [39m[38;5;186min[39m[38;5;15m [39m[38;5;186mrange(5,[39m[38;5;15m [39m[38;5;186m16)][39m[38;5;186m"[39m[38;5;15m][39m
[38;5;197mOUTPUT_DIR[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mdeop/train_log[39m[38;5;186m"[39m
[38;5;197mSOLVER[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mIMS_PER_BATCH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m8[39m
[38;5;15m  [39m[38;5;197mTEST_IMS_PER_BATCH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m8[39m
[38;5;197mTEST[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mEVAL_PERIOD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m5000[39m

[01/13 16:41:43] detectron2.utils.env INFO: Using a generated random seed 43989540
[01/13 16:41:44] mask_former.modeling.clip_adapter INFO: Prompt Learner training params: ['prefix_prompt']
[01/13 16:41:50] detectron2.engine.defaults INFO: Model:
ZeroShotClipfeatUpsample_addnorm_vit16Query2D_maskvit(
  (backbone): ResNet(
    (stem): DeepLabStem(
      (conv1): Conv2d(
        3, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
      )
      (conv3): Conv2d(
        64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
      )
    )
    (res2): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv1): Conv2d(
          128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
      )
    )
    (res3): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv1): Conv2d(
          256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
      (3): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
    )
    (res4): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
        (conv1): Conv2d(
          512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (3): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (4): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (5): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (6): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (7): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (8): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (9): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (10): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (11): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (12): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (13): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (14): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (15): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (16): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (17): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (18): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (19): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (20): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (21): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (22): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
    )
    (res5): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
        (conv1): Conv2d(
          1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
      )
    )
  )
  (sem_seg_head): ZeroShotMaskFormerHead(
    (pixel_decoder): BasePixelDecoder(
      (adapter_1): Conv2d(
        256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (layer_1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (adapter_2): Conv2d(
        512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (layer_2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (adapter_3): Conv2d(
        1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (layer_3): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (layer_4): Conv2d(
        2048, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (mask_features): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
    (predictor): ZeroShotTransformerPredictor(
      (pe_layer): PositionEmbeddingSine()
      (transformer): Transformer(
        (encoder): TransformerEncoder(
          (layers): ModuleList()
        )
        (decoder): TransformerDecoder(
          (layers): ModuleList(
            (0-5): 6 x TransformerDecoderLayer(
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (multihead_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (linear1): Linear(in_features=256, out_features=2048, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (linear2): Linear(in_features=2048, out_features=256, bias=True)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (dropout1): Dropout(p=0.1, inplace=False)
              (dropout2): Dropout(p=0.1, inplace=False)
              (dropout3): Dropout(p=0.1, inplace=False)
            )
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (query_embed): Embedding(100, 256)
      (input_proj): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
      (mask_embed): MLP(
        (layers): ModuleList(
          (0-2): 3 x Linear(in_features=256, out_features=256, bias=True)
        )
      )
      (class_embed): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=1024, bias=True)
          (1): Linear(in_features=1024, out_features=512, bias=True)
        )
      )
    )
  )
  (criterion): SetCriterion(
    (matcher): Matcher HungarianMatcher
        cost_class: 1
        cost_mask: 20.0
        cost_dice: 1.0
  )
  (clip_adapter): MaskFormerClipFeatureAdapter(
    (clip_model): CLIP(
      (visual): VisionTransformerLearnPosition(
        (conv1): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False)
        (ln_pre): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (transformer): TransformerVitPosition(
          (resblocks): Sequential(
            (0): ResidualAttentionBlockVitPosition(
              (attn): MultiheadAttentionLoRA(
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (1): ResidualAttentionBlockVitPosition(
              (attn): MultiheadAttentionLoRA(
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (2): ResidualAttentionBlockVitPosition(
              (attn): MultiheadAttentionLoRA(
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (3): ResidualAttentionBlockVitPosition(
              (attn): MultiheadAttentionLoRA(
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (4): ResidualAttentionBlockVitPosition(
              (attn): MultiheadAttentionLoRA(
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (5): ResidualAttentionBlockVitPosition(
              (attn): MultiheadAttentionLoRA(
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (6): ResidualAttentionBlockVitPosition(
              (attn): MultiheadAttentionLoRA(
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (7): ResidualAttentionBlockVitPosition(
              (attn): MultiheadAttentionLoRA(
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (8): ResidualAttentionBlockVitPosition(
              (attn): MultiheadAttentionLoRA(
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (9): ResidualAttentionBlockVitPosition(
              (attn): MultiheadAttentionLoRA(
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (10): ResidualAttentionBlockVitPosition(
              (attn): MultiheadAttentionLoRA(
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (11): ResidualAttentionBlockVitPosition(
              (attn): MultiheadAttentionVit(
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
        (ln_post): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (transformer): Transformer(
        (resblocks): Sequential(
          (0): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (1): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (2): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (3): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (4): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (5): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (6): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (7): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (8): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (9): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (10): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (11): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (token_embedding): Embedding(49408, 512)
      (ln_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (prompt_learner): LearnablePromptExtractor(
      prefix_prompt:16,suffix_prompt:0,dimension:512
      [Normal_Init(mu=0,std=0.02)]
    )
  )
  (decoder): TransformerDecoderLinear(
    (layers): ModuleList(
      (0): TransformerDecoderLayerLinear(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
        )
        (multihead_attn_my): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
        )
        (linear1): Linear(in_features=512, out_features=1024, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=1024, out_features=512, bias=True)
        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
    )
    (outlayer): TransformerDecoderOutLayerLinear(
      (self_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
      )
      (multihead_attn_my): MultiHeadAttentionMy(
        (dropout_F): Dropout(p=0.1, inplace=False)
      )
      (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (dropout1): Dropout(p=0.1, inplace=False)
      (dropout2): Dropout(p=0.1, inplace=False)
      (dropout3): Dropout(p=0.1, inplace=False)
    )
    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  )
  (pe_layer): PositionEmbeddingSine()
  (query_embedding): Embedding(100, 512)
)
[01/13 16:41:50] mask_former.data.dataset_mappers.mask_former_semantic_dataset_mapper INFO: [MaskFormerSemanticDatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=(256, 307, 358, 409, 460, 512, 563, 614, 665, 716, 768), max_size=2560, sample_style='choice'), RandomCrop_CategoryAreaConstraint(crop_type='absolute', crop_size=[640, 640], single_category_max_area=1.0, ignored_category=255), <detectron2.projects.point_rend.color_augmentation.ColorAugSSDTransform object at 0x7f656e39f0d0>, RandomFlip()]
[01/13 16:41:52] detectron2.data.datasets.coco INFO: Loaded 118287 images with semantic segmentation from /scratch/local/ssd/anjun/datasets/coco-stuff/images/train2017
[01/13 16:41:52] mask_former.data.build INFO: Using training sampler TrainingSampler
[01/13 16:41:52] detectron2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[01/13 16:41:52] detectron2.data.common INFO: Serializing 118287 elements to byte tensors and concatenating them all ...
[01/13 16:41:53] detectron2.data.common INFO: Serialized dataset takes 32.38 MiB
[01/13 16:41:53] detectron2.data.build INFO: Making batched data loader with batch_size=4
[01/13 16:41:54] detectron2.checkpoint.detection_checkpoint INFO: [DetectionCheckpointer] Loading from pretrained_models/deop_model_final.pth ...
[01/13 16:41:54] fvcore.common.checkpoint INFO: [Checkpointer] Loading from pretrained_models/deop_model_final.pth ...
[01/13 16:41:54] fvcore.common.checkpoint WARNING: Skip loading parameter 'criterion.empty_weight' to the model due to incompatible shapes: (157,) in the checkpoint but (172,) in the model! You might want to double check if this is expected.
[01/13 16:41:54] fvcore.common.checkpoint WARNING: Some model parameters or buffers are not found in the checkpoint:
[34mclip_adapter.clip_model.token_embedding.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.0.attn.out_proj.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.0.mlp.c_fc.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.0.mlp.c_proj.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.1.attn.out_proj.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.1.mlp.c_fc.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.1.mlp.c_proj.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.10.attn.out_proj.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.10.learned_position[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.10.mlp.c_fc.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.10.mlp.c_proj.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.11.attn.out_proj.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.11.learned_position[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.11.mlp.c_fc.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.11.mlp.c_proj.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.2.attn.out_proj.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.2.mlp.c_fc.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.2.mlp.c_proj.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.3.attn.out_proj.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.3.mlp.c_fc.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.3.mlp.c_proj.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.4.attn.out_proj.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.4.mlp.c_fc.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.4.mlp.c_proj.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.5.attn.out_proj.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.5.mlp.c_fc.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.5.mlp.c_proj.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.6.attn.out_proj.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.6.learned_position[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.6.mlp.c_fc.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.6.mlp.c_proj.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.7.attn.out_proj.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.7.learned_position[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.7.mlp.c_fc.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.7.mlp.c_proj.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.8.attn.out_proj.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.8.learned_position[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.8.mlp.c_fc.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.8.mlp.c_proj.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.9.attn.out_proj.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.9.learned_position[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.9.mlp.c_fc.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.9.mlp.c_proj.{lora_A, lora_B}[0m
[34mcriterion.empty_weight[0m
[01/13 16:41:54] detectron2.engine.train_loop INFO: Starting training from iteration 0
[01/13 16:42:11] detectron2.engine.train_loop ERROR: Exception during training:
Traceback (most recent call last):
  File "/homes/55/anjun/.local/lib/python3.10/site-packages/detectron2/engine/train_loop.py", line 155, in train
    self.run_step()
  File "/scratch/local/ssd/anjun/ovseg/DeOP/train_net.py", line 361, in run_step
    loss_dict = self.model(data)
  File "/scratch/local/ssd/anjun/anaconda3/envs/ldm/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/local/ssd/anjun/anaconda3/envs/ldm/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/local/ssd/anjun/anaconda3/envs/ldm/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1515, in forward
    inputs, kwargs = self._pre_forward(*inputs, **kwargs)
  File "/scratch/local/ssd/anjun/anaconda3/envs/ldm/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1409, in _pre_forward
    if torch.is_grad_enabled() and self.reducer._rebuild_buckets():
RuntimeError: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by 
making sure all `forward` function outputs participate in calculating loss. 
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameter indices which did not receive grad for rank 3: 2 3 9 10 16 17 23 24 30 31 37 38 44 45 51 52 58 59 65 66 72 73 79 80 85 86
 In addition, you can set the environment variable TORCH_DISTRIBUTED_DEBUG to either INFO or DETAIL to print out information about which particular parameters did not receive gradient on this rank as part of this error
[01/13 16:42:11] detectron2.engine.hooks INFO: Total training time: 0:00:00 (0:00:00 on hooks)
[01/13 16:46:42] detectron2 INFO: Rank of current process: 3. World size: 4
[01/13 16:46:48] detectron2 INFO: Environment info:
-------------------------------  --------------------------------------------------------------------------------------------------
sys.platform                     linux
Python                           3.10.11 (main, May 16 2023, 00:28:57) [GCC 11.2.0]
numpy                            1.26.2
detectron2                       0.6 @/homes/55/anjun/.local/lib/python3.10/site-packages/detectron2
Compiler                         GCC 8.3
CUDA compiler                    CUDA 12.1
detectron2 arch flags            8.6
DETECTRON2_ENV_MODULE            <not set>
PyTorch                          2.1.1+cu121 @/scratch/local/ssd/anjun/anaconda3/envs/ldm/lib/python3.10/site-packages/torch
PyTorch debug build              False
torch._C._GLIBCXX_USE_CXX11_ABI  False
GPU available                    Yes
GPU 0,1,2,3                      NVIDIA A40 (arch=8.6)
Driver version                   530.30.02
CUDA_HOME                        /usr/local/cuda-12.1/
Pillow                           9.5.0
torchvision                      0.16.1+cu121 @/scratch/local/ssd/anjun/anaconda3/envs/ldm/lib/python3.10/site-packages/torchvision
torchvision arch flags           5.0, 6.0, 7.0, 7.5, 8.0, 8.6, 9.0
fvcore                           0.1.5.post20221221
iopath                           0.1.9
cv2                              4.5.4-dev
-------------------------------  --------------------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v3.1.1 (Git Hash 64f6bcbcbab628e96f33a62c3e975f8535a7bde4)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 12.1
  - NVCC architecture flags: -gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.9.2
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=12.1, CUDNN_VERSION=8.9.2, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=old-style-cast -Wno-invalid-partial-specialization -Wno-unused-private-field -Wno-aligned-allocation-unavailable -Wno-missing-braces -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.1.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[01/13 16:46:48] detectron2 INFO: Command line arguments: Namespace(config_file='configs/coco-stuff-164k-156/zero_shot_maskformer_R101c_bs32_60k_proposalmask_featupsample_img512.yaml', resume=True, eval_only=False, num_gpus=4, num_machines=1, machine_rank=0, dist_url='tcp://0.0.0.0:12237', opts=['MODEL.CLIP_ADAPTER.CLIP_ENSEMBLE', 'False', 'MODEL.CLIP_ADAPTER.CLIP_ENSEMBLE_WEIGHT', '-1.0', 'SOLVER.TEST_IMS_PER_BATCH', '1', 'OUTPUT_DIR', 'deop/train_log', 'MODEL.CLIP_ADAPTER.PROMPT_LEARNER', 'learnable', 'SOLVER.IMS_PER_BATCH', '16', 'SOLVER.BASE_LR', '0.00005', 'SOLVER.CHECKPOINT_PERIOD', '1000', 'MODEL.CLIP_ADAPTER.CLIP_MODEL_NAME', 'ViT-B/16', 'MODEL.WEIGHTS', 'pretrained_models/deop_model_final.pth', 'MODEL.META_ARCHITECTURE', 'ZeroShotClipfeatUpsample_addnorm_vit16Query2D_maskvit', 'MODEL.NUM_DECODER_LAYER', '1', 'ORACLE', 'False', 'INPUT.MIN_SIZE_TEST', '512', 'TEST.EVAL_PERIOD', '1000', 'INPUT.SIZE_DIVISIBILITY', '512', 'MODEL.MASK_FORMER.DECODER_DICE_WEIGHT', '0.8', 'MODEL.MASK_FORMER.DECODER_CE_WEIGHT', '2.0', 'MODEL.CLIP_ADAPTER.LEARN_POSITION', 'True', 'MODEL.CLIP_ADAPTER.POSITION_LAYERS', '[1,2,3,4,5,6,7,8,9,10,11]', 'MODEL.CLIP_ADAPTER.LAYERMASKVIT', '[11]', 'MODEL.CLIP_ADAPTER.END2ENDTRAIN', 'False', 'MODEL.CLIP_ADAPTER.PS_SHORTCUT', '1.0', 'DATASETS.TRAIN', "('coco_2017_train_stuff_all_sem_seg',)", 'DATASETS.TEST', "('context_59_test_sem_seg',)", 'MODEL.SEM_SEG_HEAD.NUM_CLASSES', '171', 'MODEL.MASK_FORMER.LOSS_NUM_CLASS', '171'])
[01/13 16:46:48] detectron2 INFO: Contents of args.config_file=configs/coco-stuff-164k-156/zero_shot_maskformer_R101c_bs32_60k_proposalmask_featupsample_img512.yaml:
[38;5;197m_BASE_[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mzero_shot_maskformer_R50_bs32_60k.yaml[39m
[38;5;197mMODEL[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mSEM_SEG_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mNUM_CLASSES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m171[39m
[38;5;15m  [39m[38;5;197mMETA_ARCHITECTURE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mZeroShotMaskFormerClipfeatUpsample[39m[38;5;186m"[39m
[38;5;15m  [39m[38;5;197mBACKBONE[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mbuild_resnet_deeplab_backbone[39m[38;5;186m"[39m
[38;5;15m  [39m
[38;5;15m  [39m[38;5;197mWEIGHTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mdetectron2://DeepLab/R-103.pkl[39m[38;5;186m"[39m
[38;5;15m  [39m[38;5;197mRESNETS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mDEPTH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m101[39m
[38;5;15m    [39m[38;5;197mSTEM_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mdeeplab[39m[38;5;186m"[39m
[38;5;15m    [39m[38;5;197mSTEM_OUT_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m128[39m
[38;5;15m    [39m[38;5;197mSTRIDE_IN_1X1[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mFalse[39m
[38;5;15m    [39m[38;5;197mOUT_FEATURES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;186m"[39m[38;5;186mres2[39m[38;5;186m"[39m[38;5;15m,[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mres3[39m[38;5;186m"[39m[38;5;15m,[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mres4[39m[38;5;186m"[39m[38;5;15m,[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mres5[39m[38;5;186m"[39m[38;5;15m][39m
[38;5;15m    [39m[38;5;242m# NORM: "SyncBN"[39m
[38;5;15m    [39m[38;5;197mRES5_MULTI_GRID[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m1[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15m2[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15m4[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;197mCLIP_ADAPTER[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCLIP_ENSEMBLE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mTrue[39m
[38;5;15m    [39m[38;5;197mCLIP_ENSEMBLE_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.8[39m
[38;5;197mINPUT[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mMIN_SIZE_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;81m!!python/object/apply:eval[39m[38;5;15m [39m[38;5;15m[[39m[38;5;186m"[39m[38;5;186m[int(x[39m[38;5;15m [39m[38;5;186m*[39m[38;5;15m [39m[38;5;186m0.1[39m[38;5;15m [39m[38;5;186m*[39m[38;5;15m [39m[38;5;186m512)[39m[38;5;15m [39m[38;5;186mfor[39m[38;5;15m [39m[38;5;186mx[39m[38;5;15m [39m[38;5;186min[39m[38;5;15m [39m[38;5;186mrange(5,[39m[38;5;15m [39m[38;5;186m16)][39m[38;5;186m"[39m[38;5;15m][39m
[38;5;197mOUTPUT_DIR[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mdeop/train_log[39m[38;5;186m"[39m
[38;5;197mSOLVER[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mIMS_PER_BATCH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m8[39m
[38;5;15m  [39m[38;5;197mTEST_IMS_PER_BATCH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m8[39m
[38;5;197mTEST[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mEVAL_PERIOD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m5000[39m

[01/13 16:46:48] detectron2.utils.env INFO: Using a generated random seed 48872339
[01/13 16:46:49] mask_former.modeling.clip_adapter INFO: Prompt Learner training params: ['prefix_prompt']
[01/13 16:47:27] detectron2 INFO: Rank of current process: 3. World size: 4
[01/13 16:47:33] detectron2 INFO: Environment info:
-------------------------------  --------------------------------------------------------------------------------------------------
sys.platform                     linux
Python                           3.10.11 (main, May 16 2023, 00:28:57) [GCC 11.2.0]
numpy                            1.26.2
detectron2                       0.6 @/homes/55/anjun/.local/lib/python3.10/site-packages/detectron2
Compiler                         GCC 8.3
CUDA compiler                    CUDA 12.1
detectron2 arch flags            8.6
DETECTRON2_ENV_MODULE            <not set>
PyTorch                          2.1.1+cu121 @/scratch/local/ssd/anjun/anaconda3/envs/ldm/lib/python3.10/site-packages/torch
PyTorch debug build              False
torch._C._GLIBCXX_USE_CXX11_ABI  False
GPU available                    Yes
GPU 0,1,2,3                      NVIDIA A40 (arch=8.6)
Driver version                   530.30.02
CUDA_HOME                        /usr/local/cuda-12.1/
Pillow                           9.5.0
torchvision                      0.16.1+cu121 @/scratch/local/ssd/anjun/anaconda3/envs/ldm/lib/python3.10/site-packages/torchvision
torchvision arch flags           5.0, 6.0, 7.0, 7.5, 8.0, 8.6, 9.0
fvcore                           0.1.5.post20221221
iopath                           0.1.9
cv2                              4.5.4-dev
-------------------------------  --------------------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v3.1.1 (Git Hash 64f6bcbcbab628e96f33a62c3e975f8535a7bde4)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 12.1
  - NVCC architecture flags: -gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.9.2
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=12.1, CUDNN_VERSION=8.9.2, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=old-style-cast -Wno-invalid-partial-specialization -Wno-unused-private-field -Wno-aligned-allocation-unavailable -Wno-missing-braces -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.1.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[01/13 16:47:33] detectron2 INFO: Command line arguments: Namespace(config_file='configs/coco-stuff-164k-156/zero_shot_maskformer_R101c_bs32_60k_proposalmask_featupsample_img512.yaml', resume=True, eval_only=False, num_gpus=4, num_machines=1, machine_rank=0, dist_url='tcp://0.0.0.0:12237', opts=['MODEL.CLIP_ADAPTER.CLIP_ENSEMBLE', 'False', 'MODEL.CLIP_ADAPTER.CLIP_ENSEMBLE_WEIGHT', '-1.0', 'SOLVER.TEST_IMS_PER_BATCH', '1', 'OUTPUT_DIR', 'deop/train_log', 'MODEL.CLIP_ADAPTER.PROMPT_LEARNER', 'learnable', 'SOLVER.IMS_PER_BATCH', '16', 'SOLVER.BASE_LR', '0.00005', 'SOLVER.CHECKPOINT_PERIOD', '1000', 'MODEL.CLIP_ADAPTER.CLIP_MODEL_NAME', 'ViT-B/16', 'MODEL.WEIGHTS', 'pretrained_models/deop_model_final.pth', 'MODEL.META_ARCHITECTURE', 'ZeroShotClipfeatUpsample_addnorm_vit16Query2D_maskvit', 'MODEL.NUM_DECODER_LAYER', '1', 'ORACLE', 'False', 'INPUT.MIN_SIZE_TEST', '512', 'TEST.EVAL_PERIOD', '1000', 'INPUT.SIZE_DIVISIBILITY', '512', 'MODEL.MASK_FORMER.DECODER_DICE_WEIGHT', '0.8', 'MODEL.MASK_FORMER.DECODER_CE_WEIGHT', '2.0', 'MODEL.CLIP_ADAPTER.LEARN_POSITION', 'True', 'MODEL.CLIP_ADAPTER.POSITION_LAYERS', '[1,2,3,4,5,6,7,8,9,10,11]', 'MODEL.CLIP_ADAPTER.LAYERMASKVIT', '[11]', 'MODEL.CLIP_ADAPTER.END2ENDTRAIN', 'False', 'MODEL.CLIP_ADAPTER.PS_SHORTCUT', '1.0', 'DATASETS.TRAIN', "('coco_2017_train_stuff_all_sem_seg',)", 'DATASETS.TEST', "('context_59_test_sem_seg',)", 'MODEL.SEM_SEG_HEAD.NUM_CLASSES', '171', 'MODEL.MASK_FORMER.LOSS_NUM_CLASS', '171'])
[01/13 16:47:33] detectron2 INFO: Contents of args.config_file=configs/coco-stuff-164k-156/zero_shot_maskformer_R101c_bs32_60k_proposalmask_featupsample_img512.yaml:
[38;5;197m_BASE_[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mzero_shot_maskformer_R50_bs32_60k.yaml[39m
[38;5;197mMODEL[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mSEM_SEG_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mNUM_CLASSES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m171[39m
[38;5;15m  [39m[38;5;197mMETA_ARCHITECTURE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mZeroShotMaskFormerClipfeatUpsample[39m[38;5;186m"[39m
[38;5;15m  [39m[38;5;197mBACKBONE[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mbuild_resnet_deeplab_backbone[39m[38;5;186m"[39m
[38;5;15m  [39m
[38;5;15m  [39m[38;5;197mWEIGHTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mdetectron2://DeepLab/R-103.pkl[39m[38;5;186m"[39m
[38;5;15m  [39m[38;5;197mRESNETS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mDEPTH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m101[39m
[38;5;15m    [39m[38;5;197mSTEM_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mdeeplab[39m[38;5;186m"[39m
[38;5;15m    [39m[38;5;197mSTEM_OUT_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m128[39m
[38;5;15m    [39m[38;5;197mSTRIDE_IN_1X1[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mFalse[39m
[38;5;15m    [39m[38;5;197mOUT_FEATURES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;186m"[39m[38;5;186mres2[39m[38;5;186m"[39m[38;5;15m,[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mres3[39m[38;5;186m"[39m[38;5;15m,[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mres4[39m[38;5;186m"[39m[38;5;15m,[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mres5[39m[38;5;186m"[39m[38;5;15m][39m
[38;5;15m    [39m[38;5;242m# NORM: "SyncBN"[39m
[38;5;15m    [39m[38;5;197mRES5_MULTI_GRID[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m1[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15m2[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15m4[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;197mCLIP_ADAPTER[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCLIP_ENSEMBLE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mTrue[39m
[38;5;15m    [39m[38;5;197mCLIP_ENSEMBLE_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.8[39m
[38;5;197mINPUT[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mMIN_SIZE_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;81m!!python/object/apply:eval[39m[38;5;15m [39m[38;5;15m[[39m[38;5;186m"[39m[38;5;186m[int(x[39m[38;5;15m [39m[38;5;186m*[39m[38;5;15m [39m[38;5;186m0.1[39m[38;5;15m [39m[38;5;186m*[39m[38;5;15m [39m[38;5;186m512)[39m[38;5;15m [39m[38;5;186mfor[39m[38;5;15m [39m[38;5;186mx[39m[38;5;15m [39m[38;5;186min[39m[38;5;15m [39m[38;5;186mrange(5,[39m[38;5;15m [39m[38;5;186m16)][39m[38;5;186m"[39m[38;5;15m][39m
[38;5;197mOUTPUT_DIR[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mdeop/train_log[39m[38;5;186m"[39m
[38;5;197mSOLVER[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mIMS_PER_BATCH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m8[39m
[38;5;15m  [39m[38;5;197mTEST_IMS_PER_BATCH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m8[39m
[38;5;197mTEST[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mEVAL_PERIOD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m5000[39m

[01/13 16:47:33] detectron2.utils.env INFO: Using a generated random seed 33540605
[01/13 16:47:34] mask_former.modeling.clip_adapter INFO: Prompt Learner training params: ['prefix_prompt']
[01/13 16:47:37] detectron2.engine.defaults INFO: Model:
ZeroShotClipfeatUpsample_addnorm_vit16Query2D_maskvit(
  (backbone): ResNet(
    (stem): DeepLabStem(
      (conv1): Conv2d(
        3, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
      )
      (conv3): Conv2d(
        64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
      )
    )
    (res2): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv1): Conv2d(
          128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
      )
    )
    (res3): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv1): Conv2d(
          256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
      (3): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
    )
    (res4): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
        (conv1): Conv2d(
          512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (3): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (4): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (5): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (6): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (7): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (8): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (9): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (10): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (11): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (12): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (13): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (14): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (15): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (16): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (17): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (18): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (19): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (20): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (21): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (22): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
    )
    (res5): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
        (conv1): Conv2d(
          1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
      )
    )
  )
  (sem_seg_head): ZeroShotMaskFormerHead(
    (pixel_decoder): BasePixelDecoder(
      (adapter_1): Conv2d(
        256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (layer_1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (adapter_2): Conv2d(
        512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (layer_2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (adapter_3): Conv2d(
        1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (layer_3): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (layer_4): Conv2d(
        2048, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (mask_features): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
    (predictor): ZeroShotTransformerPredictor(
      (pe_layer): PositionEmbeddingSine()
      (transformer): Transformer(
        (encoder): TransformerEncoder(
          (layers): ModuleList()
        )
        (decoder): TransformerDecoder(
          (layers): ModuleList(
            (0-5): 6 x TransformerDecoderLayer(
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (multihead_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (linear1): Linear(in_features=256, out_features=2048, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (linear2): Linear(in_features=2048, out_features=256, bias=True)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (dropout1): Dropout(p=0.1, inplace=False)
              (dropout2): Dropout(p=0.1, inplace=False)
              (dropout3): Dropout(p=0.1, inplace=False)
            )
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (query_embed): Embedding(100, 256)
      (input_proj): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
      (mask_embed): MLP(
        (layers): ModuleList(
          (0-2): 3 x Linear(in_features=256, out_features=256, bias=True)
        )
      )
      (class_embed): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=1024, bias=True)
          (1): Linear(in_features=1024, out_features=512, bias=True)
        )
      )
    )
  )
  (criterion): SetCriterion(
    (matcher): Matcher HungarianMatcher
        cost_class: 1
        cost_mask: 20.0
        cost_dice: 1.0
  )
  (clip_adapter): MaskFormerClipFeatureAdapter(
    (clip_model): CLIP(
      (visual): VisionTransformerLearnPosition(
        (conv1): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False)
        (ln_pre): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (transformer): TransformerVitPosition(
          (resblocks): Sequential(
            (0): ResidualAttentionBlockVitPosition(
              (attn): MultiheadAttentionLoRA(
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (1): ResidualAttentionBlockVitPosition(
              (attn): MultiheadAttentionLoRA(
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (2): ResidualAttentionBlockVitPosition(
              (attn): MultiheadAttentionLoRA(
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (3): ResidualAttentionBlockVitPosition(
              (attn): MultiheadAttentionLoRA(
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (4): ResidualAttentionBlockVitPosition(
              (attn): MultiheadAttentionLoRA(
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (5): ResidualAttentionBlockVitPosition(
              (attn): MultiheadAttentionLoRA(
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (6): ResidualAttentionBlockVitPosition(
              (attn): MultiheadAttentionLoRA(
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (7): ResidualAttentionBlockVitPosition(
              (attn): MultiheadAttentionLoRA(
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (8): ResidualAttentionBlockVitPosition(
              (attn): MultiheadAttentionLoRA(
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (9): ResidualAttentionBlockVitPosition(
              (attn): MultiheadAttentionLoRA(
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (10): ResidualAttentionBlockVitPosition(
              (attn): MultiheadAttentionLoRA(
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (11): ResidualAttentionBlockVitPosition(
              (attn): MultiheadAttentionVit(
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
        (ln_post): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (transformer): Transformer(
        (resblocks): Sequential(
          (0): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (1): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (2): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (3): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (4): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (5): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (6): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (7): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (8): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (9): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (10): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (11): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (token_embedding): Embedding(49408, 512)
      (ln_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (prompt_learner): LearnablePromptExtractor(
      prefix_prompt:16,suffix_prompt:0,dimension:512
      [Normal_Init(mu=0,std=0.02)]
    )
  )
  (decoder): TransformerDecoderLinear(
    (layers): ModuleList(
      (0): TransformerDecoderLayerLinear(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
        )
        (multihead_attn_my): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
        )
        (linear1): Linear(in_features=512, out_features=1024, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=1024, out_features=512, bias=True)
        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
    )
    (outlayer): TransformerDecoderOutLayerLinear(
      (self_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
      )
      (multihead_attn_my): MultiHeadAttentionMy(
        (dropout_F): Dropout(p=0.1, inplace=False)
      )
      (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (dropout1): Dropout(p=0.1, inplace=False)
      (dropout2): Dropout(p=0.1, inplace=False)
      (dropout3): Dropout(p=0.1, inplace=False)
    )
    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  )
  (pe_layer): PositionEmbeddingSine()
  (query_embedding): Embedding(100, 512)
)
[01/13 16:47:38] mask_former.data.dataset_mappers.mask_former_semantic_dataset_mapper INFO: [MaskFormerSemanticDatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=(256, 307, 358, 409, 460, 512, 563, 614, 665, 716, 768), max_size=2560, sample_style='choice'), RandomCrop_CategoryAreaConstraint(crop_type='absolute', crop_size=[640, 640], single_category_max_area=1.0, ignored_category=255), <detectron2.projects.point_rend.color_augmentation.ColorAugSSDTransform object at 0x7fed1df9afe0>, RandomFlip()]
[01/13 16:47:40] detectron2.data.datasets.coco INFO: Loaded 118287 images with semantic segmentation from /scratch/local/ssd/anjun/datasets/coco-stuff/images/train2017
[01/13 16:47:40] mask_former.data.build INFO: Using training sampler TrainingSampler
[01/13 16:47:40] detectron2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[01/13 16:47:40] detectron2.data.common INFO: Serializing 118287 elements to byte tensors and concatenating them all ...
[01/13 16:47:40] detectron2.data.common INFO: Serialized dataset takes 32.38 MiB
[01/13 16:47:40] detectron2.data.build INFO: Making batched data loader with batch_size=4
[01/13 16:47:41] detectron2.checkpoint.detection_checkpoint INFO: [DetectionCheckpointer] Loading from pretrained_models/deop_model_final.pth ...
[01/13 16:47:41] fvcore.common.checkpoint INFO: [Checkpointer] Loading from pretrained_models/deop_model_final.pth ...
[01/13 16:47:41] fvcore.common.checkpoint WARNING: Skip loading parameter 'criterion.empty_weight' to the model due to incompatible shapes: (157,) in the checkpoint but (172,) in the model! You might want to double check if this is expected.
[01/13 16:47:41] fvcore.common.checkpoint WARNING: Some model parameters or buffers are not found in the checkpoint:
[34mclip_adapter.clip_model.token_embedding.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.0.mlp.c_fc.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.0.mlp.c_proj.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.1.mlp.c_fc.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.1.mlp.c_proj.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.10.learned_position[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.10.mlp.c_fc.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.10.mlp.c_proj.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.11.learned_position[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.11.mlp.c_fc.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.11.mlp.c_proj.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.2.mlp.c_fc.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.2.mlp.c_proj.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.3.mlp.c_fc.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.3.mlp.c_proj.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.4.mlp.c_fc.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.4.mlp.c_proj.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.5.mlp.c_fc.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.5.mlp.c_proj.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.6.learned_position[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.6.mlp.c_fc.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.6.mlp.c_proj.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.7.learned_position[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.7.mlp.c_fc.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.7.mlp.c_proj.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.8.learned_position[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.8.mlp.c_fc.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.8.mlp.c_proj.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.9.learned_position[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.9.mlp.c_fc.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.9.mlp.c_proj.{lora_A, lora_B}[0m
[34mcriterion.empty_weight[0m
[01/13 16:47:41] detectron2.engine.train_loop INFO: Starting training from iteration 0
[01/13 16:47:59] detectron2.engine.train_loop ERROR: Exception during training:
Traceback (most recent call last):
  File "/homes/55/anjun/.local/lib/python3.10/site-packages/detectron2/engine/train_loop.py", line 155, in train
    self.run_step()
  File "/scratch/local/ssd/anjun/ovseg/DeOP/train_net.py", line 361, in run_step
    loss_dict = self.model(data)
  File "/scratch/local/ssd/anjun/anaconda3/envs/ldm/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/local/ssd/anjun/anaconda3/envs/ldm/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/local/ssd/anjun/anaconda3/envs/ldm/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1515, in forward
    inputs, kwargs = self._pre_forward(*inputs, **kwargs)
  File "/scratch/local/ssd/anjun/anaconda3/envs/ldm/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1409, in _pre_forward
    if torch.is_grad_enabled() and self.reducer._rebuild_buckets():
RuntimeError: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by 
making sure all `forward` function outputs participate in calculating loss. 
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameter indices which did not receive grad for rank 3: 61 62
 In addition, you can set the environment variable TORCH_DISTRIBUTED_DEBUG to either INFO or DETAIL to print out information about which particular parameters did not receive gradient on this rank as part of this error
[01/13 16:47:59] detectron2.engine.hooks INFO: Total training time: 0:00:00 (0:00:00 on hooks)
[01/13 16:49:14] detectron2 INFO: Rank of current process: 3. World size: 4
[01/13 16:49:21] detectron2 INFO: Environment info:
-------------------------------  --------------------------------------------------------------------------------------------------
sys.platform                     linux
Python                           3.10.11 (main, May 16 2023, 00:28:57) [GCC 11.2.0]
numpy                            1.26.2
detectron2                       0.6 @/homes/55/anjun/.local/lib/python3.10/site-packages/detectron2
Compiler                         GCC 8.3
CUDA compiler                    CUDA 12.1
detectron2 arch flags            8.6
DETECTRON2_ENV_MODULE            <not set>
PyTorch                          2.1.1+cu121 @/scratch/local/ssd/anjun/anaconda3/envs/ldm/lib/python3.10/site-packages/torch
PyTorch debug build              False
torch._C._GLIBCXX_USE_CXX11_ABI  False
GPU available                    Yes
GPU 0,1,2,3                      NVIDIA A40 (arch=8.6)
Driver version                   530.30.02
CUDA_HOME                        /usr/local/cuda-12.1/
Pillow                           9.5.0
torchvision                      0.16.1+cu121 @/scratch/local/ssd/anjun/anaconda3/envs/ldm/lib/python3.10/site-packages/torchvision
torchvision arch flags           5.0, 6.0, 7.0, 7.5, 8.0, 8.6, 9.0
fvcore                           0.1.5.post20221221
iopath                           0.1.9
cv2                              4.5.4-dev
-------------------------------  --------------------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v3.1.1 (Git Hash 64f6bcbcbab628e96f33a62c3e975f8535a7bde4)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 12.1
  - NVCC architecture flags: -gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.9.2
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=12.1, CUDNN_VERSION=8.9.2, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=old-style-cast -Wno-invalid-partial-specialization -Wno-unused-private-field -Wno-aligned-allocation-unavailable -Wno-missing-braces -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.1.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[01/13 16:49:21] detectron2 INFO: Command line arguments: Namespace(config_file='configs/coco-stuff-164k-156/zero_shot_maskformer_R101c_bs32_60k_proposalmask_featupsample_img512.yaml', resume=True, eval_only=False, num_gpus=4, num_machines=1, machine_rank=0, dist_url='tcp://0.0.0.0:12237', opts=['MODEL.CLIP_ADAPTER.CLIP_ENSEMBLE', 'False', 'MODEL.CLIP_ADAPTER.CLIP_ENSEMBLE_WEIGHT', '-1.0', 'SOLVER.TEST_IMS_PER_BATCH', '1', 'OUTPUT_DIR', 'deop/train_log', 'MODEL.CLIP_ADAPTER.PROMPT_LEARNER', 'learnable', 'SOLVER.IMS_PER_BATCH', '16', 'SOLVER.BASE_LR', '0.00005', 'SOLVER.CHECKPOINT_PERIOD', '1000', 'MODEL.CLIP_ADAPTER.CLIP_MODEL_NAME', 'ViT-B/16', 'MODEL.WEIGHTS', 'pretrained_models/deop_model_final.pth', 'MODEL.META_ARCHITECTURE', 'ZeroShotClipfeatUpsample_addnorm_vit16Query2D_maskvit', 'MODEL.NUM_DECODER_LAYER', '1', 'ORACLE', 'False', 'INPUT.MIN_SIZE_TEST', '512', 'TEST.EVAL_PERIOD', '1000', 'INPUT.SIZE_DIVISIBILITY', '512', 'MODEL.MASK_FORMER.DECODER_DICE_WEIGHT', '0.8', 'MODEL.MASK_FORMER.DECODER_CE_WEIGHT', '2.0', 'MODEL.CLIP_ADAPTER.LEARN_POSITION', 'True', 'MODEL.CLIP_ADAPTER.POSITION_LAYERS', '[1,2,3,4,5,6,7,8,9,10,11]', 'MODEL.CLIP_ADAPTER.LAYERMASKVIT', '[11]', 'MODEL.CLIP_ADAPTER.END2ENDTRAIN', 'False', 'MODEL.CLIP_ADAPTER.PS_SHORTCUT', '1.0', 'DATASETS.TRAIN', "('coco_2017_train_stuff_all_sem_seg',)", 'DATASETS.TEST', "('context_59_test_sem_seg',)", 'MODEL.SEM_SEG_HEAD.NUM_CLASSES', '171', 'MODEL.MASK_FORMER.LOSS_NUM_CLASS', '171'])
[01/13 16:49:21] detectron2 INFO: Contents of args.config_file=configs/coco-stuff-164k-156/zero_shot_maskformer_R101c_bs32_60k_proposalmask_featupsample_img512.yaml:
[38;5;197m_BASE_[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mzero_shot_maskformer_R50_bs32_60k.yaml[39m
[38;5;197mMODEL[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mSEM_SEG_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mNUM_CLASSES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m171[39m
[38;5;15m  [39m[38;5;197mMETA_ARCHITECTURE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mZeroShotMaskFormerClipfeatUpsample[39m[38;5;186m"[39m
[38;5;15m  [39m[38;5;197mBACKBONE[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mbuild_resnet_deeplab_backbone[39m[38;5;186m"[39m
[38;5;15m  [39m
[38;5;15m  [39m[38;5;197mWEIGHTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mdetectron2://DeepLab/R-103.pkl[39m[38;5;186m"[39m
[38;5;15m  [39m[38;5;197mRESNETS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mDEPTH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m101[39m
[38;5;15m    [39m[38;5;197mSTEM_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mdeeplab[39m[38;5;186m"[39m
[38;5;15m    [39m[38;5;197mSTEM_OUT_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m128[39m
[38;5;15m    [39m[38;5;197mSTRIDE_IN_1X1[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mFalse[39m
[38;5;15m    [39m[38;5;197mOUT_FEATURES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;186m"[39m[38;5;186mres2[39m[38;5;186m"[39m[38;5;15m,[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mres3[39m[38;5;186m"[39m[38;5;15m,[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mres4[39m[38;5;186m"[39m[38;5;15m,[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mres5[39m[38;5;186m"[39m[38;5;15m][39m
[38;5;15m    [39m[38;5;242m# NORM: "SyncBN"[39m
[38;5;15m    [39m[38;5;197mRES5_MULTI_GRID[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m1[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15m2[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15m4[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;197mCLIP_ADAPTER[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCLIP_ENSEMBLE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mTrue[39m
[38;5;15m    [39m[38;5;197mCLIP_ENSEMBLE_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.8[39m
[38;5;197mINPUT[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mMIN_SIZE_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;81m!!python/object/apply:eval[39m[38;5;15m [39m[38;5;15m[[39m[38;5;186m"[39m[38;5;186m[int(x[39m[38;5;15m [39m[38;5;186m*[39m[38;5;15m [39m[38;5;186m0.1[39m[38;5;15m [39m[38;5;186m*[39m[38;5;15m [39m[38;5;186m512)[39m[38;5;15m [39m[38;5;186mfor[39m[38;5;15m [39m[38;5;186mx[39m[38;5;15m [39m[38;5;186min[39m[38;5;15m [39m[38;5;186mrange(5,[39m[38;5;15m [39m[38;5;186m16)][39m[38;5;186m"[39m[38;5;15m][39m
[38;5;197mOUTPUT_DIR[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mdeop/train_log[39m[38;5;186m"[39m
[38;5;197mSOLVER[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mIMS_PER_BATCH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m8[39m
[38;5;15m  [39m[38;5;197mTEST_IMS_PER_BATCH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m8[39m
[38;5;197mTEST[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mEVAL_PERIOD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m5000[39m

[01/13 16:49:21] detectron2.utils.env INFO: Using a generated random seed 21336076
[01/13 16:49:21] mask_former.modeling.clip_adapter INFO: Prompt Learner training params: ['prefix_prompt']
[01/13 16:49:26] detectron2.engine.defaults INFO: Model:
ZeroShotClipfeatUpsample_addnorm_vit16Query2D_maskvit(
  (backbone): ResNet(
    (stem): DeepLabStem(
      (conv1): Conv2d(
        3, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
      )
      (conv3): Conv2d(
        64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
      )
    )
    (res2): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv1): Conv2d(
          128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
      )
    )
    (res3): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv1): Conv2d(
          256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
      (3): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
    )
    (res4): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
        (conv1): Conv2d(
          512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (3): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (4): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (5): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (6): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (7): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (8): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (9): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (10): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (11): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (12): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (13): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (14): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (15): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (16): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (17): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (18): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (19): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (20): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (21): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (22): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
    )
    (res5): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
        (conv1): Conv2d(
          1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
      )
    )
  )
  (sem_seg_head): ZeroShotMaskFormerHead(
    (pixel_decoder): BasePixelDecoder(
      (adapter_1): Conv2d(
        256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (layer_1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (adapter_2): Conv2d(
        512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (layer_2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (adapter_3): Conv2d(
        1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (layer_3): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (layer_4): Conv2d(
        2048, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (mask_features): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
    (predictor): ZeroShotTransformerPredictor(
      (pe_layer): PositionEmbeddingSine()
      (transformer): Transformer(
        (encoder): TransformerEncoder(
          (layers): ModuleList()
        )
        (decoder): TransformerDecoder(
          (layers): ModuleList(
            (0-5): 6 x TransformerDecoderLayer(
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (multihead_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (linear1): Linear(in_features=256, out_features=2048, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (linear2): Linear(in_features=2048, out_features=256, bias=True)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (dropout1): Dropout(p=0.1, inplace=False)
              (dropout2): Dropout(p=0.1, inplace=False)
              (dropout3): Dropout(p=0.1, inplace=False)
            )
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (query_embed): Embedding(100, 256)
      (input_proj): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
      (mask_embed): MLP(
        (layers): ModuleList(
          (0-2): 3 x Linear(in_features=256, out_features=256, bias=True)
        )
      )
      (class_embed): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=1024, bias=True)
          (1): Linear(in_features=1024, out_features=512, bias=True)
        )
      )
    )
  )
  (criterion): SetCriterion(
    (matcher): Matcher HungarianMatcher
        cost_class: 1
        cost_mask: 20.0
        cost_dice: 1.0
  )
  (clip_adapter): MaskFormerClipFeatureAdapter(
    (clip_model): CLIP(
      (visual): VisionTransformerLearnPosition(
        (conv1): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False)
        (ln_pre): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (transformer): TransformerVitPosition(
          (resblocks): Sequential(
            (0): ResidualAttentionBlockVitPosition(
              (attn): MultiheadAttentionLoRA(
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (1): ResidualAttentionBlockVitPosition(
              (attn): MultiheadAttentionLoRA(
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (2): ResidualAttentionBlockVitPosition(
              (attn): MultiheadAttentionLoRA(
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (3): ResidualAttentionBlockVitPosition(
              (attn): MultiheadAttentionLoRA(
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (4): ResidualAttentionBlockVitPosition(
              (attn): MultiheadAttentionLoRA(
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (5): ResidualAttentionBlockVitPosition(
              (attn): MultiheadAttentionLoRA(
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (6): ResidualAttentionBlockVitPosition(
              (attn): MultiheadAttentionLoRA(
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (7): ResidualAttentionBlockVitPosition(
              (attn): MultiheadAttentionLoRA(
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (8): ResidualAttentionBlockVitPosition(
              (attn): MultiheadAttentionLoRA(
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (9): ResidualAttentionBlockVitPosition(
              (attn): MultiheadAttentionLoRA(
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (10): ResidualAttentionBlockVitPosition(
              (attn): MultiheadAttentionLoRA(
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (11): ResidualAttentionBlockVitPosition(
              (attn): MultiheadAttentionVit(
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
        (ln_post): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (transformer): Transformer(
        (resblocks): Sequential(
          (0): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (1): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (2): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (3): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (4): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (5): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (6): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (7): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (8): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (9): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (10): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (11): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (token_embedding): Embedding(49408, 512)
      (ln_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (prompt_learner): LearnablePromptExtractor(
      prefix_prompt:16,suffix_prompt:0,dimension:512
      [Normal_Init(mu=0,std=0.02)]
    )
  )
  (decoder): TransformerDecoderLinear(
    (layers): ModuleList(
      (0): TransformerDecoderLayerLinear(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
        )
        (multihead_attn_my): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
        )
        (linear1): Linear(in_features=512, out_features=1024, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=1024, out_features=512, bias=True)
        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
    )
    (outlayer): TransformerDecoderOutLayerLinear(
      (self_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
      )
      (multihead_attn_my): MultiHeadAttentionMy(
        (dropout_F): Dropout(p=0.1, inplace=False)
      )
      (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (dropout1): Dropout(p=0.1, inplace=False)
      (dropout2): Dropout(p=0.1, inplace=False)
      (dropout3): Dropout(p=0.1, inplace=False)
    )
    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  )
  (pe_layer): PositionEmbeddingSine()
  (query_embedding): Embedding(100, 512)
)
[01/13 16:49:26] mask_former.data.dataset_mappers.mask_former_semantic_dataset_mapper INFO: [MaskFormerSemanticDatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=(256, 307, 358, 409, 460, 512, 563, 614, 665, 716, 768), max_size=2560, sample_style='choice'), RandomCrop_CategoryAreaConstraint(crop_type='absolute', crop_size=[640, 640], single_category_max_area=1.0, ignored_category=255), <detectron2.projects.point_rend.color_augmentation.ColorAugSSDTransform object at 0x7fdf30343220>, RandomFlip()]
[01/13 16:49:28] detectron2.data.datasets.coco INFO: Loaded 118287 images with semantic segmentation from /scratch/local/ssd/anjun/datasets/coco-stuff/images/train2017
[01/13 16:49:28] mask_former.data.build INFO: Using training sampler TrainingSampler
[01/13 16:49:28] detectron2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[01/13 16:49:28] detectron2.data.common INFO: Serializing 118287 elements to byte tensors and concatenating them all ...
[01/13 16:49:29] detectron2.data.common INFO: Serialized dataset takes 32.38 MiB
[01/13 16:49:29] detectron2.data.build INFO: Making batched data loader with batch_size=4
[01/13 16:49:30] detectron2.checkpoint.detection_checkpoint INFO: [DetectionCheckpointer] Loading from pretrained_models/deop_model_final.pth ...
[01/13 16:49:30] fvcore.common.checkpoint INFO: [Checkpointer] Loading from pretrained_models/deop_model_final.pth ...
[01/13 16:49:30] fvcore.common.checkpoint WARNING: Skip loading parameter 'criterion.empty_weight' to the model due to incompatible shapes: (157,) in the checkpoint but (172,) in the model! You might want to double check if this is expected.
[01/13 16:49:30] fvcore.common.checkpoint WARNING: Some model parameters or buffers are not found in the checkpoint:
[34mclip_adapter.clip_model.visual.transformer.resblocks.0.mlp.c_fc.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.0.mlp.c_proj.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.1.mlp.c_fc.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.1.mlp.c_proj.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.10.learned_position[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.10.mlp.c_fc.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.10.mlp.c_proj.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.11.learned_position[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.11.mlp.c_fc.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.11.mlp.c_proj.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.2.mlp.c_fc.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.2.mlp.c_proj.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.3.mlp.c_fc.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.3.mlp.c_proj.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.4.mlp.c_fc.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.4.mlp.c_proj.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.5.mlp.c_fc.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.5.mlp.c_proj.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.6.learned_position[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.6.mlp.c_fc.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.6.mlp.c_proj.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.7.learned_position[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.7.mlp.c_fc.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.7.mlp.c_proj.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.8.learned_position[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.8.mlp.c_fc.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.8.mlp.c_proj.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.9.learned_position[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.9.mlp.c_fc.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.9.mlp.c_proj.{lora_A, lora_B}[0m
[34mcriterion.empty_weight[0m
[01/13 16:49:30] detectron2.engine.train_loop INFO: Starting training from iteration 0
[01/13 17:08:38] mask_former.data.dataset_mappers.mask_former_semantic_dataset_mapper INFO: [MaskFormerSemanticDatasetMapper] Augmentations used in inference: []
[01/13 17:08:38] detectron2.data.datasets.coco WARNING: Directory /scratch/local/ssd/anjun/datasets/VOCdevkit/VOC2010/JPEGImages and /scratch/local/ssd/anjun/datasets/VOCdevkit/VOC2010/annotations_detectron2/pc59_val has 11321 and 5104 files, respectively.
[01/13 17:08:38] detectron2.data.datasets.coco WARNING: Will use their intersection of 5104 files.
[01/13 17:08:38] detectron2.data.datasets.coco INFO: Loaded 5104 images with semantic segmentation from /scratch/local/ssd/anjun/datasets/VOCdevkit/VOC2010/JPEGImages
[01/13 17:08:38] detectron2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[01/13 17:08:38] detectron2.data.common INFO: Serializing 5104 elements to byte tensors and concatenating them all ...
[01/13 17:08:38] detectron2.data.common INFO: Serialized dataset takes 1.37 MiB
[01/13 17:08:38] detectron2.data.datasets.coco WARNING: Directory /scratch/local/ssd/anjun/datasets/VOCdevkit/VOC2010/JPEGImages and /scratch/local/ssd/anjun/datasets/VOCdevkit/VOC2010/annotations_detectron2/pc59_val has 11321 and 5104 files, respectively.
[01/13 17:08:38] detectron2.data.datasets.coco WARNING: Will use their intersection of 5104 files.
[01/13 17:08:38] detectron2.data.datasets.coco INFO: Loaded 5104 images with semantic segmentation from /scratch/local/ssd/anjun/datasets/VOCdevkit/VOC2010/JPEGImages
[01/13 17:08:38] detectron2.evaluation.evaluator INFO: Start inference on 1276 batches
[01/13 17:08:46] detectron2.evaluation.evaluator INFO: Inference done 11/1276. Dataloading: 0.0009 s/iter. Inference: 0.1475 s/iter. Eval: 0.0060 s/iter. Total: 0.1545 s/iter. ETA=0:03:15
[01/13 17:08:51] detectron2.evaluation.evaluator INFO: Inference done 46/1276. Dataloading: 0.0018 s/iter. Inference: 0.1361 s/iter. Eval: 0.0067 s/iter. Total: 0.1446 s/iter. ETA=0:02:57
[01/13 17:08:56] detectron2.evaluation.evaluator INFO: Inference done 83/1276. Dataloading: 0.0016 s/iter. Inference: 0.1326 s/iter. Eval: 0.0070 s/iter. Total: 0.1413 s/iter. ETA=0:02:48
[01/13 17:09:01] detectron2.evaluation.evaluator INFO: Inference done 120/1276. Dataloading: 0.0016 s/iter. Inference: 0.1314 s/iter. Eval: 0.0069 s/iter. Total: 0.1400 s/iter. ETA=0:02:41
[01/13 17:09:06] detectron2.evaluation.evaluator INFO: Inference done 156/1276. Dataloading: 0.0016 s/iter. Inference: 0.1314 s/iter. Eval: 0.0070 s/iter. Total: 0.1401 s/iter. ETA=0:02:36
[01/13 17:09:11] detectron2.evaluation.evaluator INFO: Inference done 192/1276. Dataloading: 0.0016 s/iter. Inference: 0.1312 s/iter. Eval: 0.0071 s/iter. Total: 0.1400 s/iter. ETA=0:02:31
[01/13 17:09:17] detectron2.evaluation.evaluator INFO: Inference done 229/1276. Dataloading: 0.0016 s/iter. Inference: 0.1309 s/iter. Eval: 0.0070 s/iter. Total: 0.1396 s/iter. ETA=0:02:26
[01/13 17:09:22] detectron2.evaluation.evaluator INFO: Inference done 267/1276. Dataloading: 0.0016 s/iter. Inference: 0.1301 s/iter. Eval: 0.0069 s/iter. Total: 0.1387 s/iter. ETA=0:02:19
[01/13 17:09:27] detectron2.evaluation.evaluator INFO: Inference done 304/1276. Dataloading: 0.0016 s/iter. Inference: 0.1301 s/iter. Eval: 0.0069 s/iter. Total: 0.1386 s/iter. ETA=0:02:14
[01/13 17:09:32] detectron2.evaluation.evaluator INFO: Inference done 340/1276. Dataloading: 0.0016 s/iter. Inference: 0.1301 s/iter. Eval: 0.0069 s/iter. Total: 0.1386 s/iter. ETA=0:02:09
[01/13 17:09:37] detectron2.evaluation.evaluator INFO: Inference done 376/1276. Dataloading: 0.0016 s/iter. Inference: 0.1301 s/iter. Eval: 0.0069 s/iter. Total: 0.1387 s/iter. ETA=0:02:04
[01/13 17:09:42] detectron2.evaluation.evaluator INFO: Inference done 412/1276. Dataloading: 0.0016 s/iter. Inference: 0.1301 s/iter. Eval: 0.0070 s/iter. Total: 0.1388 s/iter. ETA=0:01:59
[01/13 17:09:47] detectron2.evaluation.evaluator INFO: Inference done 449/1276. Dataloading: 0.0016 s/iter. Inference: 0.1299 s/iter. Eval: 0.0070 s/iter. Total: 0.1386 s/iter. ETA=0:01:54
[01/13 17:09:52] detectron2.evaluation.evaluator INFO: Inference done 485/1276. Dataloading: 0.0016 s/iter. Inference: 0.1300 s/iter. Eval: 0.0070 s/iter. Total: 0.1387 s/iter. ETA=0:01:49
[01/13 17:09:57] detectron2.evaluation.evaluator INFO: Inference done 522/1276. Dataloading: 0.0016 s/iter. Inference: 0.1299 s/iter. Eval: 0.0070 s/iter. Total: 0.1386 s/iter. ETA=0:01:44
[01/13 17:10:02] detectron2.evaluation.evaluator INFO: Inference done 558/1276. Dataloading: 0.0016 s/iter. Inference: 0.1300 s/iter. Eval: 0.0070 s/iter. Total: 0.1387 s/iter. ETA=0:01:39
[01/13 17:10:07] detectron2.evaluation.evaluator INFO: Inference done 595/1276. Dataloading: 0.0016 s/iter. Inference: 0.1299 s/iter. Eval: 0.0070 s/iter. Total: 0.1386 s/iter. ETA=0:01:34
[01/13 17:10:12] detectron2.evaluation.evaluator INFO: Inference done 632/1276. Dataloading: 0.0016 s/iter. Inference: 0.1299 s/iter. Eval: 0.0069 s/iter. Total: 0.1385 s/iter. ETA=0:01:29
[01/13 17:10:17] detectron2.evaluation.evaluator INFO: Inference done 669/1276. Dataloading: 0.0016 s/iter. Inference: 0.1298 s/iter. Eval: 0.0069 s/iter. Total: 0.1384 s/iter. ETA=0:01:24
[01/13 17:10:22] detectron2.evaluation.evaluator INFO: Inference done 707/1276. Dataloading: 0.0016 s/iter. Inference: 0.1296 s/iter. Eval: 0.0069 s/iter. Total: 0.1382 s/iter. ETA=0:01:18
[01/13 17:10:27] detectron2.evaluation.evaluator INFO: Inference done 744/1276. Dataloading: 0.0016 s/iter. Inference: 0.1296 s/iter. Eval: 0.0068 s/iter. Total: 0.1381 s/iter. ETA=0:01:13
[01/13 17:10:32] detectron2.evaluation.evaluator INFO: Inference done 781/1276. Dataloading: 0.0016 s/iter. Inference: 0.1296 s/iter. Eval: 0.0068 s/iter. Total: 0.1381 s/iter. ETA=0:01:08
[01/13 17:10:37] detectron2.evaluation.evaluator INFO: Inference done 818/1276. Dataloading: 0.0016 s/iter. Inference: 0.1294 s/iter. Eval: 0.0068 s/iter. Total: 0.1379 s/iter. ETA=0:01:03
[01/13 17:10:42] detectron2.evaluation.evaluator INFO: Inference done 854/1276. Dataloading: 0.0016 s/iter. Inference: 0.1296 s/iter. Eval: 0.0068 s/iter. Total: 0.1381 s/iter. ETA=0:00:58
[01/13 17:10:48] detectron2.evaluation.evaluator INFO: Inference done 891/1276. Dataloading: 0.0016 s/iter. Inference: 0.1296 s/iter. Eval: 0.0068 s/iter. Total: 0.1381 s/iter. ETA=0:00:53
[01/13 17:10:53] detectron2.evaluation.evaluator INFO: Inference done 928/1276. Dataloading: 0.0016 s/iter. Inference: 0.1296 s/iter. Eval: 0.0068 s/iter. Total: 0.1381 s/iter. ETA=0:00:48
[01/13 17:10:58] detectron2.evaluation.evaluator INFO: Inference done 964/1276. Dataloading: 0.0016 s/iter. Inference: 0.1297 s/iter. Eval: 0.0068 s/iter. Total: 0.1382 s/iter. ETA=0:00:43
[01/13 17:11:03] detectron2.evaluation.evaluator INFO: Inference done 1000/1276. Dataloading: 0.0016 s/iter. Inference: 0.1297 s/iter. Eval: 0.0068 s/iter. Total: 0.1383 s/iter. ETA=0:00:38
[01/13 17:11:08] detectron2.evaluation.evaluator INFO: Inference done 1035/1276. Dataloading: 0.0016 s/iter. Inference: 0.1300 s/iter. Eval: 0.0068 s/iter. Total: 0.1385 s/iter. ETA=0:00:33
[01/13 17:11:13] detectron2.evaluation.evaluator INFO: Inference done 1072/1276. Dataloading: 0.0016 s/iter. Inference: 0.1299 s/iter. Eval: 0.0068 s/iter. Total: 0.1384 s/iter. ETA=0:00:28
[01/13 17:11:18] detectron2.evaluation.evaluator INFO: Inference done 1109/1276. Dataloading: 0.0016 s/iter. Inference: 0.1299 s/iter. Eval: 0.0068 s/iter. Total: 0.1384 s/iter. ETA=0:00:23
[01/13 17:11:23] detectron2.evaluation.evaluator INFO: Inference done 1146/1276. Dataloading: 0.0016 s/iter. Inference: 0.1299 s/iter. Eval: 0.0068 s/iter. Total: 0.1384 s/iter. ETA=0:00:17
[01/13 17:11:28] detectron2.evaluation.evaluator INFO: Inference done 1182/1276. Dataloading: 0.0016 s/iter. Inference: 0.1300 s/iter. Eval: 0.0068 s/iter. Total: 0.1384 s/iter. ETA=0:00:13
[01/13 17:11:33] detectron2.evaluation.evaluator INFO: Inference done 1218/1276. Dataloading: 0.0016 s/iter. Inference: 0.1300 s/iter. Eval: 0.0068 s/iter. Total: 0.1385 s/iter. ETA=0:00:08
[01/13 17:11:38] detectron2.evaluation.evaluator INFO: Inference done 1255/1276. Dataloading: 0.0016 s/iter. Inference: 0.1300 s/iter. Eval: 0.0068 s/iter. Total: 0.1385 s/iter. ETA=0:00:02
[01/13 17:11:42] detectron2.evaluation.evaluator INFO: Total inference time: 0:02:57.051067 (0.139301 s / iter per device, on 4 devices)
[01/13 17:11:42] detectron2.evaluation.evaluator INFO: Total inference pure compute time: 0:02:45 (0.130073 s / iter per device, on 4 devices)
[01/13 17:30:34] mask_former.data.dataset_mappers.mask_former_semantic_dataset_mapper INFO: [MaskFormerSemanticDatasetMapper] Augmentations used in inference: []
[01/13 17:30:34] detectron2.data.datasets.coco WARNING: Directory /scratch/local/ssd/anjun/datasets/VOCdevkit/VOC2010/JPEGImages and /scratch/local/ssd/anjun/datasets/VOCdevkit/VOC2010/annotations_detectron2/pc59_val has 11321 and 5104 files, respectively.
[01/13 17:30:34] detectron2.data.datasets.coco WARNING: Will use their intersection of 5104 files.
[01/13 17:30:34] detectron2.data.datasets.coco INFO: Loaded 5104 images with semantic segmentation from /scratch/local/ssd/anjun/datasets/VOCdevkit/VOC2010/JPEGImages
[01/13 17:30:34] detectron2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[01/13 17:30:34] detectron2.data.common INFO: Serializing 5104 elements to byte tensors and concatenating them all ...
[01/13 17:30:34] detectron2.data.common INFO: Serialized dataset takes 1.37 MiB
[01/13 17:30:34] detectron2.data.datasets.coco WARNING: Directory /scratch/local/ssd/anjun/datasets/VOCdevkit/VOC2010/JPEGImages and /scratch/local/ssd/anjun/datasets/VOCdevkit/VOC2010/annotations_detectron2/pc59_val has 11321 and 5104 files, respectively.
[01/13 17:30:34] detectron2.data.datasets.coco WARNING: Will use their intersection of 5104 files.
[01/13 17:30:34] detectron2.data.datasets.coco INFO: Loaded 5104 images with semantic segmentation from /scratch/local/ssd/anjun/datasets/VOCdevkit/VOC2010/JPEGImages
[01/13 17:30:34] detectron2.evaluation.evaluator INFO: Start inference on 1276 batches
[01/13 17:30:42] detectron2.evaluation.evaluator INFO: Inference done 11/1276. Dataloading: 0.0010 s/iter. Inference: 0.1309 s/iter. Eval: 0.0062 s/iter. Total: 0.1382 s/iter. ETA=0:02:54
[01/13 17:30:47] detectron2.evaluation.evaluator INFO: Inference done 47/1276. Dataloading: 0.0014 s/iter. Inference: 0.1315 s/iter. Eval: 0.0064 s/iter. Total: 0.1393 s/iter. ETA=0:02:51
[01/13 17:30:52] detectron2.evaluation.evaluator INFO: Inference done 84/1276. Dataloading: 0.0015 s/iter. Inference: 0.1308 s/iter. Eval: 0.0064 s/iter. Total: 0.1387 s/iter. ETA=0:02:45
[01/13 17:30:58] detectron2.evaluation.evaluator INFO: Inference done 121/1276. Dataloading: 0.0015 s/iter. Inference: 0.1304 s/iter. Eval: 0.0065 s/iter. Total: 0.1383 s/iter. ETA=0:02:39
[01/13 17:31:03] detectron2.evaluation.evaluator INFO: Inference done 156/1276. Dataloading: 0.0015 s/iter. Inference: 0.1316 s/iter. Eval: 0.0066 s/iter. Total: 0.1398 s/iter. ETA=0:02:36
[01/13 17:31:08] detectron2.evaluation.evaluator INFO: Inference done 193/1276. Dataloading: 0.0016 s/iter. Inference: 0.1311 s/iter. Eval: 0.0066 s/iter. Total: 0.1393 s/iter. ETA=0:02:30
[01/13 17:31:13] detectron2.evaluation.evaluator INFO: Inference done 229/1276. Dataloading: 0.0016 s/iter. Inference: 0.1314 s/iter. Eval: 0.0067 s/iter. Total: 0.1397 s/iter. ETA=0:02:26
[01/13 17:31:18] detectron2.evaluation.evaluator INFO: Inference done 267/1276. Dataloading: 0.0015 s/iter. Inference: 0.1305 s/iter. Eval: 0.0067 s/iter. Total: 0.1387 s/iter. ETA=0:02:19
[01/13 17:31:23] detectron2.evaluation.evaluator INFO: Inference done 304/1276. Dataloading: 0.0015 s/iter. Inference: 0.1304 s/iter. Eval: 0.0066 s/iter. Total: 0.1385 s/iter. ETA=0:02:14
[01/13 17:31:28] detectron2.evaluation.evaluator INFO: Inference done 340/1276. Dataloading: 0.0015 s/iter. Inference: 0.1307 s/iter. Eval: 0.0066 s/iter. Total: 0.1389 s/iter. ETA=0:02:10
[01/13 17:31:33] detectron2.evaluation.evaluator INFO: Inference done 376/1276. Dataloading: 0.0015 s/iter. Inference: 0.1309 s/iter. Eval: 0.0066 s/iter. Total: 0.1391 s/iter. ETA=0:02:05
[01/13 17:31:38] detectron2.evaluation.evaluator INFO: Inference done 413/1276. Dataloading: 0.0015 s/iter. Inference: 0.1309 s/iter. Eval: 0.0066 s/iter. Total: 0.1391 s/iter. ETA=0:02:00
[01/13 17:31:43] detectron2.evaluation.evaluator INFO: Inference done 450/1276. Dataloading: 0.0015 s/iter. Inference: 0.1309 s/iter. Eval: 0.0066 s/iter. Total: 0.1391 s/iter. ETA=0:01:54
[01/13 17:31:48] detectron2.evaluation.evaluator INFO: Inference done 486/1276. Dataloading: 0.0015 s/iter. Inference: 0.1310 s/iter. Eval: 0.0066 s/iter. Total: 0.1392 s/iter. ETA=0:01:49
[01/13 17:31:53] detectron2.evaluation.evaluator INFO: Inference done 523/1276. Dataloading: 0.0015 s/iter. Inference: 0.1308 s/iter. Eval: 0.0066 s/iter. Total: 0.1389 s/iter. ETA=0:01:44
[01/13 17:31:59] detectron2.evaluation.evaluator INFO: Inference done 560/1276. Dataloading: 0.0015 s/iter. Inference: 0.1307 s/iter. Eval: 0.0066 s/iter. Total: 0.1389 s/iter. ETA=0:01:39
[01/13 17:32:04] detectron2.evaluation.evaluator INFO: Inference done 596/1276. Dataloading: 0.0015 s/iter. Inference: 0.1308 s/iter. Eval: 0.0066 s/iter. Total: 0.1390 s/iter. ETA=0:01:34
[01/13 17:32:09] detectron2.evaluation.evaluator INFO: Inference done 632/1276. Dataloading: 0.0015 s/iter. Inference: 0.1310 s/iter. Eval: 0.0066 s/iter. Total: 0.1391 s/iter. ETA=0:01:29
[01/13 17:32:14] detectron2.evaluation.evaluator INFO: Inference done 668/1276. Dataloading: 0.0016 s/iter. Inference: 0.1310 s/iter. Eval: 0.0066 s/iter. Total: 0.1391 s/iter. ETA=0:01:24
[01/13 17:32:19] detectron2.evaluation.evaluator INFO: Inference done 705/1276. Dataloading: 0.0016 s/iter. Inference: 0.1309 s/iter. Eval: 0.0066 s/iter. Total: 0.1390 s/iter. ETA=0:01:19
[01/13 17:32:24] detectron2.evaluation.evaluator INFO: Inference done 741/1276. Dataloading: 0.0016 s/iter. Inference: 0.1309 s/iter. Eval: 0.0066 s/iter. Total: 0.1391 s/iter. ETA=0:01:14
[01/13 17:32:29] detectron2.evaluation.evaluator INFO: Inference done 778/1276. Dataloading: 0.0016 s/iter. Inference: 0.1309 s/iter. Eval: 0.0066 s/iter. Total: 0.1390 s/iter. ETA=0:01:09
[01/13 17:32:34] detectron2.evaluation.evaluator INFO: Inference done 815/1276. Dataloading: 0.0016 s/iter. Inference: 0.1307 s/iter. Eval: 0.0066 s/iter. Total: 0.1389 s/iter. ETA=0:01:04
[01/13 17:32:39] detectron2.evaluation.evaluator INFO: Inference done 853/1276. Dataloading: 0.0016 s/iter. Inference: 0.1305 s/iter. Eval: 0.0066 s/iter. Total: 0.1387 s/iter. ETA=0:00:58
[01/13 17:32:44] detectron2.evaluation.evaluator INFO: Inference done 889/1276. Dataloading: 0.0016 s/iter. Inference: 0.1306 s/iter. Eval: 0.0066 s/iter. Total: 0.1387 s/iter. ETA=0:00:53
[01/13 17:32:49] detectron2.evaluation.evaluator INFO: Inference done 925/1276. Dataloading: 0.0016 s/iter. Inference: 0.1306 s/iter. Eval: 0.0066 s/iter. Total: 0.1388 s/iter. ETA=0:00:48
[01/13 17:32:54] detectron2.evaluation.evaluator INFO: Inference done 961/1276. Dataloading: 0.0016 s/iter. Inference: 0.1306 s/iter. Eval: 0.0065 s/iter. Total: 0.1388 s/iter. ETA=0:00:43
[01/13 17:32:59] detectron2.evaluation.evaluator INFO: Inference done 997/1276. Dataloading: 0.0016 s/iter. Inference: 0.1307 s/iter. Eval: 0.0065 s/iter. Total: 0.1388 s/iter. ETA=0:00:38
[01/13 17:33:04] detectron2.evaluation.evaluator INFO: Inference done 1033/1276. Dataloading: 0.0016 s/iter. Inference: 0.1307 s/iter. Eval: 0.0065 s/iter. Total: 0.1389 s/iter. ETA=0:00:33
[01/13 17:33:09] detectron2.evaluation.evaluator INFO: Inference done 1069/1276. Dataloading: 0.0016 s/iter. Inference: 0.1307 s/iter. Eval: 0.0066 s/iter. Total: 0.1389 s/iter. ETA=0:00:28
[01/13 17:33:14] detectron2.evaluation.evaluator INFO: Inference done 1106/1276. Dataloading: 0.0016 s/iter. Inference: 0.1307 s/iter. Eval: 0.0066 s/iter. Total: 0.1389 s/iter. ETA=0:00:23
[01/13 17:33:19] detectron2.evaluation.evaluator INFO: Inference done 1142/1276. Dataloading: 0.0016 s/iter. Inference: 0.1307 s/iter. Eval: 0.0066 s/iter. Total: 0.1390 s/iter. ETA=0:00:18
[01/13 17:33:24] detectron2.evaluation.evaluator INFO: Inference done 1178/1276. Dataloading: 0.0016 s/iter. Inference: 0.1307 s/iter. Eval: 0.0066 s/iter. Total: 0.1390 s/iter. ETA=0:00:13
[01/13 17:33:30] detectron2.evaluation.evaluator INFO: Inference done 1214/1276. Dataloading: 0.0016 s/iter. Inference: 0.1308 s/iter. Eval: 0.0066 s/iter. Total: 0.1390 s/iter. ETA=0:00:08
[01/13 17:33:35] detectron2.evaluation.evaluator INFO: Inference done 1250/1276. Dataloading: 0.0016 s/iter. Inference: 0.1308 s/iter. Eval: 0.0066 s/iter. Total: 0.1390 s/iter. ETA=0:00:03
[01/13 17:33:39] detectron2.evaluation.evaluator INFO: Total inference time: 0:02:57.476589 (0.139635 s / iter per device, on 4 devices)
[01/13 17:33:39] detectron2.evaluation.evaluator INFO: Total inference pure compute time: 0:02:46 (0.130856 s / iter per device, on 4 devices)
[01/13 17:52:33] mask_former.data.dataset_mappers.mask_former_semantic_dataset_mapper INFO: [MaskFormerSemanticDatasetMapper] Augmentations used in inference: []
[01/13 17:52:33] detectron2.data.datasets.coco WARNING: Directory /scratch/local/ssd/anjun/datasets/VOCdevkit/VOC2010/JPEGImages and /scratch/local/ssd/anjun/datasets/VOCdevkit/VOC2010/annotations_detectron2/pc59_val has 11321 and 5104 files, respectively.
[01/13 17:52:33] detectron2.data.datasets.coco WARNING: Will use their intersection of 5104 files.
[01/13 17:52:33] detectron2.data.datasets.coco INFO: Loaded 5104 images with semantic segmentation from /scratch/local/ssd/anjun/datasets/VOCdevkit/VOC2010/JPEGImages
[01/13 17:52:33] detectron2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[01/13 17:52:33] detectron2.data.common INFO: Serializing 5104 elements to byte tensors and concatenating them all ...
[01/13 17:52:33] detectron2.data.common INFO: Serialized dataset takes 1.37 MiB
[01/13 17:52:33] detectron2.data.datasets.coco WARNING: Directory /scratch/local/ssd/anjun/datasets/VOCdevkit/VOC2010/JPEGImages and /scratch/local/ssd/anjun/datasets/VOCdevkit/VOC2010/annotations_detectron2/pc59_val has 11321 and 5104 files, respectively.
[01/13 17:52:33] detectron2.data.datasets.coco WARNING: Will use their intersection of 5104 files.
[01/13 17:52:33] detectron2.data.datasets.coco INFO: Loaded 5104 images with semantic segmentation from /scratch/local/ssd/anjun/datasets/VOCdevkit/VOC2010/JPEGImages
[01/13 17:52:33] detectron2.evaluation.evaluator INFO: Start inference on 1276 batches
[01/13 17:52:42] detectron2.evaluation.evaluator INFO: Inference done 11/1276. Dataloading: 0.0009 s/iter. Inference: 0.1272 s/iter. Eval: 0.0062 s/iter. Total: 0.1343 s/iter. ETA=0:02:49
[01/13 17:52:47] detectron2.evaluation.evaluator INFO: Inference done 47/1276. Dataloading: 0.0015 s/iter. Inference: 0.1324 s/iter. Eval: 0.0068 s/iter. Total: 0.1407 s/iter. ETA=0:02:52
[01/13 17:52:52] detectron2.evaluation.evaluator INFO: Inference done 83/1276. Dataloading: 0.0016 s/iter. Inference: 0.1317 s/iter. Eval: 0.0068 s/iter. Total: 0.1402 s/iter. ETA=0:02:47
[01/13 17:52:57] detectron2.evaluation.evaluator INFO: Inference done 120/1276. Dataloading: 0.0016 s/iter. Inference: 0.1312 s/iter. Eval: 0.0067 s/iter. Total: 0.1396 s/iter. ETA=0:02:41
[01/13 17:53:02] detectron2.evaluation.evaluator INFO: Inference done 155/1276. Dataloading: 0.0016 s/iter. Inference: 0.1322 s/iter. Eval: 0.0069 s/iter. Total: 0.1408 s/iter. ETA=0:02:37
[01/13 17:53:07] detectron2.evaluation.evaluator INFO: Inference done 192/1276. Dataloading: 0.0017 s/iter. Inference: 0.1315 s/iter. Eval: 0.0068 s/iter. Total: 0.1400 s/iter. ETA=0:02:31
[01/13 17:53:12] detectron2.evaluation.evaluator INFO: Inference done 228/1276. Dataloading: 0.0017 s/iter. Inference: 0.1316 s/iter. Eval: 0.0068 s/iter. Total: 0.1401 s/iter. ETA=0:02:26
[01/13 17:53:17] detectron2.evaluation.evaluator INFO: Inference done 267/1276. Dataloading: 0.0016 s/iter. Inference: 0.1303 s/iter. Eval: 0.0067 s/iter. Total: 0.1386 s/iter. ETA=0:02:19
[01/13 17:53:22] detectron2.evaluation.evaluator INFO: Inference done 303/1276. Dataloading: 0.0017 s/iter. Inference: 0.1303 s/iter. Eval: 0.0067 s/iter. Total: 0.1387 s/iter. ETA=0:02:14
[01/13 17:53:27] detectron2.evaluation.evaluator INFO: Inference done 339/1276. Dataloading: 0.0017 s/iter. Inference: 0.1306 s/iter. Eval: 0.0067 s/iter. Total: 0.1390 s/iter. ETA=0:02:10
[01/13 17:53:32] detectron2.evaluation.evaluator INFO: Inference done 375/1276. Dataloading: 0.0017 s/iter. Inference: 0.1309 s/iter. Eval: 0.0067 s/iter. Total: 0.1392 s/iter. ETA=0:02:05
[01/13 17:53:37] detectron2.evaluation.evaluator INFO: Inference done 411/1276. Dataloading: 0.0017 s/iter. Inference: 0.1309 s/iter. Eval: 0.0067 s/iter. Total: 0.1393 s/iter. ETA=0:02:00
[01/13 17:53:42] detectron2.evaluation.evaluator INFO: Inference done 447/1276. Dataloading: 0.0017 s/iter. Inference: 0.1309 s/iter. Eval: 0.0067 s/iter. Total: 0.1394 s/iter. ETA=0:01:55
[01/13 17:53:47] detectron2.evaluation.evaluator INFO: Inference done 483/1276. Dataloading: 0.0017 s/iter. Inference: 0.1310 s/iter. Eval: 0.0067 s/iter. Total: 0.1395 s/iter. ETA=0:01:50
[01/13 17:53:53] detectron2.evaluation.evaluator INFO: Inference done 520/1276. Dataloading: 0.0017 s/iter. Inference: 0.1309 s/iter. Eval: 0.0067 s/iter. Total: 0.1393 s/iter. ETA=0:01:45
[01/13 17:53:58] detectron2.evaluation.evaluator INFO: Inference done 557/1276. Dataloading: 0.0017 s/iter. Inference: 0.1308 s/iter. Eval: 0.0068 s/iter. Total: 0.1393 s/iter. ETA=0:01:40
[01/13 17:54:03] detectron2.evaluation.evaluator INFO: Inference done 594/1276. Dataloading: 0.0017 s/iter. Inference: 0.1308 s/iter. Eval: 0.0067 s/iter. Total: 0.1393 s/iter. ETA=0:01:34
[01/13 17:54:08] detectron2.evaluation.evaluator INFO: Inference done 630/1276. Dataloading: 0.0017 s/iter. Inference: 0.1309 s/iter. Eval: 0.0067 s/iter. Total: 0.1393 s/iter. ETA=0:01:30
[01/13 17:54:13] detectron2.evaluation.evaluator INFO: Inference done 667/1276. Dataloading: 0.0017 s/iter. Inference: 0.1308 s/iter. Eval: 0.0067 s/iter. Total: 0.1393 s/iter. ETA=0:01:24
[01/13 17:54:18] detectron2.evaluation.evaluator INFO: Inference done 704/1276. Dataloading: 0.0017 s/iter. Inference: 0.1307 s/iter. Eval: 0.0067 s/iter. Total: 0.1391 s/iter. ETA=0:01:19
[01/13 17:54:23] detectron2.evaluation.evaluator INFO: Inference done 741/1276. Dataloading: 0.0017 s/iter. Inference: 0.1307 s/iter. Eval: 0.0067 s/iter. Total: 0.1391 s/iter. ETA=0:01:14
[01/13 17:54:28] detectron2.evaluation.evaluator INFO: Inference done 777/1276. Dataloading: 0.0017 s/iter. Inference: 0.1307 s/iter. Eval: 0.0067 s/iter. Total: 0.1391 s/iter. ETA=0:01:09
[01/13 17:54:33] detectron2.evaluation.evaluator INFO: Inference done 814/1276. Dataloading: 0.0017 s/iter. Inference: 0.1306 s/iter. Eval: 0.0067 s/iter. Total: 0.1390 s/iter. ETA=0:01:04
[01/13 17:54:38] detectron2.evaluation.evaluator INFO: Inference done 851/1276. Dataloading: 0.0017 s/iter. Inference: 0.1305 s/iter. Eval: 0.0067 s/iter. Total: 0.1389 s/iter. ETA=0:00:59
[01/13 17:54:43] detectron2.evaluation.evaluator INFO: Inference done 888/1276. Dataloading: 0.0017 s/iter. Inference: 0.1305 s/iter. Eval: 0.0067 s/iter. Total: 0.1389 s/iter. ETA=0:00:53
[01/13 17:54:48] detectron2.evaluation.evaluator INFO: Inference done 924/1276. Dataloading: 0.0017 s/iter. Inference: 0.1305 s/iter. Eval: 0.0067 s/iter. Total: 0.1389 s/iter. ETA=0:00:48
[01/13 17:54:53] detectron2.evaluation.evaluator INFO: Inference done 960/1276. Dataloading: 0.0017 s/iter. Inference: 0.1306 s/iter. Eval: 0.0067 s/iter. Total: 0.1390 s/iter. ETA=0:00:43
[01/13 17:54:58] detectron2.evaluation.evaluator INFO: Inference done 996/1276. Dataloading: 0.0017 s/iter. Inference: 0.1306 s/iter. Eval: 0.0067 s/iter. Total: 0.1390 s/iter. ETA=0:00:38
[01/13 17:55:04] detectron2.evaluation.evaluator INFO: Inference done 1033/1276. Dataloading: 0.0017 s/iter. Inference: 0.1306 s/iter. Eval: 0.0067 s/iter. Total: 0.1390 s/iter. ETA=0:00:33
[01/13 17:55:09] detectron2.evaluation.evaluator INFO: Inference done 1069/1276. Dataloading: 0.0017 s/iter. Inference: 0.1306 s/iter. Eval: 0.0067 s/iter. Total: 0.1390 s/iter. ETA=0:00:28
[01/13 17:55:14] detectron2.evaluation.evaluator INFO: Inference done 1106/1276. Dataloading: 0.0017 s/iter. Inference: 0.1305 s/iter. Eval: 0.0067 s/iter. Total: 0.1389 s/iter. ETA=0:00:23
[01/13 17:55:19] detectron2.evaluation.evaluator INFO: Inference done 1142/1276. Dataloading: 0.0017 s/iter. Inference: 0.1306 s/iter. Eval: 0.0067 s/iter. Total: 0.1390 s/iter. ETA=0:00:18
[01/13 17:55:24] detectron2.evaluation.evaluator INFO: Inference done 1178/1276. Dataloading: 0.0017 s/iter. Inference: 0.1306 s/iter. Eval: 0.0067 s/iter. Total: 0.1390 s/iter. ETA=0:00:13
[01/13 17:55:29] detectron2.evaluation.evaluator INFO: Inference done 1214/1276. Dataloading: 0.0017 s/iter. Inference: 0.1307 s/iter. Eval: 0.0067 s/iter. Total: 0.1391 s/iter. ETA=0:00:08
[01/13 17:55:34] detectron2.evaluation.evaluator INFO: Inference done 1250/1276. Dataloading: 0.0017 s/iter. Inference: 0.1307 s/iter. Eval: 0.0067 s/iter. Total: 0.1391 s/iter. ETA=0:00:03
[01/13 17:55:39] detectron2.evaluation.evaluator INFO: Total inference time: 0:02:57.750885 (0.139851 s / iter per device, on 4 devices)
[01/13 17:55:39] detectron2.evaluation.evaluator INFO: Total inference pure compute time: 0:02:46 (0.130735 s / iter per device, on 4 devices)
[01/13 18:14:34] mask_former.data.dataset_mappers.mask_former_semantic_dataset_mapper INFO: [MaskFormerSemanticDatasetMapper] Augmentations used in inference: []
[01/13 18:14:34] detectron2.data.datasets.coco WARNING: Directory /scratch/local/ssd/anjun/datasets/VOCdevkit/VOC2010/JPEGImages and /scratch/local/ssd/anjun/datasets/VOCdevkit/VOC2010/annotations_detectron2/pc59_val has 11321 and 5104 files, respectively.
[01/13 18:14:34] detectron2.data.datasets.coco WARNING: Will use their intersection of 5104 files.
[01/13 18:14:34] detectron2.data.datasets.coco INFO: Loaded 5104 images with semantic segmentation from /scratch/local/ssd/anjun/datasets/VOCdevkit/VOC2010/JPEGImages
[01/13 18:14:34] detectron2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[01/13 18:14:34] detectron2.data.common INFO: Serializing 5104 elements to byte tensors and concatenating them all ...
[01/13 18:14:34] detectron2.data.common INFO: Serialized dataset takes 1.37 MiB
[01/13 18:14:34] detectron2.data.datasets.coco WARNING: Directory /scratch/local/ssd/anjun/datasets/VOCdevkit/VOC2010/JPEGImages and /scratch/local/ssd/anjun/datasets/VOCdevkit/VOC2010/annotations_detectron2/pc59_val has 11321 and 5104 files, respectively.
[01/13 18:14:34] detectron2.data.datasets.coco WARNING: Will use their intersection of 5104 files.
[01/13 18:14:34] detectron2.data.datasets.coco INFO: Loaded 5104 images with semantic segmentation from /scratch/local/ssd/anjun/datasets/VOCdevkit/VOC2010/JPEGImages
[01/13 18:14:34] detectron2.evaluation.evaluator INFO: Start inference on 1276 batches
[01/13 18:14:42] detectron2.evaluation.evaluator INFO: Inference done 11/1276. Dataloading: 0.0007 s/iter. Inference: 0.1269 s/iter. Eval: 0.0063 s/iter. Total: 0.1338 s/iter. ETA=0:02:49
[01/13 18:14:47] detectron2.evaluation.evaluator INFO: Inference done 47/1276. Dataloading: 0.0015 s/iter. Inference: 0.1323 s/iter. Eval: 0.0066 s/iter. Total: 0.1405 s/iter. ETA=0:02:52
[01/13 18:14:52] detectron2.evaluation.evaluator INFO: Inference done 84/1276. Dataloading: 0.0016 s/iter. Inference: 0.1308 s/iter. Eval: 0.0067 s/iter. Total: 0.1391 s/iter. ETA=0:02:45
[01/13 18:14:57] detectron2.evaluation.evaluator INFO: Inference done 121/1276. Dataloading: 0.0016 s/iter. Inference: 0.1300 s/iter. Eval: 0.0065 s/iter. Total: 0.1381 s/iter. ETA=0:02:39
[01/13 18:15:02] detectron2.evaluation.evaluator INFO: Inference done 156/1276. Dataloading: 0.0017 s/iter. Inference: 0.1310 s/iter. Eval: 0.0068 s/iter. Total: 0.1395 s/iter. ETA=0:02:36
[01/13 18:15:07] detectron2.evaluation.evaluator INFO: Inference done 193/1276. Dataloading: 0.0017 s/iter. Inference: 0.1304 s/iter. Eval: 0.0067 s/iter. Total: 0.1388 s/iter. ETA=0:02:30
[01/13 18:15:12] detectron2.evaluation.evaluator INFO: Inference done 228/1276. Dataloading: 0.0016 s/iter. Inference: 0.1310 s/iter. Eval: 0.0067 s/iter. Total: 0.1395 s/iter. ETA=0:02:26
[01/13 18:15:17] detectron2.evaluation.evaluator INFO: Inference done 266/1276. Dataloading: 0.0016 s/iter. Inference: 0.1300 s/iter. Eval: 0.0066 s/iter. Total: 0.1383 s/iter. ETA=0:02:19
[01/13 18:15:22] detectron2.evaluation.evaluator INFO: Inference done 302/1276. Dataloading: 0.0016 s/iter. Inference: 0.1301 s/iter. Eval: 0.0066 s/iter. Total: 0.1384 s/iter. ETA=0:02:14
[01/13 18:15:27] detectron2.evaluation.evaluator INFO: Inference done 337/1276. Dataloading: 0.0016 s/iter. Inference: 0.1306 s/iter. Eval: 0.0066 s/iter. Total: 0.1389 s/iter. ETA=0:02:10
[01/13 18:15:32] detectron2.evaluation.evaluator INFO: Inference done 373/1276. Dataloading: 0.0016 s/iter. Inference: 0.1308 s/iter. Eval: 0.0066 s/iter. Total: 0.1391 s/iter. ETA=0:02:05
[01/13 18:15:38] detectron2.evaluation.evaluator INFO: Inference done 410/1276. Dataloading: 0.0016 s/iter. Inference: 0.1307 s/iter. Eval: 0.0066 s/iter. Total: 0.1390 s/iter. ETA=0:02:00
[01/13 18:15:43] detectron2.evaluation.evaluator INFO: Inference done 446/1276. Dataloading: 0.0017 s/iter. Inference: 0.1309 s/iter. Eval: 0.0067 s/iter. Total: 0.1392 s/iter. ETA=0:01:55
[01/13 18:15:48] detectron2.evaluation.evaluator INFO: Inference done 482/1276. Dataloading: 0.0017 s/iter. Inference: 0.1310 s/iter. Eval: 0.0067 s/iter. Total: 0.1394 s/iter. ETA=0:01:50
[01/13 18:15:53] detectron2.evaluation.evaluator INFO: Inference done 519/1276. Dataloading: 0.0017 s/iter. Inference: 0.1308 s/iter. Eval: 0.0067 s/iter. Total: 0.1392 s/iter. ETA=0:01:45
[01/13 18:15:58] detectron2.evaluation.evaluator INFO: Inference done 556/1276. Dataloading: 0.0017 s/iter. Inference: 0.1308 s/iter. Eval: 0.0067 s/iter. Total: 0.1392 s/iter. ETA=0:01:40
[01/13 18:16:03] detectron2.evaluation.evaluator INFO: Inference done 593/1276. Dataloading: 0.0017 s/iter. Inference: 0.1307 s/iter. Eval: 0.0067 s/iter. Total: 0.1391 s/iter. ETA=0:01:35
[01/13 18:16:08] detectron2.evaluation.evaluator INFO: Inference done 629/1276. Dataloading: 0.0017 s/iter. Inference: 0.1308 s/iter. Eval: 0.0067 s/iter. Total: 0.1392 s/iter. ETA=0:01:30
[01/13 18:16:13] detectron2.evaluation.evaluator INFO: Inference done 666/1276. Dataloading: 0.0017 s/iter. Inference: 0.1308 s/iter. Eval: 0.0067 s/iter. Total: 0.1392 s/iter. ETA=0:01:24
[01/13 18:16:18] detectron2.evaluation.evaluator INFO: Inference done 703/1276. Dataloading: 0.0017 s/iter. Inference: 0.1306 s/iter. Eval: 0.0067 s/iter. Total: 0.1390 s/iter. ETA=0:01:19
[01/13 18:16:23] detectron2.evaluation.evaluator INFO: Inference done 740/1276. Dataloading: 0.0017 s/iter. Inference: 0.1306 s/iter. Eval: 0.0066 s/iter. Total: 0.1389 s/iter. ETA=0:01:14
[01/13 18:16:28] detectron2.evaluation.evaluator INFO: Inference done 776/1276. Dataloading: 0.0017 s/iter. Inference: 0.1306 s/iter. Eval: 0.0066 s/iter. Total: 0.1390 s/iter. ETA=0:01:09
[01/13 18:16:33] detectron2.evaluation.evaluator INFO: Inference done 813/1276. Dataloading: 0.0017 s/iter. Inference: 0.1305 s/iter. Eval: 0.0066 s/iter. Total: 0.1389 s/iter. ETA=0:01:04
[01/13 18:16:39] detectron2.evaluation.evaluator INFO: Inference done 851/1276. Dataloading: 0.0017 s/iter. Inference: 0.1304 s/iter. Eval: 0.0067 s/iter. Total: 0.1388 s/iter. ETA=0:00:58
[01/13 18:16:44] detectron2.evaluation.evaluator INFO: Inference done 888/1276. Dataloading: 0.0017 s/iter. Inference: 0.1304 s/iter. Eval: 0.0066 s/iter. Total: 0.1388 s/iter. ETA=0:00:53
[01/13 18:16:49] detectron2.evaluation.evaluator INFO: Inference done 924/1276. Dataloading: 0.0017 s/iter. Inference: 0.1304 s/iter. Eval: 0.0067 s/iter. Total: 0.1388 s/iter. ETA=0:00:48
[01/13 18:16:54] detectron2.evaluation.evaluator INFO: Inference done 960/1276. Dataloading: 0.0017 s/iter. Inference: 0.1304 s/iter. Eval: 0.0067 s/iter. Total: 0.1388 s/iter. ETA=0:00:43
[01/13 18:16:59] detectron2.evaluation.evaluator INFO: Inference done 996/1276. Dataloading: 0.0017 s/iter. Inference: 0.1305 s/iter. Eval: 0.0066 s/iter. Total: 0.1389 s/iter. ETA=0:00:38
[01/13 18:17:04] detectron2.evaluation.evaluator INFO: Inference done 1032/1276. Dataloading: 0.0017 s/iter. Inference: 0.1305 s/iter. Eval: 0.0067 s/iter. Total: 0.1389 s/iter. ETA=0:00:33
[01/13 18:17:09] detectron2.evaluation.evaluator INFO: Inference done 1068/1276. Dataloading: 0.0017 s/iter. Inference: 0.1305 s/iter. Eval: 0.0067 s/iter. Total: 0.1389 s/iter. ETA=0:00:28
[01/13 18:17:14] detectron2.evaluation.evaluator INFO: Inference done 1105/1276. Dataloading: 0.0017 s/iter. Inference: 0.1305 s/iter. Eval: 0.0067 s/iter. Total: 0.1389 s/iter. ETA=0:00:23
[01/13 18:17:19] detectron2.evaluation.evaluator INFO: Inference done 1141/1276. Dataloading: 0.0017 s/iter. Inference: 0.1305 s/iter. Eval: 0.0067 s/iter. Total: 0.1389 s/iter. ETA=0:00:18
[01/13 18:17:24] detectron2.evaluation.evaluator INFO: Inference done 1177/1276. Dataloading: 0.0017 s/iter. Inference: 0.1305 s/iter. Eval: 0.0067 s/iter. Total: 0.1389 s/iter. ETA=0:00:13
[01/13 18:17:29] detectron2.evaluation.evaluator INFO: Inference done 1213/1276. Dataloading: 0.0017 s/iter. Inference: 0.1306 s/iter. Eval: 0.0067 s/iter. Total: 0.1390 s/iter. ETA=0:00:08
[01/13 18:17:34] detectron2.evaluation.evaluator INFO: Inference done 1250/1276. Dataloading: 0.0017 s/iter. Inference: 0.1305 s/iter. Eval: 0.0067 s/iter. Total: 0.1389 s/iter. ETA=0:00:03
[01/13 18:17:39] detectron2.evaluation.evaluator INFO: Total inference time: 0:02:57.487949 (0.139644 s / iter per device, on 4 devices)
[01/13 18:17:39] detectron2.evaluation.evaluator INFO: Total inference pure compute time: 0:02:45 (0.130542 s / iter per device, on 4 devices)
[01/13 18:36:36] mask_former.data.dataset_mappers.mask_former_semantic_dataset_mapper INFO: [MaskFormerSemanticDatasetMapper] Augmentations used in inference: []
[01/13 18:36:36] detectron2.data.datasets.coco WARNING: Directory /scratch/local/ssd/anjun/datasets/VOCdevkit/VOC2010/JPEGImages and /scratch/local/ssd/anjun/datasets/VOCdevkit/VOC2010/annotations_detectron2/pc59_val has 11321 and 5104 files, respectively.
[01/13 18:36:36] detectron2.data.datasets.coco WARNING: Will use their intersection of 5104 files.
[01/13 18:36:36] detectron2.data.datasets.coco INFO: Loaded 5104 images with semantic segmentation from /scratch/local/ssd/anjun/datasets/VOCdevkit/VOC2010/JPEGImages
[01/13 18:36:36] detectron2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[01/13 18:36:36] detectron2.data.common INFO: Serializing 5104 elements to byte tensors and concatenating them all ...
[01/13 18:36:36] detectron2.data.common INFO: Serialized dataset takes 1.37 MiB
[01/13 18:36:36] detectron2.data.datasets.coco WARNING: Directory /scratch/local/ssd/anjun/datasets/VOCdevkit/VOC2010/JPEGImages and /scratch/local/ssd/anjun/datasets/VOCdevkit/VOC2010/annotations_detectron2/pc59_val has 11321 and 5104 files, respectively.
[01/13 18:36:36] detectron2.data.datasets.coco WARNING: Will use their intersection of 5104 files.
[01/13 18:36:36] detectron2.data.datasets.coco INFO: Loaded 5104 images with semantic segmentation from /scratch/local/ssd/anjun/datasets/VOCdevkit/VOC2010/JPEGImages
[01/13 18:36:36] detectron2.evaluation.evaluator INFO: Start inference on 1276 batches
[01/13 18:36:43] detectron2.evaluation.evaluator INFO: Inference done 11/1276. Dataloading: 0.0004 s/iter. Inference: 0.1238 s/iter. Eval: 0.0056 s/iter. Total: 0.1298 s/iter. ETA=0:02:44
[01/13 18:36:48] detectron2.evaluation.evaluator INFO: Inference done 47/1276. Dataloading: 0.0014 s/iter. Inference: 0.1305 s/iter. Eval: 0.0063 s/iter. Total: 0.1382 s/iter. ETA=0:02:49
[01/13 18:36:53] detectron2.evaluation.evaluator INFO: Inference done 84/1276. Dataloading: 0.0014 s/iter. Inference: 0.1300 s/iter. Eval: 0.0064 s/iter. Total: 0.1380 s/iter. ETA=0:02:44
[01/13 18:36:58] detectron2.evaluation.evaluator INFO: Inference done 121/1276. Dataloading: 0.0014 s/iter. Inference: 0.1295 s/iter. Eval: 0.0064 s/iter. Total: 0.1374 s/iter. ETA=0:02:38
[01/13 18:37:03] detectron2.evaluation.evaluator INFO: Inference done 157/1276. Dataloading: 0.0014 s/iter. Inference: 0.1302 s/iter. Eval: 0.0065 s/iter. Total: 0.1382 s/iter. ETA=0:02:34
[01/13 18:37:09] detectron2.evaluation.evaluator INFO: Inference done 194/1276. Dataloading: 0.0014 s/iter. Inference: 0.1302 s/iter. Eval: 0.0065 s/iter. Total: 0.1381 s/iter. ETA=0:02:29
[01/13 18:37:14] detectron2.evaluation.evaluator INFO: Inference done 230/1276. Dataloading: 0.0014 s/iter. Inference: 0.1304 s/iter. Eval: 0.0065 s/iter. Total: 0.1384 s/iter. ETA=0:02:24
[01/13 18:37:19] detectron2.evaluation.evaluator INFO: Inference done 268/1276. Dataloading: 0.0014 s/iter. Inference: 0.1296 s/iter. Eval: 0.0065 s/iter. Total: 0.1375 s/iter. ETA=0:02:18
[01/13 18:37:24] detectron2.evaluation.evaluator INFO: Inference done 305/1276. Dataloading: 0.0015 s/iter. Inference: 0.1297 s/iter. Eval: 0.0065 s/iter. Total: 0.1377 s/iter. ETA=0:02:13
[01/13 18:37:29] detectron2.evaluation.evaluator INFO: Inference done 341/1276. Dataloading: 0.0015 s/iter. Inference: 0.1301 s/iter. Eval: 0.0065 s/iter. Total: 0.1382 s/iter. ETA=0:02:09
[01/13 18:37:34] detectron2.evaluation.evaluator INFO: Inference done 377/1276. Dataloading: 0.0015 s/iter. Inference: 0.1302 s/iter. Eval: 0.0065 s/iter. Total: 0.1383 s/iter. ETA=0:02:04
[01/13 18:37:39] detectron2.evaluation.evaluator INFO: Inference done 414/1276. Dataloading: 0.0015 s/iter. Inference: 0.1303 s/iter. Eval: 0.0065 s/iter. Total: 0.1384 s/iter. ETA=0:01:59
[01/13 18:37:44] detectron2.evaluation.evaluator INFO: Inference done 451/1276. Dataloading: 0.0015 s/iter. Inference: 0.1303 s/iter. Eval: 0.0066 s/iter. Total: 0.1384 s/iter. ETA=0:01:54
[01/13 18:37:49] detectron2.evaluation.evaluator INFO: Inference done 487/1276. Dataloading: 0.0015 s/iter. Inference: 0.1305 s/iter. Eval: 0.0066 s/iter. Total: 0.1386 s/iter. ETA=0:01:49
[01/13 18:37:54] detectron2.evaluation.evaluator INFO: Inference done 525/1276. Dataloading: 0.0015 s/iter. Inference: 0.1302 s/iter. Eval: 0.0066 s/iter. Total: 0.1383 s/iter. ETA=0:01:43
[01/13 18:37:59] detectron2.evaluation.evaluator INFO: Inference done 562/1276. Dataloading: 0.0015 s/iter. Inference: 0.1301 s/iter. Eval: 0.0065 s/iter. Total: 0.1382 s/iter. ETA=0:01:38
[01/13 18:38:05] detectron2.evaluation.evaluator INFO: Inference done 599/1276. Dataloading: 0.0015 s/iter. Inference: 0.1301 s/iter. Eval: 0.0065 s/iter. Total: 0.1382 s/iter. ETA=0:01:33
[01/13 18:38:10] detectron2.evaluation.evaluator INFO: Inference done 636/1276. Dataloading: 0.0015 s/iter. Inference: 0.1301 s/iter. Eval: 0.0065 s/iter. Total: 0.1382 s/iter. ETA=0:01:28
[01/13 18:38:15] detectron2.evaluation.evaluator INFO: Inference done 673/1276. Dataloading: 0.0015 s/iter. Inference: 0.1300 s/iter. Eval: 0.0065 s/iter. Total: 0.1381 s/iter. ETA=0:01:23
[01/13 18:38:20] detectron2.evaluation.evaluator INFO: Inference done 711/1276. Dataloading: 0.0015 s/iter. Inference: 0.1299 s/iter. Eval: 0.0065 s/iter. Total: 0.1379 s/iter. ETA=0:01:17
[01/13 18:38:25] detectron2.evaluation.evaluator INFO: Inference done 748/1276. Dataloading: 0.0015 s/iter. Inference: 0.1299 s/iter. Eval: 0.0065 s/iter. Total: 0.1379 s/iter. ETA=0:01:12
[01/13 18:38:30] detectron2.evaluation.evaluator INFO: Inference done 785/1276. Dataloading: 0.0015 s/iter. Inference: 0.1299 s/iter. Eval: 0.0065 s/iter. Total: 0.1379 s/iter. ETA=0:01:07
[01/13 18:38:35] detectron2.evaluation.evaluator INFO: Inference done 823/1276. Dataloading: 0.0015 s/iter. Inference: 0.1297 s/iter. Eval: 0.0065 s/iter. Total: 0.1377 s/iter. ETA=0:01:02
[01/13 18:38:40] detectron2.evaluation.evaluator INFO: Inference done 860/1276. Dataloading: 0.0015 s/iter. Inference: 0.1296 s/iter. Eval: 0.0065 s/iter. Total: 0.1376 s/iter. ETA=0:00:57
[01/13 18:38:45] detectron2.evaluation.evaluator INFO: Inference done 897/1276. Dataloading: 0.0015 s/iter. Inference: 0.1296 s/iter. Eval: 0.0065 s/iter. Total: 0.1376 s/iter. ETA=0:00:52
[01/13 18:38:50] detectron2.evaluation.evaluator INFO: Inference done 933/1276. Dataloading: 0.0015 s/iter. Inference: 0.1297 s/iter. Eval: 0.0065 s/iter. Total: 0.1377 s/iter. ETA=0:00:47
[01/13 18:38:55] detectron2.evaluation.evaluator INFO: Inference done 970/1276. Dataloading: 0.0015 s/iter. Inference: 0.1297 s/iter. Eval: 0.0065 s/iter. Total: 0.1377 s/iter. ETA=0:00:42
[01/13 18:39:00] detectron2.evaluation.evaluator INFO: Inference done 1007/1276. Dataloading: 0.0015 s/iter. Inference: 0.1297 s/iter. Eval: 0.0065 s/iter. Total: 0.1377 s/iter. ETA=0:00:37
[01/13 18:39:06] detectron2.evaluation.evaluator INFO: Inference done 1043/1276. Dataloading: 0.0015 s/iter. Inference: 0.1298 s/iter. Eval: 0.0065 s/iter. Total: 0.1378 s/iter. ETA=0:00:32
[01/13 18:39:11] detectron2.evaluation.evaluator INFO: Inference done 1080/1276. Dataloading: 0.0015 s/iter. Inference: 0.1297 s/iter. Eval: 0.0065 s/iter. Total: 0.1377 s/iter. ETA=0:00:26
[01/13 18:39:16] detectron2.evaluation.evaluator INFO: Inference done 1117/1276. Dataloading: 0.0015 s/iter. Inference: 0.1296 s/iter. Eval: 0.0065 s/iter. Total: 0.1377 s/iter. ETA=0:00:21
[01/13 18:39:21] detectron2.evaluation.evaluator INFO: Inference done 1153/1276. Dataloading: 0.0016 s/iter. Inference: 0.1297 s/iter. Eval: 0.0065 s/iter. Total: 0.1378 s/iter. ETA=0:00:16
[01/13 18:39:26] detectron2.evaluation.evaluator INFO: Inference done 1190/1276. Dataloading: 0.0016 s/iter. Inference: 0.1297 s/iter. Eval: 0.0065 s/iter. Total: 0.1378 s/iter. ETA=0:00:11
[01/13 18:39:31] detectron2.evaluation.evaluator INFO: Inference done 1227/1276. Dataloading: 0.0016 s/iter. Inference: 0.1297 s/iter. Eval: 0.0065 s/iter. Total: 0.1378 s/iter. ETA=0:00:06
[01/13 18:39:36] detectron2.evaluation.evaluator INFO: Inference done 1264/1276. Dataloading: 0.0016 s/iter. Inference: 0.1297 s/iter. Eval: 0.0065 s/iter. Total: 0.1378 s/iter. ETA=0:00:01
[01/13 18:39:38] detectron2.evaluation.evaluator INFO: Total inference time: 0:02:55.934139 (0.138422 s / iter per device, on 4 devices)
[01/13 18:39:38] detectron2.evaluation.evaluator INFO: Total inference pure compute time: 0:02:44 (0.129748 s / iter per device, on 4 devices)
[01/13 18:58:35] mask_former.data.dataset_mappers.mask_former_semantic_dataset_mapper INFO: [MaskFormerSemanticDatasetMapper] Augmentations used in inference: []
[01/13 18:58:35] detectron2.data.datasets.coco WARNING: Directory /scratch/local/ssd/anjun/datasets/VOCdevkit/VOC2010/JPEGImages and /scratch/local/ssd/anjun/datasets/VOCdevkit/VOC2010/annotations_detectron2/pc59_val has 11321 and 5104 files, respectively.
[01/13 18:58:35] detectron2.data.datasets.coco WARNING: Will use their intersection of 5104 files.
[01/13 18:58:35] detectron2.data.datasets.coco INFO: Loaded 5104 images with semantic segmentation from /scratch/local/ssd/anjun/datasets/VOCdevkit/VOC2010/JPEGImages
[01/13 18:58:35] detectron2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[01/13 18:58:35] detectron2.data.common INFO: Serializing 5104 elements to byte tensors and concatenating them all ...
[01/13 18:58:35] detectron2.data.common INFO: Serialized dataset takes 1.37 MiB
[01/13 18:58:35] detectron2.data.datasets.coco WARNING: Directory /scratch/local/ssd/anjun/datasets/VOCdevkit/VOC2010/JPEGImages and /scratch/local/ssd/anjun/datasets/VOCdevkit/VOC2010/annotations_detectron2/pc59_val has 11321 and 5104 files, respectively.
[01/13 18:58:35] detectron2.data.datasets.coco WARNING: Will use their intersection of 5104 files.
[01/13 18:58:36] detectron2.data.datasets.coco INFO: Loaded 5104 images with semantic segmentation from /scratch/local/ssd/anjun/datasets/VOCdevkit/VOC2010/JPEGImages
[01/13 18:58:36] detectron2.evaluation.evaluator INFO: Start inference on 1276 batches
[01/13 18:58:43] detectron2.evaluation.evaluator INFO: Inference done 11/1276. Dataloading: 0.0008 s/iter. Inference: 0.1232 s/iter. Eval: 0.0053 s/iter. Total: 0.1293 s/iter. ETA=0:02:43
[01/13 18:58:48] detectron2.evaluation.evaluator INFO: Inference done 47/1276. Dataloading: 0.0017 s/iter. Inference: 0.1312 s/iter. Eval: 0.0063 s/iter. Total: 0.1392 s/iter. ETA=0:02:51
[01/13 18:58:53] detectron2.evaluation.evaluator INFO: Inference done 84/1276. Dataloading: 0.0015 s/iter. Inference: 0.1303 s/iter. Eval: 0.0065 s/iter. Total: 0.1383 s/iter. ETA=0:02:44
[01/13 18:58:58] detectron2.evaluation.evaluator INFO: Inference done 121/1276. Dataloading: 0.0015 s/iter. Inference: 0.1294 s/iter. Eval: 0.0064 s/iter. Total: 0.1373 s/iter. ETA=0:02:38
[01/13 18:59:03] detectron2.evaluation.evaluator INFO: Inference done 157/1276. Dataloading: 0.0015 s/iter. Inference: 0.1299 s/iter. Eval: 0.0065 s/iter. Total: 0.1379 s/iter. ETA=0:02:34
[01/13 18:59:08] detectron2.evaluation.evaluator INFO: Inference done 194/1276. Dataloading: 0.0015 s/iter. Inference: 0.1299 s/iter. Eval: 0.0065 s/iter. Total: 0.1379 s/iter. ETA=0:02:29
[01/13 18:59:13] detectron2.evaluation.evaluator INFO: Inference done 230/1276. Dataloading: 0.0015 s/iter. Inference: 0.1305 s/iter. Eval: 0.0065 s/iter. Total: 0.1386 s/iter. ETA=0:02:24
[01/13 18:59:19] detectron2.evaluation.evaluator INFO: Inference done 268/1276. Dataloading: 0.0015 s/iter. Inference: 0.1297 s/iter. Eval: 0.0065 s/iter. Total: 0.1378 s/iter. ETA=0:02:18
[01/13 18:59:24] detectron2.evaluation.evaluator INFO: Inference done 305/1276. Dataloading: 0.0015 s/iter. Inference: 0.1299 s/iter. Eval: 0.0064 s/iter. Total: 0.1379 s/iter. ETA=0:02:13
[01/13 18:59:29] detectron2.evaluation.evaluator INFO: Inference done 340/1276. Dataloading: 0.0015 s/iter. Inference: 0.1305 s/iter. Eval: 0.0066 s/iter. Total: 0.1387 s/iter. ETA=0:02:09
[01/13 18:59:34] detectron2.evaluation.evaluator INFO: Inference done 376/1276. Dataloading: 0.0016 s/iter. Inference: 0.1308 s/iter. Eval: 0.0066 s/iter. Total: 0.1390 s/iter. ETA=0:02:05
[01/13 18:59:39] detectron2.evaluation.evaluator INFO: Inference done 413/1276. Dataloading: 0.0016 s/iter. Inference: 0.1308 s/iter. Eval: 0.0066 s/iter. Total: 0.1389 s/iter. ETA=0:01:59
[01/13 18:59:44] detectron2.evaluation.evaluator INFO: Inference done 449/1276. Dataloading: 0.0016 s/iter. Inference: 0.1309 s/iter. Eval: 0.0066 s/iter. Total: 0.1391 s/iter. ETA=0:01:54
[01/13 18:59:49] detectron2.evaluation.evaluator INFO: Inference done 485/1276. Dataloading: 0.0016 s/iter. Inference: 0.1309 s/iter. Eval: 0.0066 s/iter. Total: 0.1391 s/iter. ETA=0:01:50
[01/13 18:59:54] detectron2.evaluation.evaluator INFO: Inference done 522/1276. Dataloading: 0.0016 s/iter. Inference: 0.1307 s/iter. Eval: 0.0066 s/iter. Total: 0.1389 s/iter. ETA=0:01:44
[01/13 18:59:59] detectron2.evaluation.evaluator INFO: Inference done 559/1276. Dataloading: 0.0016 s/iter. Inference: 0.1306 s/iter. Eval: 0.0066 s/iter. Total: 0.1388 s/iter. ETA=0:01:39
[01/13 19:00:04] detectron2.evaluation.evaluator INFO: Inference done 596/1276. Dataloading: 0.0016 s/iter. Inference: 0.1304 s/iter. Eval: 0.0066 s/iter. Total: 0.1387 s/iter. ETA=0:01:34
[01/13 19:00:09] detectron2.evaluation.evaluator INFO: Inference done 633/1276. Dataloading: 0.0016 s/iter. Inference: 0.1305 s/iter. Eval: 0.0066 s/iter. Total: 0.1387 s/iter. ETA=0:01:29
[01/13 19:00:14] detectron2.evaluation.evaluator INFO: Inference done 670/1276. Dataloading: 0.0016 s/iter. Inference: 0.1304 s/iter. Eval: 0.0065 s/iter. Total: 0.1385 s/iter. ETA=0:01:23
[01/13 19:00:19] detectron2.evaluation.evaluator INFO: Inference done 707/1276. Dataloading: 0.0016 s/iter. Inference: 0.1302 s/iter. Eval: 0.0065 s/iter. Total: 0.1384 s/iter. ETA=0:01:18
[01/13 19:00:25] detectron2.evaluation.evaluator INFO: Inference done 744/1276. Dataloading: 0.0016 s/iter. Inference: 0.1302 s/iter. Eval: 0.0065 s/iter. Total: 0.1384 s/iter. ETA=0:01:13
[01/13 19:00:30] detectron2.evaluation.evaluator INFO: Inference done 780/1276. Dataloading: 0.0016 s/iter. Inference: 0.1303 s/iter. Eval: 0.0066 s/iter. Total: 0.1386 s/iter. ETA=0:01:08
[01/13 19:00:35] detectron2.evaluation.evaluator INFO: Inference done 818/1276. Dataloading: 0.0016 s/iter. Inference: 0.1301 s/iter. Eval: 0.0066 s/iter. Total: 0.1384 s/iter. ETA=0:01:03
[01/13 19:00:40] detectron2.evaluation.evaluator INFO: Inference done 855/1276. Dataloading: 0.0016 s/iter. Inference: 0.1302 s/iter. Eval: 0.0066 s/iter. Total: 0.1384 s/iter. ETA=0:00:58
[01/13 19:00:45] detectron2.evaluation.evaluator INFO: Inference done 892/1276. Dataloading: 0.0016 s/iter. Inference: 0.1301 s/iter. Eval: 0.0066 s/iter. Total: 0.1384 s/iter. ETA=0:00:53
[01/13 19:00:50] detectron2.evaluation.evaluator INFO: Inference done 929/1276. Dataloading: 0.0016 s/iter. Inference: 0.1301 s/iter. Eval: 0.0066 s/iter. Total: 0.1383 s/iter. ETA=0:00:47
[01/13 19:00:55] detectron2.evaluation.evaluator INFO: Inference done 966/1276. Dataloading: 0.0016 s/iter. Inference: 0.1301 s/iter. Eval: 0.0066 s/iter. Total: 0.1383 s/iter. ETA=0:00:42
[01/13 19:01:00] detectron2.evaluation.evaluator INFO: Inference done 1002/1276. Dataloading: 0.0016 s/iter. Inference: 0.1301 s/iter. Eval: 0.0066 s/iter. Total: 0.1383 s/iter. ETA=0:00:37
[01/13 19:01:05] detectron2.evaluation.evaluator INFO: Inference done 1039/1276. Dataloading: 0.0016 s/iter. Inference: 0.1301 s/iter. Eval: 0.0066 s/iter. Total: 0.1383 s/iter. ETA=0:00:32
[01/13 19:01:10] detectron2.evaluation.evaluator INFO: Inference done 1076/1276. Dataloading: 0.0016 s/iter. Inference: 0.1301 s/iter. Eval: 0.0066 s/iter. Total: 0.1382 s/iter. ETA=0:00:27
[01/13 19:01:15] detectron2.evaluation.evaluator INFO: Inference done 1113/1276. Dataloading: 0.0016 s/iter. Inference: 0.1300 s/iter. Eval: 0.0065 s/iter. Total: 0.1381 s/iter. ETA=0:00:22
[01/13 19:01:20] detectron2.evaluation.evaluator INFO: Inference done 1149/1276. Dataloading: 0.0016 s/iter. Inference: 0.1300 s/iter. Eval: 0.0065 s/iter. Total: 0.1382 s/iter. ETA=0:00:17
[01/13 19:01:25] detectron2.evaluation.evaluator INFO: Inference done 1186/1276. Dataloading: 0.0016 s/iter. Inference: 0.1300 s/iter. Eval: 0.0065 s/iter. Total: 0.1382 s/iter. ETA=0:00:12
[01/13 19:01:31] detectron2.evaluation.evaluator INFO: Inference done 1223/1276. Dataloading: 0.0016 s/iter. Inference: 0.1300 s/iter. Eval: 0.0065 s/iter. Total: 0.1382 s/iter. ETA=0:00:07
[01/13 19:01:36] detectron2.evaluation.evaluator INFO: Inference done 1260/1276. Dataloading: 0.0016 s/iter. Inference: 0.1300 s/iter. Eval: 0.0065 s/iter. Total: 0.1382 s/iter. ETA=0:00:02
[01/13 19:01:39] detectron2.evaluation.evaluator INFO: Total inference time: 0:02:56.388418 (0.138779 s / iter per device, on 4 devices)
[01/13 19:01:39] detectron2.evaluation.evaluator INFO: Total inference pure compute time: 0:02:45 (0.130025 s / iter per device, on 4 devices)
[01/13 19:20:35] mask_former.data.dataset_mappers.mask_former_semantic_dataset_mapper INFO: [MaskFormerSemanticDatasetMapper] Augmentations used in inference: []
[01/13 19:20:35] detectron2.data.datasets.coco WARNING: Directory /scratch/local/ssd/anjun/datasets/VOCdevkit/VOC2010/JPEGImages and /scratch/local/ssd/anjun/datasets/VOCdevkit/VOC2010/annotations_detectron2/pc59_val has 11321 and 5104 files, respectively.
[01/13 19:20:35] detectron2.data.datasets.coco WARNING: Will use their intersection of 5104 files.
[01/13 19:20:35] detectron2.data.datasets.coco INFO: Loaded 5104 images with semantic segmentation from /scratch/local/ssd/anjun/datasets/VOCdevkit/VOC2010/JPEGImages
[01/13 19:20:35] detectron2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[01/13 19:20:35] detectron2.data.common INFO: Serializing 5104 elements to byte tensors and concatenating them all ...
[01/13 19:20:35] detectron2.data.common INFO: Serialized dataset takes 1.37 MiB
[01/13 19:20:35] detectron2.data.datasets.coco WARNING: Directory /scratch/local/ssd/anjun/datasets/VOCdevkit/VOC2010/JPEGImages and /scratch/local/ssd/anjun/datasets/VOCdevkit/VOC2010/annotations_detectron2/pc59_val has 11321 and 5104 files, respectively.
[01/13 19:20:35] detectron2.data.datasets.coco WARNING: Will use their intersection of 5104 files.
[01/13 19:20:35] detectron2.data.datasets.coco INFO: Loaded 5104 images with semantic segmentation from /scratch/local/ssd/anjun/datasets/VOCdevkit/VOC2010/JPEGImages
[01/13 19:20:35] detectron2.evaluation.evaluator INFO: Start inference on 1276 batches
[01/13 19:20:44] detectron2.evaluation.evaluator INFO: Inference done 11/1276. Dataloading: 0.0009 s/iter. Inference: 0.1239 s/iter. Eval: 0.0054 s/iter. Total: 0.1302 s/iter. ETA=0:02:44
[01/13 19:20:49] detectron2.evaluation.evaluator INFO: Inference done 47/1276. Dataloading: 0.0015 s/iter. Inference: 0.1306 s/iter. Eval: 0.0062 s/iter. Total: 0.1384 s/iter. ETA=0:02:50
[01/13 19:20:54] detectron2.evaluation.evaluator INFO: Inference done 84/1276. Dataloading: 0.0016 s/iter. Inference: 0.1302 s/iter. Eval: 0.0066 s/iter. Total: 0.1384 s/iter. ETA=0:02:44
[01/13 19:20:59] detectron2.evaluation.evaluator INFO: Inference done 121/1276. Dataloading: 0.0015 s/iter. Inference: 0.1298 s/iter. Eval: 0.0065 s/iter. Total: 0.1379 s/iter. ETA=0:02:39
[01/13 19:21:04] detectron2.evaluation.evaluator INFO: Inference done 156/1276. Dataloading: 0.0015 s/iter. Inference: 0.1309 s/iter. Eval: 0.0066 s/iter. Total: 0.1391 s/iter. ETA=0:02:35
[01/13 19:21:09] detectron2.evaluation.evaluator INFO: Inference done 193/1276. Dataloading: 0.0015 s/iter. Inference: 0.1304 s/iter. Eval: 0.0066 s/iter. Total: 0.1386 s/iter. ETA=0:02:30
[01/13 19:21:14] detectron2.evaluation.evaluator INFO: Inference done 229/1276. Dataloading: 0.0016 s/iter. Inference: 0.1308 s/iter. Eval: 0.0067 s/iter. Total: 0.1390 s/iter. ETA=0:02:25
[01/13 19:21:19] detectron2.evaluation.evaluator INFO: Inference done 268/1276. Dataloading: 0.0016 s/iter. Inference: 0.1298 s/iter. Eval: 0.0066 s/iter. Total: 0.1379 s/iter. ETA=0:02:19
[01/13 19:21:24] detectron2.evaluation.evaluator INFO: Inference done 305/1276. Dataloading: 0.0016 s/iter. Inference: 0.1298 s/iter. Eval: 0.0066 s/iter. Total: 0.1380 s/iter. ETA=0:02:13
[01/13 19:21:29] detectron2.evaluation.evaluator INFO: Inference done 341/1276. Dataloading: 0.0016 s/iter. Inference: 0.1302 s/iter. Eval: 0.0066 s/iter. Total: 0.1384 s/iter. ETA=0:02:09
[01/13 19:21:34] detectron2.evaluation.evaluator INFO: Inference done 377/1276. Dataloading: 0.0016 s/iter. Inference: 0.1304 s/iter. Eval: 0.0066 s/iter. Total: 0.1386 s/iter. ETA=0:02:04
[01/13 19:21:40] detectron2.evaluation.evaluator INFO: Inference done 414/1276. Dataloading: 0.0016 s/iter. Inference: 0.1304 s/iter. Eval: 0.0066 s/iter. Total: 0.1386 s/iter. ETA=0:01:59
[01/13 19:21:45] detectron2.evaluation.evaluator INFO: Inference done 451/1276. Dataloading: 0.0016 s/iter. Inference: 0.1304 s/iter. Eval: 0.0066 s/iter. Total: 0.1386 s/iter. ETA=0:01:54
[01/13 19:21:50] detectron2.evaluation.evaluator INFO: Inference done 487/1276. Dataloading: 0.0016 s/iter. Inference: 0.1305 s/iter. Eval: 0.0066 s/iter. Total: 0.1387 s/iter. ETA=0:01:49
[01/13 19:21:55] detectron2.evaluation.evaluator INFO: Inference done 524/1276. Dataloading: 0.0016 s/iter. Inference: 0.1303 s/iter. Eval: 0.0066 s/iter. Total: 0.1385 s/iter. ETA=0:01:44
[01/13 19:22:00] detectron2.evaluation.evaluator INFO: Inference done 561/1276. Dataloading: 0.0016 s/iter. Inference: 0.1303 s/iter. Eval: 0.0066 s/iter. Total: 0.1385 s/iter. ETA=0:01:39
[01/13 19:22:05] detectron2.evaluation.evaluator INFO: Inference done 598/1276. Dataloading: 0.0016 s/iter. Inference: 0.1302 s/iter. Eval: 0.0066 s/iter. Total: 0.1384 s/iter. ETA=0:01:33
[01/13 19:22:10] detectron2.evaluation.evaluator INFO: Inference done 634/1276. Dataloading: 0.0016 s/iter. Inference: 0.1303 s/iter. Eval: 0.0066 s/iter. Total: 0.1385 s/iter. ETA=0:01:28
[01/13 19:22:15] detectron2.evaluation.evaluator INFO: Inference done 670/1276. Dataloading: 0.0016 s/iter. Inference: 0.1304 s/iter. Eval: 0.0066 s/iter. Total: 0.1386 s/iter. ETA=0:01:23
[01/13 19:22:20] detectron2.evaluation.evaluator INFO: Inference done 707/1276. Dataloading: 0.0016 s/iter. Inference: 0.1303 s/iter. Eval: 0.0066 s/iter. Total: 0.1385 s/iter. ETA=0:01:18
[01/13 19:22:25] detectron2.evaluation.evaluator INFO: Inference done 743/1276. Dataloading: 0.0016 s/iter. Inference: 0.1303 s/iter. Eval: 0.0066 s/iter. Total: 0.1385 s/iter. ETA=0:01:13
[01/13 19:22:30] detectron2.evaluation.evaluator INFO: Inference done 780/1276. Dataloading: 0.0016 s/iter. Inference: 0.1303 s/iter. Eval: 0.0066 s/iter. Total: 0.1385 s/iter. ETA=0:01:08
[01/13 19:22:35] detectron2.evaluation.evaluator INFO: Inference done 818/1276. Dataloading: 0.0016 s/iter. Inference: 0.1301 s/iter. Eval: 0.0066 s/iter. Total: 0.1383 s/iter. ETA=0:01:03
[01/13 19:22:40] detectron2.evaluation.evaluator INFO: Inference done 855/1276. Dataloading: 0.0016 s/iter. Inference: 0.1300 s/iter. Eval: 0.0066 s/iter. Total: 0.1382 s/iter. ETA=0:00:58
[01/13 19:22:45] detectron2.evaluation.evaluator INFO: Inference done 891/1276. Dataloading: 0.0016 s/iter. Inference: 0.1301 s/iter. Eval: 0.0066 s/iter. Total: 0.1383 s/iter. ETA=0:00:53
[01/13 19:22:50] detectron2.evaluation.evaluator INFO: Inference done 927/1276. Dataloading: 0.0016 s/iter. Inference: 0.1301 s/iter. Eval: 0.0066 s/iter. Total: 0.1383 s/iter. ETA=0:00:48
[01/13 19:22:55] detectron2.evaluation.evaluator INFO: Inference done 963/1276. Dataloading: 0.0016 s/iter. Inference: 0.1302 s/iter. Eval: 0.0065 s/iter. Total: 0.1384 s/iter. ETA=0:00:43
[01/13 19:23:01] detectron2.evaluation.evaluator INFO: Inference done 1000/1276. Dataloading: 0.0016 s/iter. Inference: 0.1301 s/iter. Eval: 0.0065 s/iter. Total: 0.1383 s/iter. ETA=0:00:38
[01/13 19:23:06] detectron2.evaluation.evaluator INFO: Inference done 1037/1276. Dataloading: 0.0016 s/iter. Inference: 0.1302 s/iter. Eval: 0.0065 s/iter. Total: 0.1383 s/iter. ETA=0:00:33
[01/13 19:23:11] detectron2.evaluation.evaluator INFO: Inference done 1074/1276. Dataloading: 0.0016 s/iter. Inference: 0.1301 s/iter. Eval: 0.0065 s/iter. Total: 0.1383 s/iter. ETA=0:00:27
[01/13 19:23:16] detectron2.evaluation.evaluator INFO: Inference done 1112/1276. Dataloading: 0.0016 s/iter. Inference: 0.1300 s/iter. Eval: 0.0065 s/iter. Total: 0.1382 s/iter. ETA=0:00:22
[01/13 19:23:21] detectron2.evaluation.evaluator INFO: Inference done 1148/1276. Dataloading: 0.0016 s/iter. Inference: 0.1300 s/iter. Eval: 0.0065 s/iter. Total: 0.1382 s/iter. ETA=0:00:17
[01/13 19:23:26] detectron2.evaluation.evaluator INFO: Inference done 1185/1276. Dataloading: 0.0016 s/iter. Inference: 0.1300 s/iter. Eval: 0.0065 s/iter. Total: 0.1382 s/iter. ETA=0:00:12
[01/13 19:23:31] detectron2.evaluation.evaluator INFO: Inference done 1221/1276. Dataloading: 0.0016 s/iter. Inference: 0.1301 s/iter. Eval: 0.0065 s/iter. Total: 0.1382 s/iter. ETA=0:00:07
[01/13 19:23:36] detectron2.evaluation.evaluator INFO: Inference done 1258/1276. Dataloading: 0.0016 s/iter. Inference: 0.1301 s/iter. Eval: 0.0065 s/iter. Total: 0.1382 s/iter. ETA=0:00:02
[01/13 19:23:39] detectron2.evaluation.evaluator INFO: Total inference time: 0:02:56.611442 (0.138955 s / iter per device, on 4 devices)
[01/13 19:23:39] detectron2.evaluation.evaluator INFO: Total inference pure compute time: 0:02:45 (0.130072 s / iter per device, on 4 devices)
[01/13 19:42:34] mask_former.data.dataset_mappers.mask_former_semantic_dataset_mapper INFO: [MaskFormerSemanticDatasetMapper] Augmentations used in inference: []
[01/13 19:42:34] detectron2.data.datasets.coco WARNING: Directory /scratch/local/ssd/anjun/datasets/VOCdevkit/VOC2010/JPEGImages and /scratch/local/ssd/anjun/datasets/VOCdevkit/VOC2010/annotations_detectron2/pc59_val has 11321 and 5104 files, respectively.
[01/13 19:42:34] detectron2.data.datasets.coco WARNING: Will use their intersection of 5104 files.
[01/13 19:42:34] detectron2.data.datasets.coco INFO: Loaded 5104 images with semantic segmentation from /scratch/local/ssd/anjun/datasets/VOCdevkit/VOC2010/JPEGImages
[01/13 19:42:34] detectron2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[01/13 19:42:34] detectron2.data.common INFO: Serializing 5104 elements to byte tensors and concatenating them all ...
[01/13 19:42:34] detectron2.data.common INFO: Serialized dataset takes 1.37 MiB
[01/13 19:42:34] detectron2.data.datasets.coco WARNING: Directory /scratch/local/ssd/anjun/datasets/VOCdevkit/VOC2010/JPEGImages and /scratch/local/ssd/anjun/datasets/VOCdevkit/VOC2010/annotations_detectron2/pc59_val has 11321 and 5104 files, respectively.
[01/13 19:42:34] detectron2.data.datasets.coco WARNING: Will use their intersection of 5104 files.
[01/13 19:42:34] detectron2.data.datasets.coco INFO: Loaded 5104 images with semantic segmentation from /scratch/local/ssd/anjun/datasets/VOCdevkit/VOC2010/JPEGImages
[01/13 19:42:34] detectron2.evaluation.evaluator INFO: Start inference on 1276 batches
[01/13 19:42:42] detectron2.evaluation.evaluator INFO: Inference done 11/1276. Dataloading: 0.0006 s/iter. Inference: 0.1276 s/iter. Eval: 0.0061 s/iter. Total: 0.1343 s/iter. ETA=0:02:49
[01/13 19:42:47] detectron2.evaluation.evaluator INFO: Inference done 47/1276. Dataloading: 0.0016 s/iter. Inference: 0.1329 s/iter. Eval: 0.0067 s/iter. Total: 0.1412 s/iter. ETA=0:02:53
[01/13 19:42:52] detectron2.evaluation.evaluator INFO: Inference done 83/1276. Dataloading: 0.0016 s/iter. Inference: 0.1333 s/iter. Eval: 0.0067 s/iter. Total: 0.1417 s/iter. ETA=0:02:49
[01/13 19:42:57] detectron2.evaluation.evaluator INFO: Inference done 120/1276. Dataloading: 0.0016 s/iter. Inference: 0.1325 s/iter. Eval: 0.0066 s/iter. Total: 0.1408 s/iter. ETA=0:02:42
[01/13 19:43:02] detectron2.evaluation.evaluator INFO: Inference done 156/1276. Dataloading: 0.0016 s/iter. Inference: 0.1328 s/iter. Eval: 0.0067 s/iter. Total: 0.1411 s/iter. ETA=0:02:38
[01/13 19:43:07] detectron2.evaluation.evaluator INFO: Inference done 193/1276. Dataloading: 0.0016 s/iter. Inference: 0.1318 s/iter. Eval: 0.0066 s/iter. Total: 0.1400 s/iter. ETA=0:02:31
[01/13 19:43:12] detectron2.evaluation.evaluator INFO: Inference done 229/1276. Dataloading: 0.0016 s/iter. Inference: 0.1318 s/iter. Eval: 0.0067 s/iter. Total: 0.1401 s/iter. ETA=0:02:26
[01/13 19:43:17] detectron2.evaluation.evaluator INFO: Inference done 267/1276. Dataloading: 0.0016 s/iter. Inference: 0.1307 s/iter. Eval: 0.0066 s/iter. Total: 0.1389 s/iter. ETA=0:02:20
[01/13 19:43:22] detectron2.evaluation.evaluator INFO: Inference done 304/1276. Dataloading: 0.0016 s/iter. Inference: 0.1306 s/iter. Eval: 0.0066 s/iter. Total: 0.1388 s/iter. ETA=0:02:14
[01/13 19:43:28] detectron2.evaluation.evaluator INFO: Inference done 340/1276. Dataloading: 0.0016 s/iter. Inference: 0.1309 s/iter. Eval: 0.0066 s/iter. Total: 0.1392 s/iter. ETA=0:02:10
[01/13 19:43:33] detectron2.evaluation.evaluator INFO: Inference done 376/1276. Dataloading: 0.0016 s/iter. Inference: 0.1311 s/iter. Eval: 0.0067 s/iter. Total: 0.1394 s/iter. ETA=0:02:05
[01/13 19:43:38] detectron2.evaluation.evaluator INFO: Inference done 412/1276. Dataloading: 0.0016 s/iter. Inference: 0.1311 s/iter. Eval: 0.0067 s/iter. Total: 0.1394 s/iter. ETA=0:02:00
[01/13 19:43:43] detectron2.evaluation.evaluator INFO: Inference done 448/1276. Dataloading: 0.0016 s/iter. Inference: 0.1311 s/iter. Eval: 0.0067 s/iter. Total: 0.1394 s/iter. ETA=0:01:55
[01/13 19:43:48] detectron2.evaluation.evaluator INFO: Inference done 484/1276. Dataloading: 0.0016 s/iter. Inference: 0.1311 s/iter. Eval: 0.0067 s/iter. Total: 0.1394 s/iter. ETA=0:01:50
[01/13 19:43:53] detectron2.evaluation.evaluator INFO: Inference done 521/1276. Dataloading: 0.0016 s/iter. Inference: 0.1308 s/iter. Eval: 0.0067 s/iter. Total: 0.1392 s/iter. ETA=0:01:45
[01/13 19:43:58] detectron2.evaluation.evaluator INFO: Inference done 557/1276. Dataloading: 0.0016 s/iter. Inference: 0.1308 s/iter. Eval: 0.0067 s/iter. Total: 0.1392 s/iter. ETA=0:01:40
[01/13 19:44:03] detectron2.evaluation.evaluator INFO: Inference done 594/1276. Dataloading: 0.0016 s/iter. Inference: 0.1307 s/iter. Eval: 0.0067 s/iter. Total: 0.1391 s/iter. ETA=0:01:34
[01/13 19:44:08] detectron2.evaluation.evaluator INFO: Inference done 630/1276. Dataloading: 0.0016 s/iter. Inference: 0.1307 s/iter. Eval: 0.0067 s/iter. Total: 0.1391 s/iter. ETA=0:01:29
[01/13 19:44:13] detectron2.evaluation.evaluator INFO: Inference done 667/1276. Dataloading: 0.0016 s/iter. Inference: 0.1307 s/iter. Eval: 0.0067 s/iter. Total: 0.1391 s/iter. ETA=0:01:24
[01/13 19:44:18] detectron2.evaluation.evaluator INFO: Inference done 704/1276. Dataloading: 0.0017 s/iter. Inference: 0.1307 s/iter. Eval: 0.0067 s/iter. Total: 0.1391 s/iter. ETA=0:01:19
[01/13 19:44:23] detectron2.evaluation.evaluator INFO: Inference done 740/1276. Dataloading: 0.0017 s/iter. Inference: 0.1309 s/iter. Eval: 0.0067 s/iter. Total: 0.1392 s/iter. ETA=0:01:14
[01/13 19:44:28] detectron2.evaluation.evaluator INFO: Inference done 777/1276. Dataloading: 0.0017 s/iter. Inference: 0.1308 s/iter. Eval: 0.0067 s/iter. Total: 0.1392 s/iter. ETA=0:01:09
[01/13 19:44:33] detectron2.evaluation.evaluator INFO: Inference done 814/1276. Dataloading: 0.0017 s/iter. Inference: 0.1307 s/iter. Eval: 0.0067 s/iter. Total: 0.1391 s/iter. ETA=0:01:04
[01/13 19:44:38] detectron2.evaluation.evaluator INFO: Inference done 847/1276. Dataloading: 0.0019 s/iter. Inference: 0.1310 s/iter. Eval: 0.0067 s/iter. Total: 0.1396 s/iter. ETA=0:00:59
[01/13 19:44:44] detectron2.evaluation.evaluator INFO: Inference done 878/1276. Dataloading: 0.0020 s/iter. Inference: 0.1317 s/iter. Eval: 0.0068 s/iter. Total: 0.1405 s/iter. ETA=0:00:55
[01/13 19:44:49] detectron2.evaluation.evaluator INFO: Inference done 912/1276. Dataloading: 0.0020 s/iter. Inference: 0.1320 s/iter. Eval: 0.0068 s/iter. Total: 0.1409 s/iter. ETA=0:00:51
[01/13 19:44:54] detectron2.evaluation.evaluator INFO: Inference done 933/1276. Dataloading: 0.0020 s/iter. Inference: 0.1343 s/iter. Eval: 0.0068 s/iter. Total: 0.1432 s/iter. ETA=0:00:49
[01/13 19:44:59] detectron2.evaluation.evaluator INFO: Inference done 958/1276. Dataloading: 0.0020 s/iter. Inference: 0.1359 s/iter. Eval: 0.0068 s/iter. Total: 0.1448 s/iter. ETA=0:00:46
[01/13 19:45:04] detectron2.evaluation.evaluator INFO: Inference done 994/1276. Dataloading: 0.0020 s/iter. Inference: 0.1357 s/iter. Eval: 0.0068 s/iter. Total: 0.1447 s/iter. ETA=0:00:40
[01/13 19:45:09] detectron2.evaluation.evaluator INFO: Inference done 1030/1276. Dataloading: 0.0020 s/iter. Inference: 0.1356 s/iter. Eval: 0.0068 s/iter. Total: 0.1445 s/iter. ETA=0:00:35
[01/13 19:45:14] detectron2.evaluation.evaluator INFO: Inference done 1067/1276. Dataloading: 0.0020 s/iter. Inference: 0.1354 s/iter. Eval: 0.0068 s/iter. Total: 0.1443 s/iter. ETA=0:00:30
[01/13 19:45:19] detectron2.evaluation.evaluator INFO: Inference done 1104/1276. Dataloading: 0.0020 s/iter. Inference: 0.1351 s/iter. Eval: 0.0068 s/iter. Total: 0.1440 s/iter. ETA=0:00:24
[01/13 19:45:24] detectron2.evaluation.evaluator INFO: Inference done 1140/1276. Dataloading: 0.0020 s/iter. Inference: 0.1350 s/iter. Eval: 0.0068 s/iter. Total: 0.1439 s/iter. ETA=0:00:19
[01/13 19:45:29] detectron2.evaluation.evaluator INFO: Inference done 1177/1276. Dataloading: 0.0020 s/iter. Inference: 0.1348 s/iter. Eval: 0.0068 s/iter. Total: 0.1437 s/iter. ETA=0:00:14
[01/13 19:45:34] detectron2.evaluation.evaluator INFO: Inference done 1213/1276. Dataloading: 0.0020 s/iter. Inference: 0.1347 s/iter. Eval: 0.0068 s/iter. Total: 0.1435 s/iter. ETA=0:00:09
[01/13 19:45:39] detectron2.evaluation.evaluator INFO: Inference done 1250/1276. Dataloading: 0.0020 s/iter. Inference: 0.1346 s/iter. Eval: 0.0068 s/iter. Total: 0.1434 s/iter. ETA=0:00:03
[01/13 19:45:44] detectron2.evaluation.evaluator INFO: Total inference time: 0:03:02.935474 (0.143930 s / iter per device, on 4 devices)
[01/13 19:45:44] detectron2.evaluation.evaluator INFO: Total inference pure compute time: 0:02:50 (0.134526 s / iter per device, on 4 devices)
[01/13 19:55:46] detectron2.engine.hooks INFO: Overall training speed: 8522 iterations in 2:40:49 (1.1324 s / it)
[01/13 19:55:46] detectron2.engine.hooks INFO: Total training time: 3:05:58 (0:25:08 on hooks)
