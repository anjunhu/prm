[01/13 16:23:07] detectron2 INFO: Rank of current process: 1. World size: 4
[01/13 16:23:13] detectron2 INFO: Environment info:
-------------------------------  --------------------------------------------------------------------------------------------------
sys.platform                     linux
Python                           3.10.11 (main, May 16 2023, 00:28:57) [GCC 11.2.0]
numpy                            1.26.2
detectron2                       0.6 @/homes/55/anjun/.local/lib/python3.10/site-packages/detectron2
Compiler                         GCC 8.3
CUDA compiler                    CUDA 12.1
detectron2 arch flags            8.6
DETECTRON2_ENV_MODULE            <not set>
PyTorch                          2.1.1+cu121 @/scratch/local/ssd/anjun/anaconda3/envs/ldm/lib/python3.10/site-packages/torch
PyTorch debug build              False
torch._C._GLIBCXX_USE_CXX11_ABI  False
GPU available                    Yes
GPU 0,1,2,3                      NVIDIA A40 (arch=8.6)
Driver version                   530.30.02
CUDA_HOME                        /usr/local/cuda-12.1/
Pillow                           9.5.0
torchvision                      0.16.1+cu121 @/scratch/local/ssd/anjun/anaconda3/envs/ldm/lib/python3.10/site-packages/torchvision
torchvision arch flags           5.0, 6.0, 7.0, 7.5, 8.0, 8.6, 9.0
fvcore                           0.1.5.post20221221
iopath                           0.1.9
cv2                              4.5.4-dev
-------------------------------  --------------------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v3.1.1 (Git Hash 64f6bcbcbab628e96f33a62c3e975f8535a7bde4)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 12.1
  - NVCC architecture flags: -gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.9.2
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=12.1, CUDNN_VERSION=8.9.2, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=old-style-cast -Wno-invalid-partial-specialization -Wno-unused-private-field -Wno-aligned-allocation-unavailable -Wno-missing-braces -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.1.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[01/13 16:23:13] detectron2 INFO: Command line arguments: Namespace(config_file='configs/coco-stuff-164k-156/zero_shot_maskformer_R101c_bs32_60k_proposalmask_featupsample_img512.yaml', resume=True, eval_only=False, num_gpus=4, num_machines=1, machine_rank=0, dist_url='tcp://0.0.0.0:12237', opts=['MODEL.CLIP_ADAPTER.CLIP_ENSEMBLE', 'False', 'MODEL.CLIP_ADAPTER.CLIP_ENSEMBLE_WEIGHT', '-1.0', 'SOLVER.TEST_IMS_PER_BATCH', '1', 'OUTPUT_DIR', 'deop/train_log', 'MODEL.CLIP_ADAPTER.PROMPT_LEARNER', 'learnable', 'SOLVER.IMS_PER_BATCH', '16', 'SOLVER.BASE_LR', '0.00005', 'SOLVER.CHECKPOINT_PERIOD', '1000', 'MODEL.CLIP_ADAPTER.CLIP_MODEL_NAME', 'ViT-B/16', 'MODEL.WEIGHTS', 'pretrained_models/deop_model_final.pth', 'MODEL.META_ARCHITECTURE', 'ZeroShotClipfeatUpsample_addnorm_vit16Query2D_maskvit', 'MODEL.NUM_DECODER_LAYER', '1', 'ORACLE', 'False', 'INPUT.MIN_SIZE_TEST', '512', 'TEST.EVAL_PERIOD', '1000', 'INPUT.SIZE_DIVISIBILITY', '512', 'MODEL.MASK_FORMER.DECODER_DICE_WEIGHT', '0.8', 'MODEL.MASK_FORMER.DECODER_CE_WEIGHT', '2.0', 'MODEL.CLIP_ADAPTER.LEARN_POSITION', 'True', 'MODEL.CLIP_ADAPTER.POSITION_LAYERS', '[1,2,3,4,5,6,7,8,9,10,11]', 'MODEL.CLIP_ADAPTER.LAYERMASKVIT', '[11]', 'MODEL.CLIP_ADAPTER.END2ENDTRAIN', 'False', 'MODEL.CLIP_ADAPTER.PS_SHORTCUT', '1.0', 'DATASETS.TRAIN', "('coco_2017_train_stuff_all_sem_seg',)", 'DATASETS.TEST', "('context_59_test_sem_seg',)", 'MODEL.SEM_SEG_HEAD.NUM_CLASSES', '171', 'MODEL.MASK_FORMER.LOSS_NUM_CLASS', '171'])
[01/13 16:23:13] detectron2 INFO: Contents of args.config_file=configs/coco-stuff-164k-156/zero_shot_maskformer_R101c_bs32_60k_proposalmask_featupsample_img512.yaml:
[38;5;197m_BASE_[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mzero_shot_maskformer_R50_bs32_60k.yaml[39m
[38;5;197mMODEL[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mSEM_SEG_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mNUM_CLASSES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m171[39m
[38;5;15m  [39m[38;5;197mMETA_ARCHITECTURE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mZeroShotMaskFormerClipfeatUpsample[39m[38;5;186m"[39m
[38;5;15m  [39m[38;5;197mBACKBONE[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mbuild_resnet_deeplab_backbone[39m[38;5;186m"[39m
[38;5;15m  [39m
[38;5;15m  [39m[38;5;197mWEIGHTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mdetectron2://DeepLab/R-103.pkl[39m[38;5;186m"[39m
[38;5;15m  [39m[38;5;197mRESNETS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mDEPTH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m101[39m
[38;5;15m    [39m[38;5;197mSTEM_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mdeeplab[39m[38;5;186m"[39m
[38;5;15m    [39m[38;5;197mSTEM_OUT_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m128[39m
[38;5;15m    [39m[38;5;197mSTRIDE_IN_1X1[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mFalse[39m
[38;5;15m    [39m[38;5;197mOUT_FEATURES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;186m"[39m[38;5;186mres2[39m[38;5;186m"[39m[38;5;15m,[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mres3[39m[38;5;186m"[39m[38;5;15m,[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mres4[39m[38;5;186m"[39m[38;5;15m,[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mres5[39m[38;5;186m"[39m[38;5;15m][39m
[38;5;15m    [39m[38;5;242m# NORM: "SyncBN"[39m
[38;5;15m    [39m[38;5;197mRES5_MULTI_GRID[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m1[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15m2[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15m4[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;197mCLIP_ADAPTER[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCLIP_ENSEMBLE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mTrue[39m
[38;5;15m    [39m[38;5;197mCLIP_ENSEMBLE_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.8[39m
[38;5;197mINPUT[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mMIN_SIZE_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;81m!!python/object/apply:eval[39m[38;5;15m [39m[38;5;15m[[39m[38;5;186m"[39m[38;5;186m[int(x[39m[38;5;15m [39m[38;5;186m*[39m[38;5;15m [39m[38;5;186m0.1[39m[38;5;15m [39m[38;5;186m*[39m[38;5;15m [39m[38;5;186m512)[39m[38;5;15m [39m[38;5;186mfor[39m[38;5;15m [39m[38;5;186mx[39m[38;5;15m [39m[38;5;186min[39m[38;5;15m [39m[38;5;186mrange(5,[39m[38;5;15m [39m[38;5;186m16)][39m[38;5;186m"[39m[38;5;15m][39m
[38;5;197mOUTPUT_DIR[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mdeop/train_log[39m[38;5;186m"[39m
[38;5;197mSOLVER[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mIMS_PER_BATCH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m8[39m
[38;5;15m  [39m[38;5;197mTEST_IMS_PER_BATCH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m8[39m
[38;5;197mTEST[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mEVAL_PERIOD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m5000[39m

[01/13 16:23:13] detectron2.utils.env INFO: Using a generated random seed 13456481
[01/13 16:23:14] mask_former.modeling.clip_adapter INFO: Prompt Learner training params: ['prefix_prompt']
[01/13 16:23:18] detectron2.engine.defaults INFO: Model:
ZeroShotClipfeatUpsample_addnorm_vit16Query2D_maskvit(
  (backbone): ResNet(
    (stem): DeepLabStem(
      (conv1): Conv2d(
        3, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
      )
      (conv3): Conv2d(
        64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
      )
    )
    (res2): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv1): Conv2d(
          128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
      )
    )
    (res3): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv1): Conv2d(
          256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
      (3): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
    )
    (res4): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
        (conv1): Conv2d(
          512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (3): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (4): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (5): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (6): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (7): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (8): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (9): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (10): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (11): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (12): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (13): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (14): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (15): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (16): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (17): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (18): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (19): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (20): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (21): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (22): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
    )
    (res5): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
        (conv1): Conv2d(
          1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
      )
    )
  )
  (sem_seg_head): ZeroShotMaskFormerHead(
    (pixel_decoder): BasePixelDecoder(
      (adapter_1): Conv2d(
        256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (layer_1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (adapter_2): Conv2d(
        512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (layer_2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (adapter_3): Conv2d(
        1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (layer_3): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (layer_4): Conv2d(
        2048, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (mask_features): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
    (predictor): ZeroShotTransformerPredictor(
      (pe_layer): PositionEmbeddingSine()
      (transformer): Transformer(
        (encoder): TransformerEncoder(
          (layers): ModuleList()
        )
        (decoder): TransformerDecoder(
          (layers): ModuleList(
            (0-5): 6 x TransformerDecoderLayer(
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (multihead_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (linear1): Linear(in_features=256, out_features=2048, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (linear2): Linear(in_features=2048, out_features=256, bias=True)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (dropout1): Dropout(p=0.1, inplace=False)
              (dropout2): Dropout(p=0.1, inplace=False)
              (dropout3): Dropout(p=0.1, inplace=False)
            )
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (query_embed): Embedding(100, 256)
      (input_proj): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
      (mask_embed): MLP(
        (layers): ModuleList(
          (0-2): 3 x Linear(in_features=256, out_features=256, bias=True)
        )
      )
      (class_embed): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=1024, bias=True)
          (1): Linear(in_features=1024, out_features=512, bias=True)
        )
      )
    )
  )
  (criterion): SetCriterion(
    (matcher): Matcher HungarianMatcher
        cost_class: 1
        cost_mask: 20.0
        cost_dice: 1.0
  )
  (clip_adapter): MaskFormerClipFeatureAdapter(
    (clip_model): CLIP(
      (visual): VisionTransformerLearnPosition(
        (conv1): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False)
        (ln_pre): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (transformer): TransformerVitPosition(
          (resblocks): Sequential(
            (0): ResidualAttentionBlockVitPosition(
              (attn): MultiheadAttentionLoRA(
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (1): ResidualAttentionBlockVitPosition(
              (attn): MultiheadAttentionLoRA(
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (2): ResidualAttentionBlockVitPosition(
              (attn): MultiheadAttentionLoRA(
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (3): ResidualAttentionBlockVitPosition(
              (attn): MultiheadAttentionLoRA(
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (4): ResidualAttentionBlockVitPosition(
              (attn): MultiheadAttentionLoRA(
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (5): ResidualAttentionBlockVitPosition(
              (attn): MultiheadAttentionLoRA(
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (6): ResidualAttentionBlockVitPosition(
              (attn): MultiheadAttentionLoRA(
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (7): ResidualAttentionBlockVitPosition(
              (attn): MultiheadAttentionLoRA(
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (8): ResidualAttentionBlockVitPosition(
              (attn): MultiheadAttentionLoRA(
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (9): ResidualAttentionBlockVitPosition(
              (attn): MultiheadAttentionLoRA(
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (10): ResidualAttentionBlockVitPosition(
              (attn): MultiheadAttentionLoRA(
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (11): ResidualAttentionBlockVitPosition(
              (attn): MultiheadAttentionVit(
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
        (ln_post): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (transformer): Transformer(
        (resblocks): Sequential(
          (0): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (1): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (2): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (3): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (4): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (5): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (6): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (7): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (8): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (9): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (10): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (11): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (token_embedding): Embedding(49408, 512)
      (ln_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (prompt_learner): LearnablePromptExtractor(
      prefix_prompt:16,suffix_prompt:0,dimension:512
      [Normal_Init(mu=0,std=0.02)]
    )
  )
  (decoder): TransformerDecoderLinear(
    (layers): ModuleList(
      (0): TransformerDecoderLayerLinear(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
        )
        (multihead_attn_my): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
        )
        (linear1): Linear(in_features=512, out_features=1024, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=1024, out_features=512, bias=True)
        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
    )
    (outlayer): TransformerDecoderOutLayerLinear(
      (self_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
      )
      (multihead_attn_my): MultiHeadAttentionMy(
        (dropout_F): Dropout(p=0.1, inplace=False)
      )
      (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (dropout1): Dropout(p=0.1, inplace=False)
      (dropout2): Dropout(p=0.1, inplace=False)
      (dropout3): Dropout(p=0.1, inplace=False)
    )
    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  )
  (pe_layer): PositionEmbeddingSine()
  (query_embedding): Embedding(100, 512)
)
[01/13 16:23:18] mask_former.data.dataset_mappers.mask_former_semantic_dataset_mapper INFO: [MaskFormerSemanticDatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=(256, 307, 358, 409, 460, 512, 563, 614, 665, 716, 768), max_size=2560, sample_style='choice'), RandomCrop_CategoryAreaConstraint(crop_type='absolute', crop_size=[640, 640], single_category_max_area=1.0, ignored_category=255), <detectron2.projects.point_rend.color_augmentation.ColorAugSSDTransform object at 0x7fe7446dafe0>, RandomFlip()]
[01/13 16:23:20] detectron2.data.datasets.coco INFO: Loaded 118287 images with semantic segmentation from /scratch/local/ssd/anjun/datasets/coco-stuff/images/train2017
[01/13 16:23:20] mask_former.data.build INFO: Using training sampler TrainingSampler
[01/13 16:23:21] detectron2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[01/13 16:23:21] detectron2.data.common INFO: Serializing 118287 elements to byte tensors and concatenating them all ...
[01/13 16:23:21] detectron2.data.common INFO: Serialized dataset takes 32.38 MiB
[01/13 16:23:21] detectron2.data.build INFO: Making batched data loader with batch_size=4
[01/13 16:23:22] detectron2.checkpoint.detection_checkpoint INFO: [DetectionCheckpointer] Loading from pretrained_models/deop_model_final.pth ...
[01/13 16:23:22] fvcore.common.checkpoint INFO: [Checkpointer] Loading from pretrained_models/deop_model_final.pth ...
[01/13 16:23:22] fvcore.common.checkpoint WARNING: Skip loading parameter 'criterion.empty_weight' to the model due to incompatible shapes: (157,) in the checkpoint but (172,) in the model! You might want to double check if this is expected.
[01/13 16:23:22] fvcore.common.checkpoint WARNING: Some model parameters or buffers are not found in the checkpoint:
[34mclip_adapter.clip_model.token_embedding.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.10.learned_position[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.11.attn.out_proj.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.11.learned_position[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.6.learned_position[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.7.learned_position[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.8.learned_position[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.9.learned_position[0m
[34mcriterion.empty_weight[0m
[01/13 16:23:22] detectron2.engine.train_loop INFO: Starting training from iteration 0
[01/13 16:23:40] detectron2.engine.hooks INFO: Total training time: 0:00:00 (0:00:00 on hooks)
[01/13 16:28:36] detectron2 INFO: Rank of current process: 1. World size: 4
[01/13 16:28:42] detectron2 INFO: Environment info:
-------------------------------  --------------------------------------------------------------------------------------------------
sys.platform                     linux
Python                           3.10.11 (main, May 16 2023, 00:28:57) [GCC 11.2.0]
numpy                            1.26.2
detectron2                       0.6 @/homes/55/anjun/.local/lib/python3.10/site-packages/detectron2
Compiler                         GCC 8.3
CUDA compiler                    CUDA 12.1
detectron2 arch flags            8.6
DETECTRON2_ENV_MODULE            <not set>
PyTorch                          2.1.1+cu121 @/scratch/local/ssd/anjun/anaconda3/envs/ldm/lib/python3.10/site-packages/torch
PyTorch debug build              False
torch._C._GLIBCXX_USE_CXX11_ABI  False
GPU available                    Yes
GPU 0,1,2,3                      NVIDIA A40 (arch=8.6)
Driver version                   530.30.02
CUDA_HOME                        /usr/local/cuda-12.1/
Pillow                           9.5.0
torchvision                      0.16.1+cu121 @/scratch/local/ssd/anjun/anaconda3/envs/ldm/lib/python3.10/site-packages/torchvision
torchvision arch flags           5.0, 6.0, 7.0, 7.5, 8.0, 8.6, 9.0
fvcore                           0.1.5.post20221221
iopath                           0.1.9
cv2                              4.5.4-dev
-------------------------------  --------------------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v3.1.1 (Git Hash 64f6bcbcbab628e96f33a62c3e975f8535a7bde4)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 12.1
  - NVCC architecture flags: -gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.9.2
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=12.1, CUDNN_VERSION=8.9.2, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=old-style-cast -Wno-invalid-partial-specialization -Wno-unused-private-field -Wno-aligned-allocation-unavailable -Wno-missing-braces -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.1.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[01/13 16:28:42] detectron2 INFO: Command line arguments: Namespace(config_file='configs/coco-stuff-164k-156/zero_shot_maskformer_R101c_bs32_60k_proposalmask_featupsample_img512.yaml', resume=True, eval_only=False, num_gpus=4, num_machines=1, machine_rank=0, dist_url='tcp://0.0.0.0:12237', opts=['MODEL.CLIP_ADAPTER.CLIP_ENSEMBLE', 'False', 'MODEL.CLIP_ADAPTER.CLIP_ENSEMBLE_WEIGHT', '-1.0', 'SOLVER.TEST_IMS_PER_BATCH', '1', 'OUTPUT_DIR', 'deop/train_log', 'MODEL.CLIP_ADAPTER.PROMPT_LEARNER', 'learnable', 'SOLVER.IMS_PER_BATCH', '16', 'SOLVER.BASE_LR', '0.00005', 'SOLVER.CHECKPOINT_PERIOD', '1000', 'MODEL.CLIP_ADAPTER.CLIP_MODEL_NAME', 'ViT-B/16', 'MODEL.WEIGHTS', 'pretrained_models/deop_model_final.pth', 'MODEL.META_ARCHITECTURE', 'ZeroShotClipfeatUpsample_addnorm_vit16Query2D_maskvit', 'MODEL.NUM_DECODER_LAYER', '1', 'ORACLE', 'False', 'INPUT.MIN_SIZE_TEST', '512', 'TEST.EVAL_PERIOD', '1000', 'INPUT.SIZE_DIVISIBILITY', '512', 'MODEL.MASK_FORMER.DECODER_DICE_WEIGHT', '0.8', 'MODEL.MASK_FORMER.DECODER_CE_WEIGHT', '2.0', 'MODEL.CLIP_ADAPTER.LEARN_POSITION', 'True', 'MODEL.CLIP_ADAPTER.POSITION_LAYERS', '[1,2,3,4,5,6,7,8,9,10,11]', 'MODEL.CLIP_ADAPTER.LAYERMASKVIT', '[11]', 'MODEL.CLIP_ADAPTER.END2ENDTRAIN', 'False', 'MODEL.CLIP_ADAPTER.PS_SHORTCUT', '1.0', 'DATASETS.TRAIN', "('coco_2017_train_stuff_all_sem_seg',)", 'DATASETS.TEST', "('context_59_test_sem_seg',)", 'MODEL.SEM_SEG_HEAD.NUM_CLASSES', '171', 'MODEL.MASK_FORMER.LOSS_NUM_CLASS', '171'])
[01/13 16:28:42] detectron2 INFO: Contents of args.config_file=configs/coco-stuff-164k-156/zero_shot_maskformer_R101c_bs32_60k_proposalmask_featupsample_img512.yaml:
[38;5;197m_BASE_[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mzero_shot_maskformer_R50_bs32_60k.yaml[39m
[38;5;197mMODEL[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mSEM_SEG_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mNUM_CLASSES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m171[39m
[38;5;15m  [39m[38;5;197mMETA_ARCHITECTURE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mZeroShotMaskFormerClipfeatUpsample[39m[38;5;186m"[39m
[38;5;15m  [39m[38;5;197mBACKBONE[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mbuild_resnet_deeplab_backbone[39m[38;5;186m"[39m
[38;5;15m  [39m
[38;5;15m  [39m[38;5;197mWEIGHTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mdetectron2://DeepLab/R-103.pkl[39m[38;5;186m"[39m
[38;5;15m  [39m[38;5;197mRESNETS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mDEPTH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m101[39m
[38;5;15m    [39m[38;5;197mSTEM_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mdeeplab[39m[38;5;186m"[39m
[38;5;15m    [39m[38;5;197mSTEM_OUT_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m128[39m
[38;5;15m    [39m[38;5;197mSTRIDE_IN_1X1[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mFalse[39m
[38;5;15m    [39m[38;5;197mOUT_FEATURES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;186m"[39m[38;5;186mres2[39m[38;5;186m"[39m[38;5;15m,[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mres3[39m[38;5;186m"[39m[38;5;15m,[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mres4[39m[38;5;186m"[39m[38;5;15m,[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mres5[39m[38;5;186m"[39m[38;5;15m][39m
[38;5;15m    [39m[38;5;242m# NORM: "SyncBN"[39m
[38;5;15m    [39m[38;5;197mRES5_MULTI_GRID[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m1[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15m2[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15m4[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;197mCLIP_ADAPTER[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCLIP_ENSEMBLE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mTrue[39m
[38;5;15m    [39m[38;5;197mCLIP_ENSEMBLE_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.8[39m
[38;5;197mINPUT[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mMIN_SIZE_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;81m!!python/object/apply:eval[39m[38;5;15m [39m[38;5;15m[[39m[38;5;186m"[39m[38;5;186m[int(x[39m[38;5;15m [39m[38;5;186m*[39m[38;5;15m [39m[38;5;186m0.1[39m[38;5;15m [39m[38;5;186m*[39m[38;5;15m [39m[38;5;186m512)[39m[38;5;15m [39m[38;5;186mfor[39m[38;5;15m [39m[38;5;186mx[39m[38;5;15m [39m[38;5;186min[39m[38;5;15m [39m[38;5;186mrange(5,[39m[38;5;15m [39m[38;5;186m16)][39m[38;5;186m"[39m[38;5;15m][39m
[38;5;197mOUTPUT_DIR[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mdeop/train_log[39m[38;5;186m"[39m
[38;5;197mSOLVER[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mIMS_PER_BATCH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m8[39m
[38;5;15m  [39m[38;5;197mTEST_IMS_PER_BATCH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m8[39m
[38;5;197mTEST[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mEVAL_PERIOD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m5000[39m

[01/13 16:28:42] detectron2.utils.env INFO: Using a generated random seed 42287341
[01/13 16:28:42] mask_former.modeling.clip_adapter INFO: Prompt Learner training params: ['prefix_prompt']
[01/13 16:28:47] detectron2.engine.defaults INFO: Model:
ZeroShotClipfeatUpsample_addnorm_vit16Query2D_maskvit(
  (backbone): ResNet(
    (stem): DeepLabStem(
      (conv1): Conv2d(
        3, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
      )
      (conv3): Conv2d(
        64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
      )
    )
    (res2): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv1): Conv2d(
          128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
      )
    )
    (res3): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv1): Conv2d(
          256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
      (3): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
    )
    (res4): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
        (conv1): Conv2d(
          512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (3): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (4): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (5): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (6): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (7): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (8): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (9): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (10): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (11): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (12): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (13): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (14): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (15): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (16): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (17): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (18): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (19): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (20): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (21): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (22): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
    )
    (res5): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
        (conv1): Conv2d(
          1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
      )
    )
  )
  (sem_seg_head): ZeroShotMaskFormerHead(
    (pixel_decoder): BasePixelDecoder(
      (adapter_1): Conv2d(
        256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (layer_1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (adapter_2): Conv2d(
        512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (layer_2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (adapter_3): Conv2d(
        1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (layer_3): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (layer_4): Conv2d(
        2048, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (mask_features): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
    (predictor): ZeroShotTransformerPredictor(
      (pe_layer): PositionEmbeddingSine()
      (transformer): Transformer(
        (encoder): TransformerEncoder(
          (layers): ModuleList()
        )
        (decoder): TransformerDecoder(
          (layers): ModuleList(
            (0-5): 6 x TransformerDecoderLayer(
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (multihead_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (linear1): Linear(in_features=256, out_features=2048, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (linear2): Linear(in_features=2048, out_features=256, bias=True)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (dropout1): Dropout(p=0.1, inplace=False)
              (dropout2): Dropout(p=0.1, inplace=False)
              (dropout3): Dropout(p=0.1, inplace=False)
            )
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (query_embed): Embedding(100, 256)
      (input_proj): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
      (mask_embed): MLP(
        (layers): ModuleList(
          (0-2): 3 x Linear(in_features=256, out_features=256, bias=True)
        )
      )
      (class_embed): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=1024, bias=True)
          (1): Linear(in_features=1024, out_features=512, bias=True)
        )
      )
    )
  )
  (criterion): SetCriterion(
    (matcher): Matcher HungarianMatcher
        cost_class: 1
        cost_mask: 20.0
        cost_dice: 1.0
  )
  (clip_adapter): MaskFormerClipFeatureAdapter(
    (clip_model): CLIP(
      (visual): VisionTransformerLearnPosition(
        (conv1): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False)
        (ln_pre): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (transformer): TransformerVitPosition(
          (resblocks): Sequential(
            (0): ResidualAttentionBlockVitPosition(
              (attn): MultiheadAttentionLoRA(
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (1): ResidualAttentionBlockVitPosition(
              (attn): MultiheadAttentionLoRA(
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (2): ResidualAttentionBlockVitPosition(
              (attn): MultiheadAttentionLoRA(
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (3): ResidualAttentionBlockVitPosition(
              (attn): MultiheadAttentionLoRA(
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (4): ResidualAttentionBlockVitPosition(
              (attn): MultiheadAttentionLoRA(
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (5): ResidualAttentionBlockVitPosition(
              (attn): MultiheadAttentionLoRA(
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (6): ResidualAttentionBlockVitPosition(
              (attn): MultiheadAttentionLoRA(
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (7): ResidualAttentionBlockVitPosition(
              (attn): MultiheadAttentionLoRA(
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (8): ResidualAttentionBlockVitPosition(
              (attn): MultiheadAttentionLoRA(
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (9): ResidualAttentionBlockVitPosition(
              (attn): MultiheadAttentionLoRA(
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (10): ResidualAttentionBlockVitPosition(
              (attn): MultiheadAttentionLoRA(
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (11): ResidualAttentionBlockVitPosition(
              (attn): MultiheadAttentionVit(
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
        (ln_post): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (transformer): Transformer(
        (resblocks): Sequential(
          (0): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (1): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (2): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (3): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (4): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (5): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (6): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (7): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (8): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (9): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (10): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (11): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (token_embedding): Embedding(49408, 512)
      (ln_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (prompt_learner): LearnablePromptExtractor(
      prefix_prompt:16,suffix_prompt:0,dimension:512
      [Normal_Init(mu=0,std=0.02)]
    )
  )
  (decoder): TransformerDecoderLinear(
    (layers): ModuleList(
      (0): TransformerDecoderLayerLinear(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
        )
        (multihead_attn_my): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
        )
        (linear1): Linear(in_features=512, out_features=1024, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=1024, out_features=512, bias=True)
        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
    )
    (outlayer): TransformerDecoderOutLayerLinear(
      (self_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
      )
      (multihead_attn_my): MultiHeadAttentionMy(
        (dropout_F): Dropout(p=0.1, inplace=False)
      )
      (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (dropout1): Dropout(p=0.1, inplace=False)
      (dropout2): Dropout(p=0.1, inplace=False)
      (dropout3): Dropout(p=0.1, inplace=False)
    )
    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  )
  (pe_layer): PositionEmbeddingSine()
  (query_embedding): Embedding(100, 512)
)
[01/13 16:28:47] mask_former.data.dataset_mappers.mask_former_semantic_dataset_mapper INFO: [MaskFormerSemanticDatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=(256, 307, 358, 409, 460, 512, 563, 614, 665, 716, 768), max_size=2560, sample_style='choice'), RandomCrop_CategoryAreaConstraint(crop_type='absolute', crop_size=[640, 640], single_category_max_area=1.0, ignored_category=255), <detectron2.projects.point_rend.color_augmentation.ColorAugSSDTransform object at 0x7f32ac27efe0>, RandomFlip()]
[01/13 16:28:49] detectron2.data.datasets.coco INFO: Loaded 118287 images with semantic segmentation from /scratch/local/ssd/anjun/datasets/coco-stuff/images/train2017
[01/13 16:28:49] mask_former.data.build INFO: Using training sampler TrainingSampler
[01/13 16:28:49] detectron2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[01/13 16:28:49] detectron2.data.common INFO: Serializing 118287 elements to byte tensors and concatenating them all ...
[01/13 16:28:50] detectron2.data.common INFO: Serialized dataset takes 32.38 MiB
[01/13 16:28:50] detectron2.data.build INFO: Making batched data loader with batch_size=4
[01/13 16:28:50] detectron2.checkpoint.detection_checkpoint INFO: [DetectionCheckpointer] Loading from pretrained_models/deop_model_final.pth ...
[01/13 16:28:50] fvcore.common.checkpoint INFO: [Checkpointer] Loading from pretrained_models/deop_model_final.pth ...
[01/13 16:29:51] detectron2 INFO: Rank of current process: 1. World size: 4
[01/13 16:29:58] detectron2 INFO: Environment info:
-------------------------------  --------------------------------------------------------------------------------------------------
sys.platform                     linux
Python                           3.10.11 (main, May 16 2023, 00:28:57) [GCC 11.2.0]
numpy                            1.26.2
detectron2                       0.6 @/homes/55/anjun/.local/lib/python3.10/site-packages/detectron2
Compiler                         GCC 8.3
CUDA compiler                    CUDA 12.1
detectron2 arch flags            8.6
DETECTRON2_ENV_MODULE            <not set>
PyTorch                          2.1.1+cu121 @/scratch/local/ssd/anjun/anaconda3/envs/ldm/lib/python3.10/site-packages/torch
PyTorch debug build              False
torch._C._GLIBCXX_USE_CXX11_ABI  False
GPU available                    Yes
GPU 0,1,2,3                      NVIDIA A40 (arch=8.6)
Driver version                   530.30.02
CUDA_HOME                        /usr/local/cuda-12.1/
Pillow                           9.5.0
torchvision                      0.16.1+cu121 @/scratch/local/ssd/anjun/anaconda3/envs/ldm/lib/python3.10/site-packages/torchvision
torchvision arch flags           5.0, 6.0, 7.0, 7.5, 8.0, 8.6, 9.0
fvcore                           0.1.5.post20221221
iopath                           0.1.9
cv2                              4.5.4-dev
-------------------------------  --------------------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v3.1.1 (Git Hash 64f6bcbcbab628e96f33a62c3e975f8535a7bde4)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 12.1
  - NVCC architecture flags: -gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.9.2
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=12.1, CUDNN_VERSION=8.9.2, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=old-style-cast -Wno-invalid-partial-specialization -Wno-unused-private-field -Wno-aligned-allocation-unavailable -Wno-missing-braces -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.1.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[01/13 16:29:58] detectron2 INFO: Command line arguments: Namespace(config_file='configs/coco-stuff-164k-156/zero_shot_maskformer_R101c_bs32_60k_proposalmask_featupsample_img512.yaml', resume=True, eval_only=False, num_gpus=4, num_machines=1, machine_rank=0, dist_url='tcp://0.0.0.0:12237', opts=['MODEL.CLIP_ADAPTER.CLIP_ENSEMBLE', 'False', 'MODEL.CLIP_ADAPTER.CLIP_ENSEMBLE_WEIGHT', '-1.0', 'SOLVER.TEST_IMS_PER_BATCH', '1', 'OUTPUT_DIR', 'deop/train_log', 'MODEL.CLIP_ADAPTER.PROMPT_LEARNER', 'learnable', 'SOLVER.IMS_PER_BATCH', '16', 'SOLVER.BASE_LR', '0.00005', 'SOLVER.CHECKPOINT_PERIOD', '1000', 'MODEL.CLIP_ADAPTER.CLIP_MODEL_NAME', 'ViT-B/16', 'MODEL.WEIGHTS', 'pretrained_models/deop_model_final.pth', 'MODEL.META_ARCHITECTURE', 'ZeroShotClipfeatUpsample_addnorm_vit16Query2D_maskvit', 'MODEL.NUM_DECODER_LAYER', '1', 'ORACLE', 'False', 'INPUT.MIN_SIZE_TEST', '512', 'TEST.EVAL_PERIOD', '1000', 'INPUT.SIZE_DIVISIBILITY', '512', 'MODEL.MASK_FORMER.DECODER_DICE_WEIGHT', '0.8', 'MODEL.MASK_FORMER.DECODER_CE_WEIGHT', '2.0', 'MODEL.CLIP_ADAPTER.LEARN_POSITION', 'True', 'MODEL.CLIP_ADAPTER.POSITION_LAYERS', '[1,2,3,4,5,6,7,8,9,10,11]', 'MODEL.CLIP_ADAPTER.LAYERMASKVIT', '[11]', 'MODEL.CLIP_ADAPTER.END2ENDTRAIN', 'False', 'MODEL.CLIP_ADAPTER.PS_SHORTCUT', '1.0', 'DATASETS.TRAIN', "('coco_2017_train_stuff_all_sem_seg',)", 'DATASETS.TEST', "('context_59_test_sem_seg',)", 'MODEL.SEM_SEG_HEAD.NUM_CLASSES', '171', 'MODEL.MASK_FORMER.LOSS_NUM_CLASS', '171'])
[01/13 16:29:58] detectron2 INFO: Contents of args.config_file=configs/coco-stuff-164k-156/zero_shot_maskformer_R101c_bs32_60k_proposalmask_featupsample_img512.yaml:
[38;5;197m_BASE_[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mzero_shot_maskformer_R50_bs32_60k.yaml[39m
[38;5;197mMODEL[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mSEM_SEG_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mNUM_CLASSES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m171[39m
[38;5;15m  [39m[38;5;197mMETA_ARCHITECTURE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mZeroShotMaskFormerClipfeatUpsample[39m[38;5;186m"[39m
[38;5;15m  [39m[38;5;197mBACKBONE[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mbuild_resnet_deeplab_backbone[39m[38;5;186m"[39m
[38;5;15m  [39m
[38;5;15m  [39m[38;5;197mWEIGHTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mdetectron2://DeepLab/R-103.pkl[39m[38;5;186m"[39m
[38;5;15m  [39m[38;5;197mRESNETS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mDEPTH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m101[39m
[38;5;15m    [39m[38;5;197mSTEM_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mdeeplab[39m[38;5;186m"[39m
[38;5;15m    [39m[38;5;197mSTEM_OUT_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m128[39m
[38;5;15m    [39m[38;5;197mSTRIDE_IN_1X1[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mFalse[39m
[38;5;15m    [39m[38;5;197mOUT_FEATURES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;186m"[39m[38;5;186mres2[39m[38;5;186m"[39m[38;5;15m,[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mres3[39m[38;5;186m"[39m[38;5;15m,[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mres4[39m[38;5;186m"[39m[38;5;15m,[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mres5[39m[38;5;186m"[39m[38;5;15m][39m
[38;5;15m    [39m[38;5;242m# NORM: "SyncBN"[39m
[38;5;15m    [39m[38;5;197mRES5_MULTI_GRID[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m1[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15m2[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15m4[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;197mCLIP_ADAPTER[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCLIP_ENSEMBLE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mTrue[39m
[38;5;15m    [39m[38;5;197mCLIP_ENSEMBLE_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.8[39m
[38;5;197mINPUT[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mMIN_SIZE_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;81m!!python/object/apply:eval[39m[38;5;15m [39m[38;5;15m[[39m[38;5;186m"[39m[38;5;186m[int(x[39m[38;5;15m [39m[38;5;186m*[39m[38;5;15m [39m[38;5;186m0.1[39m[38;5;15m [39m[38;5;186m*[39m[38;5;15m [39m[38;5;186m512)[39m[38;5;15m [39m[38;5;186mfor[39m[38;5;15m [39m[38;5;186mx[39m[38;5;15m [39m[38;5;186min[39m[38;5;15m [39m[38;5;186mrange(5,[39m[38;5;15m [39m[38;5;186m16)][39m[38;5;186m"[39m[38;5;15m][39m
[38;5;197mOUTPUT_DIR[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mdeop/train_log[39m[38;5;186m"[39m
[38;5;197mSOLVER[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mIMS_PER_BATCH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m8[39m
[38;5;15m  [39m[38;5;197mTEST_IMS_PER_BATCH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m8[39m
[38;5;197mTEST[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mEVAL_PERIOD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m5000[39m

[01/13 16:29:58] detectron2.utils.env INFO: Using a generated random seed 58274849
[01/13 16:29:58] mask_former.modeling.clip_adapter INFO: Prompt Learner training params: ['prefix_prompt']
[01/13 16:30:01] detectron2.engine.defaults INFO: Model:
ZeroShotClipfeatUpsample_addnorm_vit16Query2D_maskvit(
  (backbone): ResNet(
    (stem): DeepLabStem(
      (conv1): Conv2d(
        3, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
      )
      (conv3): Conv2d(
        64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
      )
    )
    (res2): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv1): Conv2d(
          128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
      )
    )
    (res3): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv1): Conv2d(
          256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
      (3): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
    )
    (res4): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
        (conv1): Conv2d(
          512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (3): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (4): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (5): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (6): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (7): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (8): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (9): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (10): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (11): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (12): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (13): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (14): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (15): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (16): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (17): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (18): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (19): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (20): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (21): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (22): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
    )
    (res5): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
        (conv1): Conv2d(
          1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
      )
    )
  )
  (sem_seg_head): ZeroShotMaskFormerHead(
    (pixel_decoder): BasePixelDecoder(
      (adapter_1): Conv2d(
        256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (layer_1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (adapter_2): Conv2d(
        512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (layer_2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (adapter_3): Conv2d(
        1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (layer_3): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (layer_4): Conv2d(
        2048, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (mask_features): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
    (predictor): ZeroShotTransformerPredictor(
      (pe_layer): PositionEmbeddingSine()
      (transformer): Transformer(
        (encoder): TransformerEncoder(
          (layers): ModuleList()
        )
        (decoder): TransformerDecoder(
          (layers): ModuleList(
            (0-5): 6 x TransformerDecoderLayer(
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (multihead_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (linear1): Linear(in_features=256, out_features=2048, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (linear2): Linear(in_features=2048, out_features=256, bias=True)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (dropout1): Dropout(p=0.1, inplace=False)
              (dropout2): Dropout(p=0.1, inplace=False)
              (dropout3): Dropout(p=0.1, inplace=False)
            )
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (query_embed): Embedding(100, 256)
      (input_proj): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
      (mask_embed): MLP(
        (layers): ModuleList(
          (0-2): 3 x Linear(in_features=256, out_features=256, bias=True)
        )
      )
      (class_embed): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=1024, bias=True)
          (1): Linear(in_features=1024, out_features=512, bias=True)
        )
      )
    )
  )
  (criterion): SetCriterion(
    (matcher): Matcher HungarianMatcher
        cost_class: 1
        cost_mask: 20.0
        cost_dice: 1.0
  )
  (clip_adapter): MaskFormerClipFeatureAdapter(
    (clip_model): CLIP(
      (visual): VisionTransformerLearnPosition(
        (conv1): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False)
        (ln_pre): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (transformer): TransformerVitPosition(
          (resblocks): Sequential(
            (0): ResidualAttentionBlockVitPosition(
              (attn): MultiheadAttentionLoRA(
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (1): ResidualAttentionBlockVitPosition(
              (attn): MultiheadAttentionLoRA(
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (2): ResidualAttentionBlockVitPosition(
              (attn): MultiheadAttentionLoRA(
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (3): ResidualAttentionBlockVitPosition(
              (attn): MultiheadAttentionLoRA(
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (4): ResidualAttentionBlockVitPosition(
              (attn): MultiheadAttentionLoRA(
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (5): ResidualAttentionBlockVitPosition(
              (attn): MultiheadAttentionLoRA(
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (6): ResidualAttentionBlockVitPosition(
              (attn): MultiheadAttentionLoRA(
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (7): ResidualAttentionBlockVitPosition(
              (attn): MultiheadAttentionLoRA(
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (8): ResidualAttentionBlockVitPosition(
              (attn): MultiheadAttentionLoRA(
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (9): ResidualAttentionBlockVitPosition(
              (attn): MultiheadAttentionLoRA(
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (10): ResidualAttentionBlockVitPosition(
              (attn): MultiheadAttentionLoRA(
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (11): ResidualAttentionBlockVitPosition(
              (attn): MultiheadAttentionVit(
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
        (ln_post): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (transformer): Transformer(
        (resblocks): Sequential(
          (0): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (1): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (2): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (3): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (4): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (5): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (6): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (7): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (8): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (9): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (10): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (11): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (token_embedding): Embedding(49408, 512)
      (ln_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (prompt_learner): LearnablePromptExtractor(
      prefix_prompt:16,suffix_prompt:0,dimension:512
      [Normal_Init(mu=0,std=0.02)]
    )
  )
  (decoder): TransformerDecoderLinear(
    (layers): ModuleList(
      (0): TransformerDecoderLayerLinear(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
        )
        (multihead_attn_my): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
        )
        (linear1): Linear(in_features=512, out_features=1024, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=1024, out_features=512, bias=True)
        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
    )
    (outlayer): TransformerDecoderOutLayerLinear(
      (self_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
      )
      (multihead_attn_my): MultiHeadAttentionMy(
        (dropout_F): Dropout(p=0.1, inplace=False)
      )
      (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (dropout1): Dropout(p=0.1, inplace=False)
      (dropout2): Dropout(p=0.1, inplace=False)
      (dropout3): Dropout(p=0.1, inplace=False)
    )
    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  )
  (pe_layer): PositionEmbeddingSine()
  (query_embedding): Embedding(100, 512)
)
[01/13 16:30:01] mask_former.data.dataset_mappers.mask_former_semantic_dataset_mapper INFO: [MaskFormerSemanticDatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=(256, 307, 358, 409, 460, 512, 563, 614, 665, 716, 768), max_size=2560, sample_style='choice'), RandomCrop_CategoryAreaConstraint(crop_type='absolute', crop_size=[640, 640], single_category_max_area=1.0, ignored_category=255), <detectron2.projects.point_rend.color_augmentation.ColorAugSSDTransform object at 0x7f82a8217130>, RandomFlip()]
[01/13 16:30:04] detectron2.data.datasets.coco INFO: Loaded 118287 images with semantic segmentation from /scratch/local/ssd/anjun/datasets/coco-stuff/images/train2017
[01/13 16:30:04] mask_former.data.build INFO: Using training sampler TrainingSampler
[01/13 16:30:04] detectron2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[01/13 16:30:04] detectron2.data.common INFO: Serializing 118287 elements to byte tensors and concatenating them all ...
[01/13 16:30:04] detectron2.data.common INFO: Serialized dataset takes 32.38 MiB
[01/13 16:30:04] detectron2.data.build INFO: Making batched data loader with batch_size=4
[01/13 16:30:05] detectron2.checkpoint.detection_checkpoint INFO: [DetectionCheckpointer] Loading from pretrained_models/deop_model_final.pth ...
[01/13 16:30:05] fvcore.common.checkpoint INFO: [Checkpointer] Loading from pretrained_models/deop_model_final.pth ...
[01/13 16:30:05] fvcore.common.checkpoint WARNING: Skip loading parameter 'criterion.empty_weight' to the model due to incompatible shapes: (157,) in the checkpoint but (172,) in the model! You might want to double check if this is expected.
[01/13 16:30:05] fvcore.common.checkpoint WARNING: Some model parameters or buffers are not found in the checkpoint:
[34mclip_adapter.clip_model.token_embedding.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.10.learned_position[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.11.attn.out_proj.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.11.learned_position[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.6.learned_position[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.7.learned_position[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.8.learned_position[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.9.learned_position[0m
[34mcriterion.empty_weight[0m
[01/13 16:30:05] detectron2.engine.train_loop INFO: Starting training from iteration 0
[01/13 16:30:22] detectron2.engine.train_loop ERROR: Exception during training:
Traceback (most recent call last):
  File "/homes/55/anjun/.local/lib/python3.10/site-packages/detectron2/engine/train_loop.py", line 155, in train
    self.run_step()
  File "/scratch/local/ssd/anjun/ovseg/DeOP/train_net.py", line 361, in run_step
    loss_dict = self.model(data)
  File "/scratch/local/ssd/anjun/anaconda3/envs/ldm/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/local/ssd/anjun/anaconda3/envs/ldm/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/local/ssd/anjun/anaconda3/envs/ldm/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1515, in forward
    inputs, kwargs = self._pre_forward(*inputs, **kwargs)
  File "/scratch/local/ssd/anjun/anaconda3/envs/ldm/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1409, in _pre_forward
    if torch.is_grad_enabled() and self.reducer._rebuild_buckets():
RuntimeError: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by 
making sure all `forward` function outputs participate in calculating loss. 
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameter indices which did not receive grad for rank 1: 13 14 15 16
 In addition, you can set the environment variable TORCH_DISTRIBUTED_DEBUG to either INFO or DETAIL to print out information about which particular parameters did not receive gradient on this rank as part of this error
[01/13 16:30:22] detectron2.engine.hooks INFO: Total training time: 0:00:00 (0:00:00 on hooks)
[01/13 16:39:24] detectron2 INFO: Rank of current process: 1. World size: 4
[01/13 16:39:30] detectron2 INFO: Environment info:
-------------------------------  --------------------------------------------------------------------------------------------------
sys.platform                     linux
Python                           3.10.11 (main, May 16 2023, 00:28:57) [GCC 11.2.0]
numpy                            1.26.2
detectron2                       0.6 @/homes/55/anjun/.local/lib/python3.10/site-packages/detectron2
Compiler                         GCC 8.3
CUDA compiler                    CUDA 12.1
detectron2 arch flags            8.6
DETECTRON2_ENV_MODULE            <not set>
PyTorch                          2.1.1+cu121 @/scratch/local/ssd/anjun/anaconda3/envs/ldm/lib/python3.10/site-packages/torch
PyTorch debug build              False
torch._C._GLIBCXX_USE_CXX11_ABI  False
GPU available                    Yes
GPU 0,1,2,3                      NVIDIA A40 (arch=8.6)
Driver version                   530.30.02
CUDA_HOME                        /usr/local/cuda-12.1/
Pillow                           9.5.0
torchvision                      0.16.1+cu121 @/scratch/local/ssd/anjun/anaconda3/envs/ldm/lib/python3.10/site-packages/torchvision
torchvision arch flags           5.0, 6.0, 7.0, 7.5, 8.0, 8.6, 9.0
fvcore                           0.1.5.post20221221
iopath                           0.1.9
cv2                              4.5.4-dev
-------------------------------  --------------------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v3.1.1 (Git Hash 64f6bcbcbab628e96f33a62c3e975f8535a7bde4)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 12.1
  - NVCC architecture flags: -gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.9.2
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=12.1, CUDNN_VERSION=8.9.2, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=old-style-cast -Wno-invalid-partial-specialization -Wno-unused-private-field -Wno-aligned-allocation-unavailable -Wno-missing-braces -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.1.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[01/13 16:39:30] detectron2 INFO: Command line arguments: Namespace(config_file='configs/coco-stuff-164k-156/zero_shot_maskformer_R101c_bs32_60k_proposalmask_featupsample_img512.yaml', resume=True, eval_only=False, num_gpus=4, num_machines=1, machine_rank=0, dist_url='tcp://0.0.0.0:12237', opts=['MODEL.CLIP_ADAPTER.CLIP_ENSEMBLE', 'False', 'MODEL.CLIP_ADAPTER.CLIP_ENSEMBLE_WEIGHT', '-1.0', 'SOLVER.TEST_IMS_PER_BATCH', '1', 'OUTPUT_DIR', 'deop/train_log', 'MODEL.CLIP_ADAPTER.PROMPT_LEARNER', 'learnable', 'SOLVER.IMS_PER_BATCH', '16', 'SOLVER.BASE_LR', '0.00005', 'SOLVER.CHECKPOINT_PERIOD', '1000', 'MODEL.CLIP_ADAPTER.CLIP_MODEL_NAME', 'ViT-B/16', 'MODEL.WEIGHTS', 'pretrained_models/deop_model_final.pth', 'MODEL.META_ARCHITECTURE', 'ZeroShotClipfeatUpsample_addnorm_vit16Query2D_maskvit', 'MODEL.NUM_DECODER_LAYER', '1', 'ORACLE', 'False', 'INPUT.MIN_SIZE_TEST', '512', 'TEST.EVAL_PERIOD', '1000', 'INPUT.SIZE_DIVISIBILITY', '512', 'MODEL.MASK_FORMER.DECODER_DICE_WEIGHT', '0.8', 'MODEL.MASK_FORMER.DECODER_CE_WEIGHT', '2.0', 'MODEL.CLIP_ADAPTER.LEARN_POSITION', 'True', 'MODEL.CLIP_ADAPTER.POSITION_LAYERS', '[1,2,3,4,5,6,7,8,9,10,11]', 'MODEL.CLIP_ADAPTER.LAYERMASKVIT', '[11]', 'MODEL.CLIP_ADAPTER.END2ENDTRAIN', 'False', 'MODEL.CLIP_ADAPTER.PS_SHORTCUT', '1.0', 'DATASETS.TRAIN', "('coco_2017_train_stuff_all_sem_seg',)", 'DATASETS.TEST', "('context_59_test_sem_seg',)", 'MODEL.SEM_SEG_HEAD.NUM_CLASSES', '171', 'MODEL.MASK_FORMER.LOSS_NUM_CLASS', '171'])
[01/13 16:39:30] detectron2 INFO: Contents of args.config_file=configs/coco-stuff-164k-156/zero_shot_maskformer_R101c_bs32_60k_proposalmask_featupsample_img512.yaml:
[38;5;197m_BASE_[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mzero_shot_maskformer_R50_bs32_60k.yaml[39m
[38;5;197mMODEL[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mSEM_SEG_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mNUM_CLASSES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m171[39m
[38;5;15m  [39m[38;5;197mMETA_ARCHITECTURE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mZeroShotMaskFormerClipfeatUpsample[39m[38;5;186m"[39m
[38;5;15m  [39m[38;5;197mBACKBONE[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mbuild_resnet_deeplab_backbone[39m[38;5;186m"[39m
[38;5;15m  [39m
[38;5;15m  [39m[38;5;197mWEIGHTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mdetectron2://DeepLab/R-103.pkl[39m[38;5;186m"[39m
[38;5;15m  [39m[38;5;197mRESNETS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mDEPTH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m101[39m
[38;5;15m    [39m[38;5;197mSTEM_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mdeeplab[39m[38;5;186m"[39m
[38;5;15m    [39m[38;5;197mSTEM_OUT_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m128[39m
[38;5;15m    [39m[38;5;197mSTRIDE_IN_1X1[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mFalse[39m
[38;5;15m    [39m[38;5;197mOUT_FEATURES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;186m"[39m[38;5;186mres2[39m[38;5;186m"[39m[38;5;15m,[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mres3[39m[38;5;186m"[39m[38;5;15m,[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mres4[39m[38;5;186m"[39m[38;5;15m,[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mres5[39m[38;5;186m"[39m[38;5;15m][39m
[38;5;15m    [39m[38;5;242m# NORM: "SyncBN"[39m
[38;5;15m    [39m[38;5;197mRES5_MULTI_GRID[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m1[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15m2[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15m4[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;197mCLIP_ADAPTER[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCLIP_ENSEMBLE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mTrue[39m
[38;5;15m    [39m[38;5;197mCLIP_ENSEMBLE_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.8[39m
[38;5;197mINPUT[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mMIN_SIZE_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;81m!!python/object/apply:eval[39m[38;5;15m [39m[38;5;15m[[39m[38;5;186m"[39m[38;5;186m[int(x[39m[38;5;15m [39m[38;5;186m*[39m[38;5;15m [39m[38;5;186m0.1[39m[38;5;15m [39m[38;5;186m*[39m[38;5;15m [39m[38;5;186m512)[39m[38;5;15m [39m[38;5;186mfor[39m[38;5;15m [39m[38;5;186mx[39m[38;5;15m [39m[38;5;186min[39m[38;5;15m [39m[38;5;186mrange(5,[39m[38;5;15m [39m[38;5;186m16)][39m[38;5;186m"[39m[38;5;15m][39m
[38;5;197mOUTPUT_DIR[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mdeop/train_log[39m[38;5;186m"[39m
[38;5;197mSOLVER[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mIMS_PER_BATCH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m8[39m
[38;5;15m  [39m[38;5;197mTEST_IMS_PER_BATCH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m8[39m
[38;5;197mTEST[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mEVAL_PERIOD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m5000[39m

[01/13 16:39:30] detectron2.utils.env INFO: Using a generated random seed 30608454
[01/13 16:39:31] mask_former.modeling.clip_adapter INFO: Prompt Learner training params: ['prefix_prompt']
[01/13 16:41:38] detectron2 INFO: Rank of current process: 1. World size: 4
[01/13 16:41:43] detectron2 INFO: Environment info:
-------------------------------  --------------------------------------------------------------------------------------------------
sys.platform                     linux
Python                           3.10.11 (main, May 16 2023, 00:28:57) [GCC 11.2.0]
numpy                            1.26.2
detectron2                       0.6 @/homes/55/anjun/.local/lib/python3.10/site-packages/detectron2
Compiler                         GCC 8.3
CUDA compiler                    CUDA 12.1
detectron2 arch flags            8.6
DETECTRON2_ENV_MODULE            <not set>
PyTorch                          2.1.1+cu121 @/scratch/local/ssd/anjun/anaconda3/envs/ldm/lib/python3.10/site-packages/torch
PyTorch debug build              False
torch._C._GLIBCXX_USE_CXX11_ABI  False
GPU available                    Yes
GPU 0,1,2,3                      NVIDIA A40 (arch=8.6)
Driver version                   530.30.02
CUDA_HOME                        /usr/local/cuda-12.1/
Pillow                           9.5.0
torchvision                      0.16.1+cu121 @/scratch/local/ssd/anjun/anaconda3/envs/ldm/lib/python3.10/site-packages/torchvision
torchvision arch flags           5.0, 6.0, 7.0, 7.5, 8.0, 8.6, 9.0
fvcore                           0.1.5.post20221221
iopath                           0.1.9
cv2                              4.5.4-dev
-------------------------------  --------------------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v3.1.1 (Git Hash 64f6bcbcbab628e96f33a62c3e975f8535a7bde4)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 12.1
  - NVCC architecture flags: -gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.9.2
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=12.1, CUDNN_VERSION=8.9.2, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=old-style-cast -Wno-invalid-partial-specialization -Wno-unused-private-field -Wno-aligned-allocation-unavailable -Wno-missing-braces -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.1.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[01/13 16:41:43] detectron2 INFO: Command line arguments: Namespace(config_file='configs/coco-stuff-164k-156/zero_shot_maskformer_R101c_bs32_60k_proposalmask_featupsample_img512.yaml', resume=True, eval_only=False, num_gpus=4, num_machines=1, machine_rank=0, dist_url='tcp://0.0.0.0:12237', opts=['MODEL.CLIP_ADAPTER.CLIP_ENSEMBLE', 'False', 'MODEL.CLIP_ADAPTER.CLIP_ENSEMBLE_WEIGHT', '-1.0', 'SOLVER.TEST_IMS_PER_BATCH', '1', 'OUTPUT_DIR', 'deop/train_log', 'MODEL.CLIP_ADAPTER.PROMPT_LEARNER', 'learnable', 'SOLVER.IMS_PER_BATCH', '16', 'SOLVER.BASE_LR', '0.00005', 'SOLVER.CHECKPOINT_PERIOD', '1000', 'MODEL.CLIP_ADAPTER.CLIP_MODEL_NAME', 'ViT-B/16', 'MODEL.WEIGHTS', 'pretrained_models/deop_model_final.pth', 'MODEL.META_ARCHITECTURE', 'ZeroShotClipfeatUpsample_addnorm_vit16Query2D_maskvit', 'MODEL.NUM_DECODER_LAYER', '1', 'ORACLE', 'False', 'INPUT.MIN_SIZE_TEST', '512', 'TEST.EVAL_PERIOD', '1000', 'INPUT.SIZE_DIVISIBILITY', '512', 'MODEL.MASK_FORMER.DECODER_DICE_WEIGHT', '0.8', 'MODEL.MASK_FORMER.DECODER_CE_WEIGHT', '2.0', 'MODEL.CLIP_ADAPTER.LEARN_POSITION', 'True', 'MODEL.CLIP_ADAPTER.POSITION_LAYERS', '[1,2,3,4,5,6,7,8,9,10,11]', 'MODEL.CLIP_ADAPTER.LAYERMASKVIT', '[11]', 'MODEL.CLIP_ADAPTER.END2ENDTRAIN', 'False', 'MODEL.CLIP_ADAPTER.PS_SHORTCUT', '1.0', 'DATASETS.TRAIN', "('coco_2017_train_stuff_all_sem_seg',)", 'DATASETS.TEST', "('context_59_test_sem_seg',)", 'MODEL.SEM_SEG_HEAD.NUM_CLASSES', '171', 'MODEL.MASK_FORMER.LOSS_NUM_CLASS', '171'])
[01/13 16:41:43] detectron2 INFO: Contents of args.config_file=configs/coco-stuff-164k-156/zero_shot_maskformer_R101c_bs32_60k_proposalmask_featupsample_img512.yaml:
[38;5;197m_BASE_[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mzero_shot_maskformer_R50_bs32_60k.yaml[39m
[38;5;197mMODEL[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mSEM_SEG_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mNUM_CLASSES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m171[39m
[38;5;15m  [39m[38;5;197mMETA_ARCHITECTURE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mZeroShotMaskFormerClipfeatUpsample[39m[38;5;186m"[39m
[38;5;15m  [39m[38;5;197mBACKBONE[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mbuild_resnet_deeplab_backbone[39m[38;5;186m"[39m
[38;5;15m  [39m
[38;5;15m  [39m[38;5;197mWEIGHTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mdetectron2://DeepLab/R-103.pkl[39m[38;5;186m"[39m
[38;5;15m  [39m[38;5;197mRESNETS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mDEPTH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m101[39m
[38;5;15m    [39m[38;5;197mSTEM_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mdeeplab[39m[38;5;186m"[39m
[38;5;15m    [39m[38;5;197mSTEM_OUT_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m128[39m
[38;5;15m    [39m[38;5;197mSTRIDE_IN_1X1[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mFalse[39m
[38;5;15m    [39m[38;5;197mOUT_FEATURES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;186m"[39m[38;5;186mres2[39m[38;5;186m"[39m[38;5;15m,[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mres3[39m[38;5;186m"[39m[38;5;15m,[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mres4[39m[38;5;186m"[39m[38;5;15m,[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mres5[39m[38;5;186m"[39m[38;5;15m][39m
[38;5;15m    [39m[38;5;242m# NORM: "SyncBN"[39m
[38;5;15m    [39m[38;5;197mRES5_MULTI_GRID[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m1[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15m2[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15m4[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;197mCLIP_ADAPTER[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCLIP_ENSEMBLE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mTrue[39m
[38;5;15m    [39m[38;5;197mCLIP_ENSEMBLE_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.8[39m
[38;5;197mINPUT[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mMIN_SIZE_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;81m!!python/object/apply:eval[39m[38;5;15m [39m[38;5;15m[[39m[38;5;186m"[39m[38;5;186m[int(x[39m[38;5;15m [39m[38;5;186m*[39m[38;5;15m [39m[38;5;186m0.1[39m[38;5;15m [39m[38;5;186m*[39m[38;5;15m [39m[38;5;186m512)[39m[38;5;15m [39m[38;5;186mfor[39m[38;5;15m [39m[38;5;186mx[39m[38;5;15m [39m[38;5;186min[39m[38;5;15m [39m[38;5;186mrange(5,[39m[38;5;15m [39m[38;5;186m16)][39m[38;5;186m"[39m[38;5;15m][39m
[38;5;197mOUTPUT_DIR[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mdeop/train_log[39m[38;5;186m"[39m
[38;5;197mSOLVER[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mIMS_PER_BATCH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m8[39m
[38;5;15m  [39m[38;5;197mTEST_IMS_PER_BATCH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m8[39m
[38;5;197mTEST[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mEVAL_PERIOD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m5000[39m

[01/13 16:41:43] detectron2.utils.env INFO: Using a generated random seed 44005556
[01/13 16:41:44] mask_former.modeling.clip_adapter INFO: Prompt Learner training params: ['prefix_prompt']
[01/13 16:41:50] detectron2.engine.defaults INFO: Model:
ZeroShotClipfeatUpsample_addnorm_vit16Query2D_maskvit(
  (backbone): ResNet(
    (stem): DeepLabStem(
      (conv1): Conv2d(
        3, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
      )
      (conv3): Conv2d(
        64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
      )
    )
    (res2): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv1): Conv2d(
          128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
      )
    )
    (res3): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv1): Conv2d(
          256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
      (3): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
    )
    (res4): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
        (conv1): Conv2d(
          512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (3): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (4): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (5): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (6): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (7): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (8): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (9): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (10): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (11): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (12): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (13): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (14): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (15): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (16): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (17): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (18): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (19): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (20): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (21): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (22): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
    )
    (res5): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
        (conv1): Conv2d(
          1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
      )
    )
  )
  (sem_seg_head): ZeroShotMaskFormerHead(
    (pixel_decoder): BasePixelDecoder(
      (adapter_1): Conv2d(
        256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (layer_1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (adapter_2): Conv2d(
        512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (layer_2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (adapter_3): Conv2d(
        1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (layer_3): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (layer_4): Conv2d(
        2048, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (mask_features): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
    (predictor): ZeroShotTransformerPredictor(
      (pe_layer): PositionEmbeddingSine()
      (transformer): Transformer(
        (encoder): TransformerEncoder(
          (layers): ModuleList()
        )
        (decoder): TransformerDecoder(
          (layers): ModuleList(
            (0-5): 6 x TransformerDecoderLayer(
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (multihead_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (linear1): Linear(in_features=256, out_features=2048, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (linear2): Linear(in_features=2048, out_features=256, bias=True)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (dropout1): Dropout(p=0.1, inplace=False)
              (dropout2): Dropout(p=0.1, inplace=False)
              (dropout3): Dropout(p=0.1, inplace=False)
            )
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (query_embed): Embedding(100, 256)
      (input_proj): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
      (mask_embed): MLP(
        (layers): ModuleList(
          (0-2): 3 x Linear(in_features=256, out_features=256, bias=True)
        )
      )
      (class_embed): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=1024, bias=True)
          (1): Linear(in_features=1024, out_features=512, bias=True)
        )
      )
    )
  )
  (criterion): SetCriterion(
    (matcher): Matcher HungarianMatcher
        cost_class: 1
        cost_mask: 20.0
        cost_dice: 1.0
  )
  (clip_adapter): MaskFormerClipFeatureAdapter(
    (clip_model): CLIP(
      (visual): VisionTransformerLearnPosition(
        (conv1): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False)
        (ln_pre): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (transformer): TransformerVitPosition(
          (resblocks): Sequential(
            (0): ResidualAttentionBlockVitPosition(
              (attn): MultiheadAttentionLoRA(
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (1): ResidualAttentionBlockVitPosition(
              (attn): MultiheadAttentionLoRA(
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (2): ResidualAttentionBlockVitPosition(
              (attn): MultiheadAttentionLoRA(
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (3): ResidualAttentionBlockVitPosition(
              (attn): MultiheadAttentionLoRA(
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (4): ResidualAttentionBlockVitPosition(
              (attn): MultiheadAttentionLoRA(
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (5): ResidualAttentionBlockVitPosition(
              (attn): MultiheadAttentionLoRA(
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (6): ResidualAttentionBlockVitPosition(
              (attn): MultiheadAttentionLoRA(
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (7): ResidualAttentionBlockVitPosition(
              (attn): MultiheadAttentionLoRA(
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (8): ResidualAttentionBlockVitPosition(
              (attn): MultiheadAttentionLoRA(
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (9): ResidualAttentionBlockVitPosition(
              (attn): MultiheadAttentionLoRA(
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (10): ResidualAttentionBlockVitPosition(
              (attn): MultiheadAttentionLoRA(
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (11): ResidualAttentionBlockVitPosition(
              (attn): MultiheadAttentionVit(
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
        (ln_post): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (transformer): Transformer(
        (resblocks): Sequential(
          (0): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (1): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (2): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (3): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (4): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (5): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (6): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (7): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (8): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (9): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (10): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (11): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (token_embedding): Embedding(49408, 512)
      (ln_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (prompt_learner): LearnablePromptExtractor(
      prefix_prompt:16,suffix_prompt:0,dimension:512
      [Normal_Init(mu=0,std=0.02)]
    )
  )
  (decoder): TransformerDecoderLinear(
    (layers): ModuleList(
      (0): TransformerDecoderLayerLinear(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
        )
        (multihead_attn_my): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
        )
        (linear1): Linear(in_features=512, out_features=1024, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=1024, out_features=512, bias=True)
        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
    )
    (outlayer): TransformerDecoderOutLayerLinear(
      (self_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
      )
      (multihead_attn_my): MultiHeadAttentionMy(
        (dropout_F): Dropout(p=0.1, inplace=False)
      )
      (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (dropout1): Dropout(p=0.1, inplace=False)
      (dropout2): Dropout(p=0.1, inplace=False)
      (dropout3): Dropout(p=0.1, inplace=False)
    )
    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  )
  (pe_layer): PositionEmbeddingSine()
  (query_embedding): Embedding(100, 512)
)
[01/13 16:41:50] mask_former.data.dataset_mappers.mask_former_semantic_dataset_mapper INFO: [MaskFormerSemanticDatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=(256, 307, 358, 409, 460, 512, 563, 614, 665, 716, 768), max_size=2560, sample_style='choice'), RandomCrop_CategoryAreaConstraint(crop_type='absolute', crop_size=[640, 640], single_category_max_area=1.0, ignored_category=255), <detectron2.projects.point_rend.color_augmentation.ColorAugSSDTransform object at 0x7fd456083130>, RandomFlip()]
[01/13 16:41:52] detectron2.data.datasets.coco INFO: Loaded 118287 images with semantic segmentation from /scratch/local/ssd/anjun/datasets/coco-stuff/images/train2017
[01/13 16:41:52] mask_former.data.build INFO: Using training sampler TrainingSampler
[01/13 16:41:52] detectron2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[01/13 16:41:52] detectron2.data.common INFO: Serializing 118287 elements to byte tensors and concatenating them all ...
[01/13 16:41:53] detectron2.data.common INFO: Serialized dataset takes 32.38 MiB
[01/13 16:41:53] detectron2.data.build INFO: Making batched data loader with batch_size=4
[01/13 16:41:54] detectron2.checkpoint.detection_checkpoint INFO: [DetectionCheckpointer] Loading from pretrained_models/deop_model_final.pth ...
[01/13 16:41:54] fvcore.common.checkpoint INFO: [Checkpointer] Loading from pretrained_models/deop_model_final.pth ...
[01/13 16:41:54] fvcore.common.checkpoint WARNING: Skip loading parameter 'criterion.empty_weight' to the model due to incompatible shapes: (157,) in the checkpoint but (172,) in the model! You might want to double check if this is expected.
[01/13 16:41:54] fvcore.common.checkpoint WARNING: Some model parameters or buffers are not found in the checkpoint:
[34mclip_adapter.clip_model.token_embedding.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.0.attn.out_proj.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.0.mlp.c_fc.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.0.mlp.c_proj.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.1.attn.out_proj.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.1.mlp.c_fc.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.1.mlp.c_proj.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.10.attn.out_proj.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.10.learned_position[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.10.mlp.c_fc.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.10.mlp.c_proj.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.11.attn.out_proj.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.11.learned_position[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.11.mlp.c_fc.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.11.mlp.c_proj.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.2.attn.out_proj.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.2.mlp.c_fc.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.2.mlp.c_proj.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.3.attn.out_proj.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.3.mlp.c_fc.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.3.mlp.c_proj.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.4.attn.out_proj.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.4.mlp.c_fc.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.4.mlp.c_proj.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.5.attn.out_proj.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.5.mlp.c_fc.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.5.mlp.c_proj.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.6.attn.out_proj.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.6.learned_position[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.6.mlp.c_fc.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.6.mlp.c_proj.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.7.attn.out_proj.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.7.learned_position[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.7.mlp.c_fc.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.7.mlp.c_proj.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.8.attn.out_proj.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.8.learned_position[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.8.mlp.c_fc.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.8.mlp.c_proj.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.9.attn.out_proj.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.9.learned_position[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.9.mlp.c_fc.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.9.mlp.c_proj.{lora_A, lora_B}[0m
[34mcriterion.empty_weight[0m
[01/13 16:41:54] detectron2.engine.train_loop INFO: Starting training from iteration 0
[01/13 16:42:11] detectron2.engine.train_loop ERROR: Exception during training:
Traceback (most recent call last):
  File "/homes/55/anjun/.local/lib/python3.10/site-packages/detectron2/engine/train_loop.py", line 155, in train
    self.run_step()
  File "/scratch/local/ssd/anjun/ovseg/DeOP/train_net.py", line 361, in run_step
    loss_dict = self.model(data)
  File "/scratch/local/ssd/anjun/anaconda3/envs/ldm/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/local/ssd/anjun/anaconda3/envs/ldm/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/local/ssd/anjun/anaconda3/envs/ldm/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1515, in forward
    inputs, kwargs = self._pre_forward(*inputs, **kwargs)
  File "/scratch/local/ssd/anjun/anaconda3/envs/ldm/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1409, in _pre_forward
    if torch.is_grad_enabled() and self.reducer._rebuild_buckets():
RuntimeError: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by 
making sure all `forward` function outputs participate in calculating loss. 
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameter indices which did not receive grad for rank 1: 2 3 9 10 16 17 23 24 30 31 37 38 44 45 51 52 58 59 65 66 72 73 79 80 85 86
 In addition, you can set the environment variable TORCH_DISTRIBUTED_DEBUG to either INFO or DETAIL to print out information about which particular parameters did not receive gradient on this rank as part of this error
[01/13 16:42:11] detectron2.engine.hooks INFO: Total training time: 0:00:00 (0:00:00 on hooks)
[01/13 16:46:42] detectron2 INFO: Rank of current process: 1. World size: 4
[01/13 16:46:49] detectron2 INFO: Environment info:
-------------------------------  --------------------------------------------------------------------------------------------------
sys.platform                     linux
Python                           3.10.11 (main, May 16 2023, 00:28:57) [GCC 11.2.0]
numpy                            1.26.2
detectron2                       0.6 @/homes/55/anjun/.local/lib/python3.10/site-packages/detectron2
Compiler                         GCC 8.3
CUDA compiler                    CUDA 12.1
detectron2 arch flags            8.6
DETECTRON2_ENV_MODULE            <not set>
PyTorch                          2.1.1+cu121 @/scratch/local/ssd/anjun/anaconda3/envs/ldm/lib/python3.10/site-packages/torch
PyTorch debug build              False
torch._C._GLIBCXX_USE_CXX11_ABI  False
GPU available                    Yes
GPU 0,1,2,3                      NVIDIA A40 (arch=8.6)
Driver version                   530.30.02
CUDA_HOME                        /usr/local/cuda-12.1/
Pillow                           9.5.0
torchvision                      0.16.1+cu121 @/scratch/local/ssd/anjun/anaconda3/envs/ldm/lib/python3.10/site-packages/torchvision
torchvision arch flags           5.0, 6.0, 7.0, 7.5, 8.0, 8.6, 9.0
fvcore                           0.1.5.post20221221
iopath                           0.1.9
cv2                              4.5.4-dev
-------------------------------  --------------------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v3.1.1 (Git Hash 64f6bcbcbab628e96f33a62c3e975f8535a7bde4)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 12.1
  - NVCC architecture flags: -gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.9.2
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=12.1, CUDNN_VERSION=8.9.2, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=old-style-cast -Wno-invalid-partial-specialization -Wno-unused-private-field -Wno-aligned-allocation-unavailable -Wno-missing-braces -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.1.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[01/13 16:46:49] detectron2 INFO: Command line arguments: Namespace(config_file='configs/coco-stuff-164k-156/zero_shot_maskformer_R101c_bs32_60k_proposalmask_featupsample_img512.yaml', resume=True, eval_only=False, num_gpus=4, num_machines=1, machine_rank=0, dist_url='tcp://0.0.0.0:12237', opts=['MODEL.CLIP_ADAPTER.CLIP_ENSEMBLE', 'False', 'MODEL.CLIP_ADAPTER.CLIP_ENSEMBLE_WEIGHT', '-1.0', 'SOLVER.TEST_IMS_PER_BATCH', '1', 'OUTPUT_DIR', 'deop/train_log', 'MODEL.CLIP_ADAPTER.PROMPT_LEARNER', 'learnable', 'SOLVER.IMS_PER_BATCH', '16', 'SOLVER.BASE_LR', '0.00005', 'SOLVER.CHECKPOINT_PERIOD', '1000', 'MODEL.CLIP_ADAPTER.CLIP_MODEL_NAME', 'ViT-B/16', 'MODEL.WEIGHTS', 'pretrained_models/deop_model_final.pth', 'MODEL.META_ARCHITECTURE', 'ZeroShotClipfeatUpsample_addnorm_vit16Query2D_maskvit', 'MODEL.NUM_DECODER_LAYER', '1', 'ORACLE', 'False', 'INPUT.MIN_SIZE_TEST', '512', 'TEST.EVAL_PERIOD', '1000', 'INPUT.SIZE_DIVISIBILITY', '512', 'MODEL.MASK_FORMER.DECODER_DICE_WEIGHT', '0.8', 'MODEL.MASK_FORMER.DECODER_CE_WEIGHT', '2.0', 'MODEL.CLIP_ADAPTER.LEARN_POSITION', 'True', 'MODEL.CLIP_ADAPTER.POSITION_LAYERS', '[1,2,3,4,5,6,7,8,9,10,11]', 'MODEL.CLIP_ADAPTER.LAYERMASKVIT', '[11]', 'MODEL.CLIP_ADAPTER.END2ENDTRAIN', 'False', 'MODEL.CLIP_ADAPTER.PS_SHORTCUT', '1.0', 'DATASETS.TRAIN', "('coco_2017_train_stuff_all_sem_seg',)", 'DATASETS.TEST', "('context_59_test_sem_seg',)", 'MODEL.SEM_SEG_HEAD.NUM_CLASSES', '171', 'MODEL.MASK_FORMER.LOSS_NUM_CLASS', '171'])
[01/13 16:46:49] detectron2 INFO: Contents of args.config_file=configs/coco-stuff-164k-156/zero_shot_maskformer_R101c_bs32_60k_proposalmask_featupsample_img512.yaml:
[38;5;197m_BASE_[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mzero_shot_maskformer_R50_bs32_60k.yaml[39m
[38;5;197mMODEL[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mSEM_SEG_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mNUM_CLASSES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m171[39m
[38;5;15m  [39m[38;5;197mMETA_ARCHITECTURE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mZeroShotMaskFormerClipfeatUpsample[39m[38;5;186m"[39m
[38;5;15m  [39m[38;5;197mBACKBONE[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mbuild_resnet_deeplab_backbone[39m[38;5;186m"[39m
[38;5;15m  [39m
[38;5;15m  [39m[38;5;197mWEIGHTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mdetectron2://DeepLab/R-103.pkl[39m[38;5;186m"[39m
[38;5;15m  [39m[38;5;197mRESNETS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mDEPTH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m101[39m
[38;5;15m    [39m[38;5;197mSTEM_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mdeeplab[39m[38;5;186m"[39m
[38;5;15m    [39m[38;5;197mSTEM_OUT_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m128[39m
[38;5;15m    [39m[38;5;197mSTRIDE_IN_1X1[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mFalse[39m
[38;5;15m    [39m[38;5;197mOUT_FEATURES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;186m"[39m[38;5;186mres2[39m[38;5;186m"[39m[38;5;15m,[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mres3[39m[38;5;186m"[39m[38;5;15m,[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mres4[39m[38;5;186m"[39m[38;5;15m,[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mres5[39m[38;5;186m"[39m[38;5;15m][39m
[38;5;15m    [39m[38;5;242m# NORM: "SyncBN"[39m
[38;5;15m    [39m[38;5;197mRES5_MULTI_GRID[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m1[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15m2[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15m4[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;197mCLIP_ADAPTER[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCLIP_ENSEMBLE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mTrue[39m
[38;5;15m    [39m[38;5;197mCLIP_ENSEMBLE_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.8[39m
[38;5;197mINPUT[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mMIN_SIZE_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;81m!!python/object/apply:eval[39m[38;5;15m [39m[38;5;15m[[39m[38;5;186m"[39m[38;5;186m[int(x[39m[38;5;15m [39m[38;5;186m*[39m[38;5;15m [39m[38;5;186m0.1[39m[38;5;15m [39m[38;5;186m*[39m[38;5;15m [39m[38;5;186m512)[39m[38;5;15m [39m[38;5;186mfor[39m[38;5;15m [39m[38;5;186mx[39m[38;5;15m [39m[38;5;186min[39m[38;5;15m [39m[38;5;186mrange(5,[39m[38;5;15m [39m[38;5;186m16)][39m[38;5;186m"[39m[38;5;15m][39m
[38;5;197mOUTPUT_DIR[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mdeop/train_log[39m[38;5;186m"[39m
[38;5;197mSOLVER[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mIMS_PER_BATCH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m8[39m
[38;5;15m  [39m[38;5;197mTEST_IMS_PER_BATCH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m8[39m
[38;5;197mTEST[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mEVAL_PERIOD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m5000[39m

[01/13 16:46:49] detectron2.utils.env INFO: Using a generated random seed 49154651
[01/13 16:46:49] mask_former.modeling.clip_adapter INFO: Prompt Learner training params: ['prefix_prompt']
[01/13 16:47:27] detectron2 INFO: Rank of current process: 1. World size: 4
[01/13 16:47:33] detectron2 INFO: Environment info:
-------------------------------  --------------------------------------------------------------------------------------------------
sys.platform                     linux
Python                           3.10.11 (main, May 16 2023, 00:28:57) [GCC 11.2.0]
numpy                            1.26.2
detectron2                       0.6 @/homes/55/anjun/.local/lib/python3.10/site-packages/detectron2
Compiler                         GCC 8.3
CUDA compiler                    CUDA 12.1
detectron2 arch flags            8.6
DETECTRON2_ENV_MODULE            <not set>
PyTorch                          2.1.1+cu121 @/scratch/local/ssd/anjun/anaconda3/envs/ldm/lib/python3.10/site-packages/torch
PyTorch debug build              False
torch._C._GLIBCXX_USE_CXX11_ABI  False
GPU available                    Yes
GPU 0,1,2,3                      NVIDIA A40 (arch=8.6)
Driver version                   530.30.02
CUDA_HOME                        /usr/local/cuda-12.1/
Pillow                           9.5.0
torchvision                      0.16.1+cu121 @/scratch/local/ssd/anjun/anaconda3/envs/ldm/lib/python3.10/site-packages/torchvision
torchvision arch flags           5.0, 6.0, 7.0, 7.5, 8.0, 8.6, 9.0
fvcore                           0.1.5.post20221221
iopath                           0.1.9
cv2                              4.5.4-dev
-------------------------------  --------------------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v3.1.1 (Git Hash 64f6bcbcbab628e96f33a62c3e975f8535a7bde4)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 12.1
  - NVCC architecture flags: -gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.9.2
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=12.1, CUDNN_VERSION=8.9.2, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=old-style-cast -Wno-invalid-partial-specialization -Wno-unused-private-field -Wno-aligned-allocation-unavailable -Wno-missing-braces -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.1.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[01/13 16:47:33] detectron2 INFO: Command line arguments: Namespace(config_file='configs/coco-stuff-164k-156/zero_shot_maskformer_R101c_bs32_60k_proposalmask_featupsample_img512.yaml', resume=True, eval_only=False, num_gpus=4, num_machines=1, machine_rank=0, dist_url='tcp://0.0.0.0:12237', opts=['MODEL.CLIP_ADAPTER.CLIP_ENSEMBLE', 'False', 'MODEL.CLIP_ADAPTER.CLIP_ENSEMBLE_WEIGHT', '-1.0', 'SOLVER.TEST_IMS_PER_BATCH', '1', 'OUTPUT_DIR', 'deop/train_log', 'MODEL.CLIP_ADAPTER.PROMPT_LEARNER', 'learnable', 'SOLVER.IMS_PER_BATCH', '16', 'SOLVER.BASE_LR', '0.00005', 'SOLVER.CHECKPOINT_PERIOD', '1000', 'MODEL.CLIP_ADAPTER.CLIP_MODEL_NAME', 'ViT-B/16', 'MODEL.WEIGHTS', 'pretrained_models/deop_model_final.pth', 'MODEL.META_ARCHITECTURE', 'ZeroShotClipfeatUpsample_addnorm_vit16Query2D_maskvit', 'MODEL.NUM_DECODER_LAYER', '1', 'ORACLE', 'False', 'INPUT.MIN_SIZE_TEST', '512', 'TEST.EVAL_PERIOD', '1000', 'INPUT.SIZE_DIVISIBILITY', '512', 'MODEL.MASK_FORMER.DECODER_DICE_WEIGHT', '0.8', 'MODEL.MASK_FORMER.DECODER_CE_WEIGHT', '2.0', 'MODEL.CLIP_ADAPTER.LEARN_POSITION', 'True', 'MODEL.CLIP_ADAPTER.POSITION_LAYERS', '[1,2,3,4,5,6,7,8,9,10,11]', 'MODEL.CLIP_ADAPTER.LAYERMASKVIT', '[11]', 'MODEL.CLIP_ADAPTER.END2ENDTRAIN', 'False', 'MODEL.CLIP_ADAPTER.PS_SHORTCUT', '1.0', 'DATASETS.TRAIN', "('coco_2017_train_stuff_all_sem_seg',)", 'DATASETS.TEST', "('context_59_test_sem_seg',)", 'MODEL.SEM_SEG_HEAD.NUM_CLASSES', '171', 'MODEL.MASK_FORMER.LOSS_NUM_CLASS', '171'])
[01/13 16:47:33] detectron2 INFO: Contents of args.config_file=configs/coco-stuff-164k-156/zero_shot_maskformer_R101c_bs32_60k_proposalmask_featupsample_img512.yaml:
[38;5;197m_BASE_[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mzero_shot_maskformer_R50_bs32_60k.yaml[39m
[38;5;197mMODEL[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mSEM_SEG_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mNUM_CLASSES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m171[39m
[38;5;15m  [39m[38;5;197mMETA_ARCHITECTURE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mZeroShotMaskFormerClipfeatUpsample[39m[38;5;186m"[39m
[38;5;15m  [39m[38;5;197mBACKBONE[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mbuild_resnet_deeplab_backbone[39m[38;5;186m"[39m
[38;5;15m  [39m
[38;5;15m  [39m[38;5;197mWEIGHTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mdetectron2://DeepLab/R-103.pkl[39m[38;5;186m"[39m
[38;5;15m  [39m[38;5;197mRESNETS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mDEPTH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m101[39m
[38;5;15m    [39m[38;5;197mSTEM_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mdeeplab[39m[38;5;186m"[39m
[38;5;15m    [39m[38;5;197mSTEM_OUT_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m128[39m
[38;5;15m    [39m[38;5;197mSTRIDE_IN_1X1[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mFalse[39m
[38;5;15m    [39m[38;5;197mOUT_FEATURES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;186m"[39m[38;5;186mres2[39m[38;5;186m"[39m[38;5;15m,[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mres3[39m[38;5;186m"[39m[38;5;15m,[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mres4[39m[38;5;186m"[39m[38;5;15m,[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mres5[39m[38;5;186m"[39m[38;5;15m][39m
[38;5;15m    [39m[38;5;242m# NORM: "SyncBN"[39m
[38;5;15m    [39m[38;5;197mRES5_MULTI_GRID[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m1[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15m2[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15m4[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;197mCLIP_ADAPTER[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCLIP_ENSEMBLE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mTrue[39m
[38;5;15m    [39m[38;5;197mCLIP_ENSEMBLE_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.8[39m
[38;5;197mINPUT[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mMIN_SIZE_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;81m!!python/object/apply:eval[39m[38;5;15m [39m[38;5;15m[[39m[38;5;186m"[39m[38;5;186m[int(x[39m[38;5;15m [39m[38;5;186m*[39m[38;5;15m [39m[38;5;186m0.1[39m[38;5;15m [39m[38;5;186m*[39m[38;5;15m [39m[38;5;186m512)[39m[38;5;15m [39m[38;5;186mfor[39m[38;5;15m [39m[38;5;186mx[39m[38;5;15m [39m[38;5;186min[39m[38;5;15m [39m[38;5;186mrange(5,[39m[38;5;15m [39m[38;5;186m16)][39m[38;5;186m"[39m[38;5;15m][39m
[38;5;197mOUTPUT_DIR[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mdeop/train_log[39m[38;5;186m"[39m
[38;5;197mSOLVER[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mIMS_PER_BATCH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m8[39m
[38;5;15m  [39m[38;5;197mTEST_IMS_PER_BATCH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m8[39m
[38;5;197mTEST[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mEVAL_PERIOD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m5000[39m

[01/13 16:47:33] detectron2.utils.env INFO: Using a generated random seed 33555332
[01/13 16:47:34] mask_former.modeling.clip_adapter INFO: Prompt Learner training params: ['prefix_prompt']
[01/13 16:47:38] detectron2.engine.defaults INFO: Model:
ZeroShotClipfeatUpsample_addnorm_vit16Query2D_maskvit(
  (backbone): ResNet(
    (stem): DeepLabStem(
      (conv1): Conv2d(
        3, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
      )
      (conv3): Conv2d(
        64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
      )
    )
    (res2): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv1): Conv2d(
          128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
      )
    )
    (res3): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv1): Conv2d(
          256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
      (3): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
    )
    (res4): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
        (conv1): Conv2d(
          512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (3): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (4): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (5): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (6): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (7): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (8): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (9): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (10): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (11): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (12): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (13): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (14): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (15): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (16): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (17): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (18): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (19): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (20): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (21): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (22): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
    )
    (res5): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
        (conv1): Conv2d(
          1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
      )
    )
  )
  (sem_seg_head): ZeroShotMaskFormerHead(
    (pixel_decoder): BasePixelDecoder(
      (adapter_1): Conv2d(
        256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (layer_1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (adapter_2): Conv2d(
        512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (layer_2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (adapter_3): Conv2d(
        1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (layer_3): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (layer_4): Conv2d(
        2048, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (mask_features): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
    (predictor): ZeroShotTransformerPredictor(
      (pe_layer): PositionEmbeddingSine()
      (transformer): Transformer(
        (encoder): TransformerEncoder(
          (layers): ModuleList()
        )
        (decoder): TransformerDecoder(
          (layers): ModuleList(
            (0-5): 6 x TransformerDecoderLayer(
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (multihead_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (linear1): Linear(in_features=256, out_features=2048, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (linear2): Linear(in_features=2048, out_features=256, bias=True)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (dropout1): Dropout(p=0.1, inplace=False)
              (dropout2): Dropout(p=0.1, inplace=False)
              (dropout3): Dropout(p=0.1, inplace=False)
            )
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (query_embed): Embedding(100, 256)
      (input_proj): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
      (mask_embed): MLP(
        (layers): ModuleList(
          (0-2): 3 x Linear(in_features=256, out_features=256, bias=True)
        )
      )
      (class_embed): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=1024, bias=True)
          (1): Linear(in_features=1024, out_features=512, bias=True)
        )
      )
    )
  )
  (criterion): SetCriterion(
    (matcher): Matcher HungarianMatcher
        cost_class: 1
        cost_mask: 20.0
        cost_dice: 1.0
  )
  (clip_adapter): MaskFormerClipFeatureAdapter(
    (clip_model): CLIP(
      (visual): VisionTransformerLearnPosition(
        (conv1): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False)
        (ln_pre): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (transformer): TransformerVitPosition(
          (resblocks): Sequential(
            (0): ResidualAttentionBlockVitPosition(
              (attn): MultiheadAttentionLoRA(
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (1): ResidualAttentionBlockVitPosition(
              (attn): MultiheadAttentionLoRA(
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (2): ResidualAttentionBlockVitPosition(
              (attn): MultiheadAttentionLoRA(
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (3): ResidualAttentionBlockVitPosition(
              (attn): MultiheadAttentionLoRA(
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (4): ResidualAttentionBlockVitPosition(
              (attn): MultiheadAttentionLoRA(
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (5): ResidualAttentionBlockVitPosition(
              (attn): MultiheadAttentionLoRA(
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (6): ResidualAttentionBlockVitPosition(
              (attn): MultiheadAttentionLoRA(
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (7): ResidualAttentionBlockVitPosition(
              (attn): MultiheadAttentionLoRA(
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (8): ResidualAttentionBlockVitPosition(
              (attn): MultiheadAttentionLoRA(
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (9): ResidualAttentionBlockVitPosition(
              (attn): MultiheadAttentionLoRA(
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (10): ResidualAttentionBlockVitPosition(
              (attn): MultiheadAttentionLoRA(
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (11): ResidualAttentionBlockVitPosition(
              (attn): MultiheadAttentionVit(
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
        (ln_post): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (transformer): Transformer(
        (resblocks): Sequential(
          (0): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (1): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (2): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (3): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (4): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (5): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (6): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (7): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (8): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (9): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (10): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (11): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (token_embedding): Embedding(49408, 512)
      (ln_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (prompt_learner): LearnablePromptExtractor(
      prefix_prompt:16,suffix_prompt:0,dimension:512
      [Normal_Init(mu=0,std=0.02)]
    )
  )
  (decoder): TransformerDecoderLinear(
    (layers): ModuleList(
      (0): TransformerDecoderLayerLinear(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
        )
        (multihead_attn_my): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
        )
        (linear1): Linear(in_features=512, out_features=1024, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=1024, out_features=512, bias=True)
        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
    )
    (outlayer): TransformerDecoderOutLayerLinear(
      (self_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
      )
      (multihead_attn_my): MultiHeadAttentionMy(
        (dropout_F): Dropout(p=0.1, inplace=False)
      )
      (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (dropout1): Dropout(p=0.1, inplace=False)
      (dropout2): Dropout(p=0.1, inplace=False)
      (dropout3): Dropout(p=0.1, inplace=False)
    )
    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  )
  (pe_layer): PositionEmbeddingSine()
  (query_embedding): Embedding(100, 512)
)
[01/13 16:47:38] mask_former.data.dataset_mappers.mask_former_semantic_dataset_mapper INFO: [MaskFormerSemanticDatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=(256, 307, 358, 409, 460, 512, 563, 614, 665, 716, 768), max_size=2560, sample_style='choice'), RandomCrop_CategoryAreaConstraint(crop_type='absolute', crop_size=[640, 640], single_category_max_area=1.0, ignored_category=255), <detectron2.projects.point_rend.color_augmentation.ColorAugSSDTransform object at 0x7f97201ab160>, RandomFlip()]
[01/13 16:47:40] detectron2.data.datasets.coco INFO: Loaded 118287 images with semantic segmentation from /scratch/local/ssd/anjun/datasets/coco-stuff/images/train2017
[01/13 16:47:40] mask_former.data.build INFO: Using training sampler TrainingSampler
[01/13 16:47:40] detectron2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[01/13 16:47:40] detectron2.data.common INFO: Serializing 118287 elements to byte tensors and concatenating them all ...
[01/13 16:47:40] detectron2.data.common INFO: Serialized dataset takes 32.38 MiB
[01/13 16:47:40] detectron2.data.build INFO: Making batched data loader with batch_size=4
[01/13 16:47:41] detectron2.checkpoint.detection_checkpoint INFO: [DetectionCheckpointer] Loading from pretrained_models/deop_model_final.pth ...
[01/13 16:47:41] fvcore.common.checkpoint INFO: [Checkpointer] Loading from pretrained_models/deop_model_final.pth ...
[01/13 16:47:41] fvcore.common.checkpoint WARNING: Skip loading parameter 'criterion.empty_weight' to the model due to incompatible shapes: (157,) in the checkpoint but (172,) in the model! You might want to double check if this is expected.
[01/13 16:47:41] fvcore.common.checkpoint WARNING: Some model parameters or buffers are not found in the checkpoint:
[34mclip_adapter.clip_model.token_embedding.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.0.mlp.c_fc.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.0.mlp.c_proj.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.1.mlp.c_fc.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.1.mlp.c_proj.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.10.learned_position[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.10.mlp.c_fc.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.10.mlp.c_proj.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.11.learned_position[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.11.mlp.c_fc.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.11.mlp.c_proj.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.2.mlp.c_fc.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.2.mlp.c_proj.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.3.mlp.c_fc.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.3.mlp.c_proj.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.4.mlp.c_fc.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.4.mlp.c_proj.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.5.mlp.c_fc.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.5.mlp.c_proj.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.6.learned_position[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.6.mlp.c_fc.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.6.mlp.c_proj.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.7.learned_position[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.7.mlp.c_fc.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.7.mlp.c_proj.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.8.learned_position[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.8.mlp.c_fc.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.8.mlp.c_proj.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.9.learned_position[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.9.mlp.c_fc.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.9.mlp.c_proj.{lora_A, lora_B}[0m
[34mcriterion.empty_weight[0m
[01/13 16:47:41] detectron2.engine.train_loop INFO: Starting training from iteration 0
[01/13 16:47:59] detectron2.engine.train_loop ERROR: Exception during training:
Traceback (most recent call last):
  File "/homes/55/anjun/.local/lib/python3.10/site-packages/detectron2/engine/train_loop.py", line 155, in train
    self.run_step()
  File "/scratch/local/ssd/anjun/ovseg/DeOP/train_net.py", line 361, in run_step
    loss_dict = self.model(data)
  File "/scratch/local/ssd/anjun/anaconda3/envs/ldm/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/local/ssd/anjun/anaconda3/envs/ldm/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/local/ssd/anjun/anaconda3/envs/ldm/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1515, in forward
    inputs, kwargs = self._pre_forward(*inputs, **kwargs)
  File "/scratch/local/ssd/anjun/anaconda3/envs/ldm/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1409, in _pre_forward
    if torch.is_grad_enabled() and self.reducer._rebuild_buckets():
RuntimeError: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by 
making sure all `forward` function outputs participate in calculating loss. 
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameter indices which did not receive grad for rank 1: 61 62
 In addition, you can set the environment variable TORCH_DISTRIBUTED_DEBUG to either INFO or DETAIL to print out information about which particular parameters did not receive gradient on this rank as part of this error
[01/13 16:47:59] detectron2.engine.hooks INFO: Total training time: 0:00:00 (0:00:00 on hooks)
[01/13 16:49:14] detectron2 INFO: Rank of current process: 1. World size: 4
[01/13 16:49:21] detectron2 INFO: Environment info:
-------------------------------  --------------------------------------------------------------------------------------------------
sys.platform                     linux
Python                           3.10.11 (main, May 16 2023, 00:28:57) [GCC 11.2.0]
numpy                            1.26.2
detectron2                       0.6 @/homes/55/anjun/.local/lib/python3.10/site-packages/detectron2
Compiler                         GCC 8.3
CUDA compiler                    CUDA 12.1
detectron2 arch flags            8.6
DETECTRON2_ENV_MODULE            <not set>
PyTorch                          2.1.1+cu121 @/scratch/local/ssd/anjun/anaconda3/envs/ldm/lib/python3.10/site-packages/torch
PyTorch debug build              False
torch._C._GLIBCXX_USE_CXX11_ABI  False
GPU available                    Yes
GPU 0,1,2,3                      NVIDIA A40 (arch=8.6)
Driver version                   530.30.02
CUDA_HOME                        /usr/local/cuda-12.1/
Pillow                           9.5.0
torchvision                      0.16.1+cu121 @/scratch/local/ssd/anjun/anaconda3/envs/ldm/lib/python3.10/site-packages/torchvision
torchvision arch flags           5.0, 6.0, 7.0, 7.5, 8.0, 8.6, 9.0
fvcore                           0.1.5.post20221221
iopath                           0.1.9
cv2                              4.5.4-dev
-------------------------------  --------------------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v3.1.1 (Git Hash 64f6bcbcbab628e96f33a62c3e975f8535a7bde4)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 12.1
  - NVCC architecture flags: -gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.9.2
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=12.1, CUDNN_VERSION=8.9.2, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=old-style-cast -Wno-invalid-partial-specialization -Wno-unused-private-field -Wno-aligned-allocation-unavailable -Wno-missing-braces -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.1.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[01/13 16:49:21] detectron2 INFO: Command line arguments: Namespace(config_file='configs/coco-stuff-164k-156/zero_shot_maskformer_R101c_bs32_60k_proposalmask_featupsample_img512.yaml', resume=True, eval_only=False, num_gpus=4, num_machines=1, machine_rank=0, dist_url='tcp://0.0.0.0:12237', opts=['MODEL.CLIP_ADAPTER.CLIP_ENSEMBLE', 'False', 'MODEL.CLIP_ADAPTER.CLIP_ENSEMBLE_WEIGHT', '-1.0', 'SOLVER.TEST_IMS_PER_BATCH', '1', 'OUTPUT_DIR', 'deop/train_log', 'MODEL.CLIP_ADAPTER.PROMPT_LEARNER', 'learnable', 'SOLVER.IMS_PER_BATCH', '16', 'SOLVER.BASE_LR', '0.00005', 'SOLVER.CHECKPOINT_PERIOD', '1000', 'MODEL.CLIP_ADAPTER.CLIP_MODEL_NAME', 'ViT-B/16', 'MODEL.WEIGHTS', 'pretrained_models/deop_model_final.pth', 'MODEL.META_ARCHITECTURE', 'ZeroShotClipfeatUpsample_addnorm_vit16Query2D_maskvit', 'MODEL.NUM_DECODER_LAYER', '1', 'ORACLE', 'False', 'INPUT.MIN_SIZE_TEST', '512', 'TEST.EVAL_PERIOD', '1000', 'INPUT.SIZE_DIVISIBILITY', '512', 'MODEL.MASK_FORMER.DECODER_DICE_WEIGHT', '0.8', 'MODEL.MASK_FORMER.DECODER_CE_WEIGHT', '2.0', 'MODEL.CLIP_ADAPTER.LEARN_POSITION', 'True', 'MODEL.CLIP_ADAPTER.POSITION_LAYERS', '[1,2,3,4,5,6,7,8,9,10,11]', 'MODEL.CLIP_ADAPTER.LAYERMASKVIT', '[11]', 'MODEL.CLIP_ADAPTER.END2ENDTRAIN', 'False', 'MODEL.CLIP_ADAPTER.PS_SHORTCUT', '1.0', 'DATASETS.TRAIN', "('coco_2017_train_stuff_all_sem_seg',)", 'DATASETS.TEST', "('context_59_test_sem_seg',)", 'MODEL.SEM_SEG_HEAD.NUM_CLASSES', '171', 'MODEL.MASK_FORMER.LOSS_NUM_CLASS', '171'])
[01/13 16:49:21] detectron2 INFO: Contents of args.config_file=configs/coco-stuff-164k-156/zero_shot_maskformer_R101c_bs32_60k_proposalmask_featupsample_img512.yaml:
[38;5;197m_BASE_[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mzero_shot_maskformer_R50_bs32_60k.yaml[39m
[38;5;197mMODEL[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mSEM_SEG_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mNUM_CLASSES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m171[39m
[38;5;15m  [39m[38;5;197mMETA_ARCHITECTURE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mZeroShotMaskFormerClipfeatUpsample[39m[38;5;186m"[39m
[38;5;15m  [39m[38;5;197mBACKBONE[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mbuild_resnet_deeplab_backbone[39m[38;5;186m"[39m
[38;5;15m  [39m
[38;5;15m  [39m[38;5;197mWEIGHTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mdetectron2://DeepLab/R-103.pkl[39m[38;5;186m"[39m
[38;5;15m  [39m[38;5;197mRESNETS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mDEPTH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m101[39m
[38;5;15m    [39m[38;5;197mSTEM_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mdeeplab[39m[38;5;186m"[39m
[38;5;15m    [39m[38;5;197mSTEM_OUT_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m128[39m
[38;5;15m    [39m[38;5;197mSTRIDE_IN_1X1[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mFalse[39m
[38;5;15m    [39m[38;5;197mOUT_FEATURES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;186m"[39m[38;5;186mres2[39m[38;5;186m"[39m[38;5;15m,[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mres3[39m[38;5;186m"[39m[38;5;15m,[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mres4[39m[38;5;186m"[39m[38;5;15m,[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mres5[39m[38;5;186m"[39m[38;5;15m][39m
[38;5;15m    [39m[38;5;242m# NORM: "SyncBN"[39m
[38;5;15m    [39m[38;5;197mRES5_MULTI_GRID[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m1[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15m2[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15m4[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;197mCLIP_ADAPTER[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCLIP_ENSEMBLE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mTrue[39m
[38;5;15m    [39m[38;5;197mCLIP_ENSEMBLE_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.8[39m
[38;5;197mINPUT[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mMIN_SIZE_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;81m!!python/object/apply:eval[39m[38;5;15m [39m[38;5;15m[[39m[38;5;186m"[39m[38;5;186m[int(x[39m[38;5;15m [39m[38;5;186m*[39m[38;5;15m [39m[38;5;186m0.1[39m[38;5;15m [39m[38;5;186m*[39m[38;5;15m [39m[38;5;186m512)[39m[38;5;15m [39m[38;5;186mfor[39m[38;5;15m [39m[38;5;186mx[39m[38;5;15m [39m[38;5;186min[39m[38;5;15m [39m[38;5;186mrange(5,[39m[38;5;15m [39m[38;5;186m16)][39m[38;5;186m"[39m[38;5;15m][39m
[38;5;197mOUTPUT_DIR[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mdeop/train_log[39m[38;5;186m"[39m
[38;5;197mSOLVER[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mIMS_PER_BATCH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m8[39m
[38;5;15m  [39m[38;5;197mTEST_IMS_PER_BATCH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m8[39m
[38;5;197mTEST[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mEVAL_PERIOD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m5000[39m

[01/13 16:49:21] detectron2.utils.env INFO: Using a generated random seed 21356460
[01/13 16:49:21] mask_former.modeling.clip_adapter INFO: Prompt Learner training params: ['prefix_prompt']
[01/13 16:49:26] detectron2.engine.defaults INFO: Model:
ZeroShotClipfeatUpsample_addnorm_vit16Query2D_maskvit(
  (backbone): ResNet(
    (stem): DeepLabStem(
      (conv1): Conv2d(
        3, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
      )
      (conv3): Conv2d(
        64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
      )
    )
    (res2): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv1): Conv2d(
          128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
      )
    )
    (res3): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv1): Conv2d(
          256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
      (3): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
    )
    (res4): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
        (conv1): Conv2d(
          512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (3): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (4): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (5): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (6): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (7): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (8): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (9): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (10): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (11): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (12): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (13): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (14): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (15): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (16): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (17): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (18): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (19): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (20): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (21): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (22): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
    )
    (res5): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
        (conv1): Conv2d(
          1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
      )
    )
  )
  (sem_seg_head): ZeroShotMaskFormerHead(
    (pixel_decoder): BasePixelDecoder(
      (adapter_1): Conv2d(
        256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (layer_1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (adapter_2): Conv2d(
        512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (layer_2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (adapter_3): Conv2d(
        1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (layer_3): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (layer_4): Conv2d(
        2048, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (mask_features): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
    (predictor): ZeroShotTransformerPredictor(
      (pe_layer): PositionEmbeddingSine()
      (transformer): Transformer(
        (encoder): TransformerEncoder(
          (layers): ModuleList()
        )
        (decoder): TransformerDecoder(
          (layers): ModuleList(
            (0-5): 6 x TransformerDecoderLayer(
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (multihead_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (linear1): Linear(in_features=256, out_features=2048, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (linear2): Linear(in_features=2048, out_features=256, bias=True)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (dropout1): Dropout(p=0.1, inplace=False)
              (dropout2): Dropout(p=0.1, inplace=False)
              (dropout3): Dropout(p=0.1, inplace=False)
            )
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (query_embed): Embedding(100, 256)
      (input_proj): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
      (mask_embed): MLP(
        (layers): ModuleList(
          (0-2): 3 x Linear(in_features=256, out_features=256, bias=True)
        )
      )
      (class_embed): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=1024, bias=True)
          (1): Linear(in_features=1024, out_features=512, bias=True)
        )
      )
    )
  )
  (criterion): SetCriterion(
    (matcher): Matcher HungarianMatcher
        cost_class: 1
        cost_mask: 20.0
        cost_dice: 1.0
  )
  (clip_adapter): MaskFormerClipFeatureAdapter(
    (clip_model): CLIP(
      (visual): VisionTransformerLearnPosition(
        (conv1): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False)
        (ln_pre): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (transformer): TransformerVitPosition(
          (resblocks): Sequential(
            (0): ResidualAttentionBlockVitPosition(
              (attn): MultiheadAttentionLoRA(
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (1): ResidualAttentionBlockVitPosition(
              (attn): MultiheadAttentionLoRA(
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (2): ResidualAttentionBlockVitPosition(
              (attn): MultiheadAttentionLoRA(
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (3): ResidualAttentionBlockVitPosition(
              (attn): MultiheadAttentionLoRA(
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (4): ResidualAttentionBlockVitPosition(
              (attn): MultiheadAttentionLoRA(
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (5): ResidualAttentionBlockVitPosition(
              (attn): MultiheadAttentionLoRA(
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (6): ResidualAttentionBlockVitPosition(
              (attn): MultiheadAttentionLoRA(
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (7): ResidualAttentionBlockVitPosition(
              (attn): MultiheadAttentionLoRA(
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (8): ResidualAttentionBlockVitPosition(
              (attn): MultiheadAttentionLoRA(
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (9): ResidualAttentionBlockVitPosition(
              (attn): MultiheadAttentionLoRA(
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (10): ResidualAttentionBlockVitPosition(
              (attn): MultiheadAttentionLoRA(
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (11): ResidualAttentionBlockVitPosition(
              (attn): MultiheadAttentionVit(
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
        (ln_post): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (transformer): Transformer(
        (resblocks): Sequential(
          (0): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (1): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (2): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (3): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (4): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (5): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (6): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (7): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (8): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (9): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (10): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (11): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (token_embedding): Embedding(49408, 512)
      (ln_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (prompt_learner): LearnablePromptExtractor(
      prefix_prompt:16,suffix_prompt:0,dimension:512
      [Normal_Init(mu=0,std=0.02)]
    )
  )
  (decoder): TransformerDecoderLinear(
    (layers): ModuleList(
      (0): TransformerDecoderLayerLinear(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
        )
        (multihead_attn_my): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
        )
        (linear1): Linear(in_features=512, out_features=1024, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=1024, out_features=512, bias=True)
        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
    )
    (outlayer): TransformerDecoderOutLayerLinear(
      (self_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
      )
      (multihead_attn_my): MultiHeadAttentionMy(
        (dropout_F): Dropout(p=0.1, inplace=False)
      )
      (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (dropout1): Dropout(p=0.1, inplace=False)
      (dropout2): Dropout(p=0.1, inplace=False)
      (dropout3): Dropout(p=0.1, inplace=False)
    )
    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  )
  (pe_layer): PositionEmbeddingSine()
  (query_embedding): Embedding(100, 512)
)
[01/13 16:49:26] mask_former.data.dataset_mappers.mask_former_semantic_dataset_mapper INFO: [MaskFormerSemanticDatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=(256, 307, 358, 409, 460, 512, 563, 614, 665, 716, 768), max_size=2560, sample_style='choice'), RandomCrop_CategoryAreaConstraint(crop_type='absolute', crop_size=[640, 640], single_category_max_area=1.0, ignored_category=255), <detectron2.projects.point_rend.color_augmentation.ColorAugSSDTransform object at 0x7f213c347160>, RandomFlip()]
[01/13 16:49:28] detectron2.data.datasets.coco INFO: Loaded 118287 images with semantic segmentation from /scratch/local/ssd/anjun/datasets/coco-stuff/images/train2017
[01/13 16:49:28] mask_former.data.build INFO: Using training sampler TrainingSampler
[01/13 16:49:28] detectron2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[01/13 16:49:28] detectron2.data.common INFO: Serializing 118287 elements to byte tensors and concatenating them all ...
[01/13 16:49:29] detectron2.data.common INFO: Serialized dataset takes 32.38 MiB
[01/13 16:49:29] detectron2.data.build INFO: Making batched data loader with batch_size=4
[01/13 16:49:30] detectron2.checkpoint.detection_checkpoint INFO: [DetectionCheckpointer] Loading from pretrained_models/deop_model_final.pth ...
[01/13 16:49:30] fvcore.common.checkpoint INFO: [Checkpointer] Loading from pretrained_models/deop_model_final.pth ...
[01/13 16:49:30] fvcore.common.checkpoint WARNING: Skip loading parameter 'criterion.empty_weight' to the model due to incompatible shapes: (157,) in the checkpoint but (172,) in the model! You might want to double check if this is expected.
[01/13 16:49:30] fvcore.common.checkpoint WARNING: Some model parameters or buffers are not found in the checkpoint:
[34mclip_adapter.clip_model.visual.transformer.resblocks.0.mlp.c_fc.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.0.mlp.c_proj.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.1.mlp.c_fc.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.1.mlp.c_proj.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.10.learned_position[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.10.mlp.c_fc.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.10.mlp.c_proj.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.11.learned_position[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.11.mlp.c_fc.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.11.mlp.c_proj.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.2.mlp.c_fc.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.2.mlp.c_proj.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.3.mlp.c_fc.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.3.mlp.c_proj.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.4.mlp.c_fc.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.4.mlp.c_proj.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.5.mlp.c_fc.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.5.mlp.c_proj.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.6.learned_position[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.6.mlp.c_fc.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.6.mlp.c_proj.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.7.learned_position[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.7.mlp.c_fc.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.7.mlp.c_proj.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.8.learned_position[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.8.mlp.c_fc.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.8.mlp.c_proj.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.9.learned_position[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.9.mlp.c_fc.{lora_A, lora_B}[0m
[34mclip_adapter.clip_model.visual.transformer.resblocks.9.mlp.c_proj.{lora_A, lora_B}[0m
[34mcriterion.empty_weight[0m
[01/13 16:49:30] detectron2.engine.train_loop INFO: Starting training from iteration 0
[01/13 17:08:38] mask_former.data.dataset_mappers.mask_former_semantic_dataset_mapper INFO: [MaskFormerSemanticDatasetMapper] Augmentations used in inference: []
[01/13 17:08:38] detectron2.data.datasets.coco WARNING: Directory /scratch/local/ssd/anjun/datasets/VOCdevkit/VOC2010/JPEGImages and /scratch/local/ssd/anjun/datasets/VOCdevkit/VOC2010/annotations_detectron2/pc59_val has 11321 and 5104 files, respectively.
[01/13 17:08:38] detectron2.data.datasets.coco WARNING: Will use their intersection of 5104 files.
[01/13 17:08:38] detectron2.data.datasets.coco INFO: Loaded 5104 images with semantic segmentation from /scratch/local/ssd/anjun/datasets/VOCdevkit/VOC2010/JPEGImages
[01/13 17:08:38] detectron2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[01/13 17:08:38] detectron2.data.common INFO: Serializing 5104 elements to byte tensors and concatenating them all ...
[01/13 17:08:38] detectron2.data.common INFO: Serialized dataset takes 1.37 MiB
[01/13 17:08:38] detectron2.data.datasets.coco WARNING: Directory /scratch/local/ssd/anjun/datasets/VOCdevkit/VOC2010/JPEGImages and /scratch/local/ssd/anjun/datasets/VOCdevkit/VOC2010/annotations_detectron2/pc59_val has 11321 and 5104 files, respectively.
[01/13 17:08:38] detectron2.data.datasets.coco WARNING: Will use their intersection of 5104 files.
[01/13 17:08:38] detectron2.data.datasets.coco INFO: Loaded 5104 images with semantic segmentation from /scratch/local/ssd/anjun/datasets/VOCdevkit/VOC2010/JPEGImages
[01/13 17:08:38] detectron2.evaluation.evaluator INFO: Start inference on 1276 batches
[01/13 17:08:46] detectron2.evaluation.evaluator INFO: Inference done 11/1276. Dataloading: 0.0008 s/iter. Inference: 0.1183 s/iter. Eval: 0.0058 s/iter. Total: 0.1249 s/iter. ETA=0:02:37
[01/13 17:08:51] detectron2.evaluation.evaluator INFO: Inference done 47/1276. Dataloading: 0.0013 s/iter. Inference: 0.1314 s/iter. Eval: 0.0061 s/iter. Total: 0.1389 s/iter. ETA=0:02:50
[01/13 17:08:56] detectron2.evaluation.evaluator INFO: Inference done 84/1276. Dataloading: 0.0014 s/iter. Inference: 0.1303 s/iter. Eval: 0.0062 s/iter. Total: 0.1380 s/iter. ETA=0:02:44
[01/13 17:09:01] detectron2.evaluation.evaluator INFO: Inference done 123/1276. Dataloading: 0.0014 s/iter. Inference: 0.1278 s/iter. Eval: 0.0061 s/iter. Total: 0.1353 s/iter. ETA=0:02:36
[01/13 17:09:06] detectron2.evaluation.evaluator INFO: Inference done 159/1276. Dataloading: 0.0014 s/iter. Inference: 0.1288 s/iter. Eval: 0.0060 s/iter. Total: 0.1363 s/iter. ETA=0:02:32
[01/13 17:09:11] detectron2.evaluation.evaluator INFO: Inference done 196/1276. Dataloading: 0.0015 s/iter. Inference: 0.1287 s/iter. Eval: 0.0060 s/iter. Total: 0.1362 s/iter. ETA=0:02:27
[01/13 17:09:16] detectron2.evaluation.evaluator INFO: Inference done 232/1276. Dataloading: 0.0015 s/iter. Inference: 0.1292 s/iter. Eval: 0.0061 s/iter. Total: 0.1368 s/iter. ETA=0:02:22
[01/13 17:09:21] detectron2.evaluation.evaluator INFO: Inference done 269/1276. Dataloading: 0.0015 s/iter. Inference: 0.1292 s/iter. Eval: 0.0063 s/iter. Total: 0.1369 s/iter. ETA=0:02:17
[01/13 17:09:26] detectron2.evaluation.evaluator INFO: Inference done 307/1276. Dataloading: 0.0015 s/iter. Inference: 0.1287 s/iter. Eval: 0.0062 s/iter. Total: 0.1364 s/iter. ETA=0:02:12
[01/13 17:09:31] detectron2.evaluation.evaluator INFO: Inference done 345/1276. Dataloading: 0.0015 s/iter. Inference: 0.1284 s/iter. Eval: 0.0062 s/iter. Total: 0.1361 s/iter. ETA=0:02:06
[01/13 17:09:36] detectron2.evaluation.evaluator INFO: Inference done 382/1276. Dataloading: 0.0015 s/iter. Inference: 0.1284 s/iter. Eval: 0.0062 s/iter. Total: 0.1361 s/iter. ETA=0:02:01
[01/13 17:09:41] detectron2.evaluation.evaluator INFO: Inference done 420/1276. Dataloading: 0.0015 s/iter. Inference: 0.1281 s/iter. Eval: 0.0062 s/iter. Total: 0.1358 s/iter. ETA=0:01:56
[01/13 17:09:47] detectron2.evaluation.evaluator INFO: Inference done 457/1276. Dataloading: 0.0015 s/iter. Inference: 0.1283 s/iter. Eval: 0.0062 s/iter. Total: 0.1359 s/iter. ETA=0:01:51
[01/13 17:09:52] detectron2.evaluation.evaluator INFO: Inference done 495/1276. Dataloading: 0.0015 s/iter. Inference: 0.1282 s/iter. Eval: 0.0062 s/iter. Total: 0.1359 s/iter. ETA=0:01:46
[01/13 17:09:57] detectron2.evaluation.evaluator INFO: Inference done 533/1276. Dataloading: 0.0015 s/iter. Inference: 0.1281 s/iter. Eval: 0.0062 s/iter. Total: 0.1358 s/iter. ETA=0:01:40
[01/13 17:10:02] detectron2.evaluation.evaluator INFO: Inference done 570/1276. Dataloading: 0.0015 s/iter. Inference: 0.1283 s/iter. Eval: 0.0062 s/iter. Total: 0.1360 s/iter. ETA=0:01:35
[01/13 17:10:07] detectron2.evaluation.evaluator INFO: Inference done 608/1276. Dataloading: 0.0015 s/iter. Inference: 0.1281 s/iter. Eval: 0.0062 s/iter. Total: 0.1358 s/iter. ETA=0:01:30
[01/13 17:10:12] detectron2.evaluation.evaluator INFO: Inference done 645/1276. Dataloading: 0.0015 s/iter. Inference: 0.1283 s/iter. Eval: 0.0062 s/iter. Total: 0.1359 s/iter. ETA=0:01:25
[01/13 17:10:17] detectron2.evaluation.evaluator INFO: Inference done 682/1276. Dataloading: 0.0015 s/iter. Inference: 0.1284 s/iter. Eval: 0.0062 s/iter. Total: 0.1361 s/iter. ETA=0:01:20
[01/13 17:10:22] detectron2.evaluation.evaluator INFO: Inference done 718/1276. Dataloading: 0.0015 s/iter. Inference: 0.1286 s/iter. Eval: 0.0062 s/iter. Total: 0.1362 s/iter. ETA=0:01:16
[01/13 17:10:27] detectron2.evaluation.evaluator INFO: Inference done 755/1276. Dataloading: 0.0015 s/iter. Inference: 0.1286 s/iter. Eval: 0.0062 s/iter. Total: 0.1363 s/iter. ETA=0:01:11
[01/13 17:10:32] detectron2.evaluation.evaluator INFO: Inference done 791/1276. Dataloading: 0.0015 s/iter. Inference: 0.1288 s/iter. Eval: 0.0062 s/iter. Total: 0.1364 s/iter. ETA=0:01:06
[01/13 17:10:37] detectron2.evaluation.evaluator INFO: Inference done 827/1276. Dataloading: 0.0015 s/iter. Inference: 0.1289 s/iter. Eval: 0.0062 s/iter. Total: 0.1366 s/iter. ETA=0:01:01
[01/13 17:10:42] detectron2.evaluation.evaluator INFO: Inference done 864/1276. Dataloading: 0.0015 s/iter. Inference: 0.1289 s/iter. Eval: 0.0062 s/iter. Total: 0.1366 s/iter. ETA=0:00:56
[01/13 17:10:48] detectron2.evaluation.evaluator INFO: Inference done 901/1276. Dataloading: 0.0015 s/iter. Inference: 0.1290 s/iter. Eval: 0.0062 s/iter. Total: 0.1367 s/iter. ETA=0:00:51
[01/13 17:10:53] detectron2.evaluation.evaluator INFO: Inference done 937/1276. Dataloading: 0.0015 s/iter. Inference: 0.1291 s/iter. Eval: 0.0062 s/iter. Total: 0.1368 s/iter. ETA=0:00:46
[01/13 17:10:58] detectron2.evaluation.evaluator INFO: Inference done 974/1276. Dataloading: 0.0015 s/iter. Inference: 0.1291 s/iter. Eval: 0.0062 s/iter. Total: 0.1368 s/iter. ETA=0:00:41
[01/13 17:11:03] detectron2.evaluation.evaluator INFO: Inference done 1011/1276. Dataloading: 0.0015 s/iter. Inference: 0.1292 s/iter. Eval: 0.0061 s/iter. Total: 0.1369 s/iter. ETA=0:00:36
[01/13 17:11:08] detectron2.evaluation.evaluator INFO: Inference done 1049/1276. Dataloading: 0.0015 s/iter. Inference: 0.1291 s/iter. Eval: 0.0061 s/iter. Total: 0.1368 s/iter. ETA=0:00:31
[01/13 17:11:13] detectron2.evaluation.evaluator INFO: Inference done 1086/1276. Dataloading: 0.0015 s/iter. Inference: 0.1291 s/iter. Eval: 0.0061 s/iter. Total: 0.1368 s/iter. ETA=0:00:25
[01/13 17:11:18] detectron2.evaluation.evaluator INFO: Inference done 1123/1276. Dataloading: 0.0015 s/iter. Inference: 0.1292 s/iter. Eval: 0.0061 s/iter. Total: 0.1368 s/iter. ETA=0:00:20
[01/13 17:11:23] detectron2.evaluation.evaluator INFO: Inference done 1160/1276. Dataloading: 0.0015 s/iter. Inference: 0.1292 s/iter. Eval: 0.0061 s/iter. Total: 0.1368 s/iter. ETA=0:00:15
[01/13 17:11:28] detectron2.evaluation.evaluator INFO: Inference done 1198/1276. Dataloading: 0.0015 s/iter. Inference: 0.1291 s/iter. Eval: 0.0061 s/iter. Total: 0.1367 s/iter. ETA=0:00:10
[01/13 17:11:33] detectron2.evaluation.evaluator INFO: Inference done 1234/1276. Dataloading: 0.0015 s/iter. Inference: 0.1292 s/iter. Eval: 0.0061 s/iter. Total: 0.1368 s/iter. ETA=0:00:05
[01/13 17:11:38] detectron2.evaluation.evaluator INFO: Inference done 1271/1276. Dataloading: 0.0015 s/iter. Inference: 0.1292 s/iter. Eval: 0.0061 s/iter. Total: 0.1368 s/iter. ETA=0:00:00
[01/13 17:11:40] detectron2.evaluation.evaluator INFO: Total inference time: 0:02:54.629899 (0.137396 s / iter per device, on 4 devices)
[01/13 17:11:40] detectron2.evaluation.evaluator INFO: Total inference pure compute time: 0:02:44 (0.129158 s / iter per device, on 4 devices)
[01/13 17:30:34] mask_former.data.dataset_mappers.mask_former_semantic_dataset_mapper INFO: [MaskFormerSemanticDatasetMapper] Augmentations used in inference: []
[01/13 17:30:34] detectron2.data.datasets.coco WARNING: Directory /scratch/local/ssd/anjun/datasets/VOCdevkit/VOC2010/JPEGImages and /scratch/local/ssd/anjun/datasets/VOCdevkit/VOC2010/annotations_detectron2/pc59_val has 11321 and 5104 files, respectively.
[01/13 17:30:34] detectron2.data.datasets.coco WARNING: Will use their intersection of 5104 files.
[01/13 17:30:34] detectron2.data.datasets.coco INFO: Loaded 5104 images with semantic segmentation from /scratch/local/ssd/anjun/datasets/VOCdevkit/VOC2010/JPEGImages
[01/13 17:30:34] detectron2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[01/13 17:30:34] detectron2.data.common INFO: Serializing 5104 elements to byte tensors and concatenating them all ...
[01/13 17:30:34] detectron2.data.common INFO: Serialized dataset takes 1.37 MiB
[01/13 17:30:34] detectron2.data.datasets.coco WARNING: Directory /scratch/local/ssd/anjun/datasets/VOCdevkit/VOC2010/JPEGImages and /scratch/local/ssd/anjun/datasets/VOCdevkit/VOC2010/annotations_detectron2/pc59_val has 11321 and 5104 files, respectively.
[01/13 17:30:34] detectron2.data.datasets.coco WARNING: Will use their intersection of 5104 files.
[01/13 17:30:34] detectron2.data.datasets.coco INFO: Loaded 5104 images with semantic segmentation from /scratch/local/ssd/anjun/datasets/VOCdevkit/VOC2010/JPEGImages
[01/13 17:30:34] detectron2.evaluation.evaluator INFO: Start inference on 1276 batches
[01/13 17:30:42] detectron2.evaluation.evaluator INFO: Inference done 11/1276. Dataloading: 0.0005 s/iter. Inference: 0.1334 s/iter. Eval: 0.0066 s/iter. Total: 0.1405 s/iter. ETA=0:02:57
[01/13 17:30:47] detectron2.evaluation.evaluator INFO: Inference done 47/1276. Dataloading: 0.0015 s/iter. Inference: 0.1326 s/iter. Eval: 0.0065 s/iter. Total: 0.1406 s/iter. ETA=0:02:52
[01/13 17:30:52] detectron2.evaluation.evaluator INFO: Inference done 84/1276. Dataloading: 0.0016 s/iter. Inference: 0.1318 s/iter. Eval: 0.0063 s/iter. Total: 0.1398 s/iter. ETA=0:02:46
[01/13 17:30:57] detectron2.evaluation.evaluator INFO: Inference done 121/1276. Dataloading: 0.0016 s/iter. Inference: 0.1312 s/iter. Eval: 0.0062 s/iter. Total: 0.1390 s/iter. ETA=0:02:40
[01/13 17:31:02] detectron2.evaluation.evaluator INFO: Inference done 158/1276. Dataloading: 0.0016 s/iter. Inference: 0.1310 s/iter. Eval: 0.0060 s/iter. Total: 0.1386 s/iter. ETA=0:02:35
[01/13 17:31:08] detectron2.evaluation.evaluator INFO: Inference done 195/1276. Dataloading: 0.0017 s/iter. Inference: 0.1310 s/iter. Eval: 0.0060 s/iter. Total: 0.1387 s/iter. ETA=0:02:29
[01/13 17:31:13] detectron2.evaluation.evaluator INFO: Inference done 232/1276. Dataloading: 0.0016 s/iter. Inference: 0.1309 s/iter. Eval: 0.0060 s/iter. Total: 0.1386 s/iter. ETA=0:02:24
[01/13 17:31:18] detectron2.evaluation.evaluator INFO: Inference done 269/1276. Dataloading: 0.0016 s/iter. Inference: 0.1309 s/iter. Eval: 0.0060 s/iter. Total: 0.1385 s/iter. ETA=0:02:19
[01/13 17:31:23] detectron2.evaluation.evaluator INFO: Inference done 305/1276. Dataloading: 0.0016 s/iter. Inference: 0.1310 s/iter. Eval: 0.0060 s/iter. Total: 0.1386 s/iter. ETA=0:02:14
[01/13 17:31:28] detectron2.evaluation.evaluator INFO: Inference done 342/1276. Dataloading: 0.0016 s/iter. Inference: 0.1308 s/iter. Eval: 0.0059 s/iter. Total: 0.1384 s/iter. ETA=0:02:09
[01/13 17:31:33] detectron2.evaluation.evaluator INFO: Inference done 379/1276. Dataloading: 0.0016 s/iter. Inference: 0.1307 s/iter. Eval: 0.0059 s/iter. Total: 0.1383 s/iter. ETA=0:02:04
[01/13 17:31:38] detectron2.evaluation.evaluator INFO: Inference done 417/1276. Dataloading: 0.0016 s/iter. Inference: 0.1304 s/iter. Eval: 0.0059 s/iter. Total: 0.1379 s/iter. ETA=0:01:58
[01/13 17:31:43] detectron2.evaluation.evaluator INFO: Inference done 454/1276. Dataloading: 0.0016 s/iter. Inference: 0.1302 s/iter. Eval: 0.0059 s/iter. Total: 0.1377 s/iter. ETA=0:01:53
[01/13 17:31:48] detectron2.evaluation.evaluator INFO: Inference done 491/1276. Dataloading: 0.0016 s/iter. Inference: 0.1301 s/iter. Eval: 0.0059 s/iter. Total: 0.1376 s/iter. ETA=0:01:48
[01/13 17:31:53] detectron2.evaluation.evaluator INFO: Inference done 528/1276. Dataloading: 0.0016 s/iter. Inference: 0.1301 s/iter. Eval: 0.0059 s/iter. Total: 0.1376 s/iter. ETA=0:01:42
[01/13 17:31:58] detectron2.evaluation.evaluator INFO: Inference done 565/1276. Dataloading: 0.0016 s/iter. Inference: 0.1300 s/iter. Eval: 0.0059 s/iter. Total: 0.1376 s/iter. ETA=0:01:37
[01/13 17:32:03] detectron2.evaluation.evaluator INFO: Inference done 602/1276. Dataloading: 0.0016 s/iter. Inference: 0.1300 s/iter. Eval: 0.0059 s/iter. Total: 0.1376 s/iter. ETA=0:01:32
[01/13 17:32:08] detectron2.evaluation.evaluator INFO: Inference done 638/1276. Dataloading: 0.0016 s/iter. Inference: 0.1302 s/iter. Eval: 0.0060 s/iter. Total: 0.1378 s/iter. ETA=0:01:27
[01/13 17:32:13] detectron2.evaluation.evaluator INFO: Inference done 675/1276. Dataloading: 0.0016 s/iter. Inference: 0.1301 s/iter. Eval: 0.0060 s/iter. Total: 0.1377 s/iter. ETA=0:01:22
[01/13 17:32:18] detectron2.evaluation.evaluator INFO: Inference done 711/1276. Dataloading: 0.0016 s/iter. Inference: 0.1302 s/iter. Eval: 0.0060 s/iter. Total: 0.1378 s/iter. ETA=0:01:17
[01/13 17:32:24] detectron2.evaluation.evaluator INFO: Inference done 747/1276. Dataloading: 0.0016 s/iter. Inference: 0.1303 s/iter. Eval: 0.0060 s/iter. Total: 0.1379 s/iter. ETA=0:01:12
[01/13 17:32:29] detectron2.evaluation.evaluator INFO: Inference done 784/1276. Dataloading: 0.0016 s/iter. Inference: 0.1303 s/iter. Eval: 0.0060 s/iter. Total: 0.1379 s/iter. ETA=0:01:07
[01/13 17:32:34] detectron2.evaluation.evaluator INFO: Inference done 820/1276. Dataloading: 0.0016 s/iter. Inference: 0.1304 s/iter. Eval: 0.0060 s/iter. Total: 0.1380 s/iter. ETA=0:01:02
[01/13 17:32:39] detectron2.evaluation.evaluator INFO: Inference done 857/1276. Dataloading: 0.0016 s/iter. Inference: 0.1304 s/iter. Eval: 0.0060 s/iter. Total: 0.1380 s/iter. ETA=0:00:57
[01/13 17:32:44] detectron2.evaluation.evaluator INFO: Inference done 894/1276. Dataloading: 0.0016 s/iter. Inference: 0.1304 s/iter. Eval: 0.0060 s/iter. Total: 0.1380 s/iter. ETA=0:00:52
[01/13 17:32:49] detectron2.evaluation.evaluator INFO: Inference done 930/1276. Dataloading: 0.0016 s/iter. Inference: 0.1304 s/iter. Eval: 0.0061 s/iter. Total: 0.1381 s/iter. ETA=0:00:47
[01/13 17:32:54] detectron2.evaluation.evaluator INFO: Inference done 966/1276. Dataloading: 0.0016 s/iter. Inference: 0.1305 s/iter. Eval: 0.0061 s/iter. Total: 0.1382 s/iter. ETA=0:00:42
[01/13 17:32:59] detectron2.evaluation.evaluator INFO: Inference done 1002/1276. Dataloading: 0.0016 s/iter. Inference: 0.1306 s/iter. Eval: 0.0061 s/iter. Total: 0.1383 s/iter. ETA=0:00:37
[01/13 17:33:04] detectron2.evaluation.evaluator INFO: Inference done 1039/1276. Dataloading: 0.0016 s/iter. Inference: 0.1306 s/iter. Eval: 0.0061 s/iter. Total: 0.1382 s/iter. ETA=0:00:32
[01/13 17:33:09] detectron2.evaluation.evaluator INFO: Inference done 1077/1276. Dataloading: 0.0016 s/iter. Inference: 0.1304 s/iter. Eval: 0.0061 s/iter. Total: 0.1381 s/iter. ETA=0:00:27
[01/13 17:33:14] detectron2.evaluation.evaluator INFO: Inference done 1114/1276. Dataloading: 0.0016 s/iter. Inference: 0.1304 s/iter. Eval: 0.0061 s/iter. Total: 0.1381 s/iter. ETA=0:00:22
[01/13 17:33:19] detectron2.evaluation.evaluator INFO: Inference done 1151/1276. Dataloading: 0.0016 s/iter. Inference: 0.1304 s/iter. Eval: 0.0061 s/iter. Total: 0.1381 s/iter. ETA=0:00:17
[01/13 17:33:24] detectron2.evaluation.evaluator INFO: Inference done 1188/1276. Dataloading: 0.0016 s/iter. Inference: 0.1304 s/iter. Eval: 0.0061 s/iter. Total: 0.1380 s/iter. ETA=0:00:12
[01/13 17:33:30] detectron2.evaluation.evaluator INFO: Inference done 1225/1276. Dataloading: 0.0016 s/iter. Inference: 0.1304 s/iter. Eval: 0.0061 s/iter. Total: 0.1380 s/iter. ETA=0:00:07
[01/13 17:33:35] detectron2.evaluation.evaluator INFO: Inference done 1261/1276. Dataloading: 0.0016 s/iter. Inference: 0.1304 s/iter. Eval: 0.0061 s/iter. Total: 0.1381 s/iter. ETA=0:00:02
[01/13 17:33:37] detectron2.evaluation.evaluator INFO: Total inference time: 0:02:56.289604 (0.138701 s / iter per device, on 4 devices)
[01/13 17:33:37] detectron2.evaluation.evaluator INFO: Total inference pure compute time: 0:02:45 (0.130369 s / iter per device, on 4 devices)
[01/13 17:52:33] mask_former.data.dataset_mappers.mask_former_semantic_dataset_mapper INFO: [MaskFormerSemanticDatasetMapper] Augmentations used in inference: []
[01/13 17:52:33] detectron2.data.datasets.coco WARNING: Directory /scratch/local/ssd/anjun/datasets/VOCdevkit/VOC2010/JPEGImages and /scratch/local/ssd/anjun/datasets/VOCdevkit/VOC2010/annotations_detectron2/pc59_val has 11321 and 5104 files, respectively.
[01/13 17:52:33] detectron2.data.datasets.coco WARNING: Will use their intersection of 5104 files.
[01/13 17:52:33] detectron2.data.datasets.coco INFO: Loaded 5104 images with semantic segmentation from /scratch/local/ssd/anjun/datasets/VOCdevkit/VOC2010/JPEGImages
[01/13 17:52:33] detectron2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[01/13 17:52:33] detectron2.data.common INFO: Serializing 5104 elements to byte tensors and concatenating them all ...
[01/13 17:52:33] detectron2.data.common INFO: Serialized dataset takes 1.37 MiB
[01/13 17:52:33] detectron2.data.datasets.coco WARNING: Directory /scratch/local/ssd/anjun/datasets/VOCdevkit/VOC2010/JPEGImages and /scratch/local/ssd/anjun/datasets/VOCdevkit/VOC2010/annotations_detectron2/pc59_val has 11321 and 5104 files, respectively.
[01/13 17:52:33] detectron2.data.datasets.coco WARNING: Will use their intersection of 5104 files.
[01/13 17:52:33] detectron2.data.datasets.coco INFO: Loaded 5104 images with semantic segmentation from /scratch/local/ssd/anjun/datasets/VOCdevkit/VOC2010/JPEGImages
[01/13 17:52:33] detectron2.evaluation.evaluator INFO: Start inference on 1276 batches
[01/13 17:52:41] detectron2.evaluation.evaluator INFO: Inference done 11/1276. Dataloading: 0.0007 s/iter. Inference: 0.1310 s/iter. Eval: 0.0064 s/iter. Total: 0.1381 s/iter. ETA=0:02:54
[01/13 17:52:46] detectron2.evaluation.evaluator INFO: Inference done 48/1276. Dataloading: 0.0015 s/iter. Inference: 0.1306 s/iter. Eval: 0.0065 s/iter. Total: 0.1386 s/iter. ETA=0:02:50
[01/13 17:52:52] detectron2.evaluation.evaluator INFO: Inference done 84/1276. Dataloading: 0.0017 s/iter. Inference: 0.1310 s/iter. Eval: 0.0063 s/iter. Total: 0.1390 s/iter. ETA=0:02:45
[01/13 17:52:57] detectron2.evaluation.evaluator INFO: Inference done 121/1276. Dataloading: 0.0016 s/iter. Inference: 0.1302 s/iter. Eval: 0.0062 s/iter. Total: 0.1379 s/iter. ETA=0:02:39
[01/13 17:53:02] detectron2.evaluation.evaluator INFO: Inference done 158/1276. Dataloading: 0.0016 s/iter. Inference: 0.1299 s/iter. Eval: 0.0060 s/iter. Total: 0.1375 s/iter. ETA=0:02:33
[01/13 17:53:07] detectron2.evaluation.evaluator INFO: Inference done 194/1276. Dataloading: 0.0016 s/iter. Inference: 0.1303 s/iter. Eval: 0.0060 s/iter. Total: 0.1379 s/iter. ETA=0:02:29
[01/13 17:53:12] detectron2.evaluation.evaluator INFO: Inference done 230/1276. Dataloading: 0.0016 s/iter. Inference: 0.1305 s/iter. Eval: 0.0060 s/iter. Total: 0.1381 s/iter. ETA=0:02:24
[01/13 17:53:17] detectron2.evaluation.evaluator INFO: Inference done 267/1276. Dataloading: 0.0016 s/iter. Inference: 0.1306 s/iter. Eval: 0.0060 s/iter. Total: 0.1382 s/iter. ETA=0:02:19
[01/13 17:53:22] detectron2.evaluation.evaluator INFO: Inference done 303/1276. Dataloading: 0.0016 s/iter. Inference: 0.1308 s/iter. Eval: 0.0060 s/iter. Total: 0.1385 s/iter. ETA=0:02:14
[01/13 17:53:27] detectron2.evaluation.evaluator INFO: Inference done 340/1276. Dataloading: 0.0016 s/iter. Inference: 0.1307 s/iter. Eval: 0.0060 s/iter. Total: 0.1383 s/iter. ETA=0:02:09
[01/13 17:53:32] detectron2.evaluation.evaluator INFO: Inference done 376/1276. Dataloading: 0.0016 s/iter. Inference: 0.1308 s/iter. Eval: 0.0060 s/iter. Total: 0.1384 s/iter. ETA=0:02:04
[01/13 17:53:37] detectron2.evaluation.evaluator INFO: Inference done 414/1276. Dataloading: 0.0016 s/iter. Inference: 0.1303 s/iter. Eval: 0.0060 s/iter. Total: 0.1380 s/iter. ETA=0:01:58
[01/13 17:53:42] detectron2.evaluation.evaluator INFO: Inference done 450/1276. Dataloading: 0.0016 s/iter. Inference: 0.1303 s/iter. Eval: 0.0061 s/iter. Total: 0.1380 s/iter. ETA=0:01:54
[01/13 17:53:47] detectron2.evaluation.evaluator INFO: Inference done 485/1276. Dataloading: 0.0016 s/iter. Inference: 0.1306 s/iter. Eval: 0.0063 s/iter. Total: 0.1386 s/iter. ETA=0:01:49
[01/13 17:53:52] detectron2.evaluation.evaluator INFO: Inference done 520/1276. Dataloading: 0.0017 s/iter. Inference: 0.1310 s/iter. Eval: 0.0064 s/iter. Total: 0.1391 s/iter. ETA=0:01:45
[01/13 17:53:57] detectron2.evaluation.evaluator INFO: Inference done 556/1276. Dataloading: 0.0017 s/iter. Inference: 0.1311 s/iter. Eval: 0.0064 s/iter. Total: 0.1392 s/iter. ETA=0:01:40
[01/13 17:54:02] detectron2.evaluation.evaluator INFO: Inference done 592/1276. Dataloading: 0.0017 s/iter. Inference: 0.1311 s/iter. Eval: 0.0064 s/iter. Total: 0.1392 s/iter. ETA=0:01:35
[01/13 17:54:07] detectron2.evaluation.evaluator INFO: Inference done 629/1276. Dataloading: 0.0017 s/iter. Inference: 0.1310 s/iter. Eval: 0.0064 s/iter. Total: 0.1391 s/iter. ETA=0:01:29
[01/13 17:54:12] detectron2.evaluation.evaluator INFO: Inference done 666/1276. Dataloading: 0.0017 s/iter. Inference: 0.1309 s/iter. Eval: 0.0063 s/iter. Total: 0.1389 s/iter. ETA=0:01:24
[01/13 17:54:17] detectron2.evaluation.evaluator INFO: Inference done 702/1276. Dataloading: 0.0017 s/iter. Inference: 0.1309 s/iter. Eval: 0.0063 s/iter. Total: 0.1390 s/iter. ETA=0:01:19
[01/13 17:54:22] detectron2.evaluation.evaluator INFO: Inference done 737/1276. Dataloading: 0.0017 s/iter. Inference: 0.1311 s/iter. Eval: 0.0063 s/iter. Total: 0.1392 s/iter. ETA=0:01:15
[01/13 17:54:27] detectron2.evaluation.evaluator INFO: Inference done 773/1276. Dataloading: 0.0017 s/iter. Inference: 0.1312 s/iter. Eval: 0.0063 s/iter. Total: 0.1392 s/iter. ETA=0:01:10
[01/13 17:54:32] detectron2.evaluation.evaluator INFO: Inference done 808/1276. Dataloading: 0.0017 s/iter. Inference: 0.1313 s/iter. Eval: 0.0063 s/iter. Total: 0.1394 s/iter. ETA=0:01:05
[01/13 17:54:37] detectron2.evaluation.evaluator INFO: Inference done 844/1276. Dataloading: 0.0017 s/iter. Inference: 0.1313 s/iter. Eval: 0.0063 s/iter. Total: 0.1394 s/iter. ETA=0:01:00
[01/13 17:54:43] detectron2.evaluation.evaluator INFO: Inference done 880/1276. Dataloading: 0.0017 s/iter. Inference: 0.1314 s/iter. Eval: 0.0063 s/iter. Total: 0.1395 s/iter. ETA=0:00:55
[01/13 17:54:48] detectron2.evaluation.evaluator INFO: Inference done 917/1276. Dataloading: 0.0017 s/iter. Inference: 0.1313 s/iter. Eval: 0.0063 s/iter. Total: 0.1394 s/iter. ETA=0:00:50
[01/13 17:54:53] detectron2.evaluation.evaluator INFO: Inference done 953/1276. Dataloading: 0.0017 s/iter. Inference: 0.1314 s/iter. Eval: 0.0063 s/iter. Total: 0.1394 s/iter. ETA=0:00:45
[01/13 17:54:58] detectron2.evaluation.evaluator INFO: Inference done 989/1276. Dataloading: 0.0017 s/iter. Inference: 0.1314 s/iter. Eval: 0.0063 s/iter. Total: 0.1395 s/iter. ETA=0:00:40
[01/13 17:55:03] detectron2.evaluation.evaluator INFO: Inference done 1025/1276. Dataloading: 0.0017 s/iter. Inference: 0.1315 s/iter. Eval: 0.0063 s/iter. Total: 0.1395 s/iter. ETA=0:00:35
[01/13 17:55:08] detectron2.evaluation.evaluator INFO: Inference done 1063/1276. Dataloading: 0.0017 s/iter. Inference: 0.1313 s/iter. Eval: 0.0063 s/iter. Total: 0.1393 s/iter. ETA=0:00:29
[01/13 17:55:13] detectron2.evaluation.evaluator INFO: Inference done 1100/1276. Dataloading: 0.0017 s/iter. Inference: 0.1313 s/iter. Eval: 0.0063 s/iter. Total: 0.1393 s/iter. ETA=0:00:24
[01/13 17:55:18] detectron2.evaluation.evaluator INFO: Inference done 1137/1276. Dataloading: 0.0017 s/iter. Inference: 0.1313 s/iter. Eval: 0.0062 s/iter. Total: 0.1392 s/iter. ETA=0:00:19
[01/13 17:55:23] detectron2.evaluation.evaluator INFO: Inference done 1174/1276. Dataloading: 0.0017 s/iter. Inference: 0.1312 s/iter. Eval: 0.0062 s/iter. Total: 0.1391 s/iter. ETA=0:00:14
[01/13 17:55:28] detectron2.evaluation.evaluator INFO: Inference done 1211/1276. Dataloading: 0.0017 s/iter. Inference: 0.1311 s/iter. Eval: 0.0062 s/iter. Total: 0.1390 s/iter. ETA=0:00:09
[01/13 17:55:33] detectron2.evaluation.evaluator INFO: Inference done 1248/1276. Dataloading: 0.0017 s/iter. Inference: 0.1311 s/iter. Eval: 0.0062 s/iter. Total: 0.1390 s/iter. ETA=0:00:03
[01/13 17:55:38] detectron2.evaluation.evaluator INFO: Total inference time: 0:02:57.369816 (0.139551 s / iter per device, on 4 devices)
[01/13 17:55:38] detectron2.evaluation.evaluator INFO: Total inference pure compute time: 0:02:46 (0.131014 s / iter per device, on 4 devices)
[01/13 18:14:34] mask_former.data.dataset_mappers.mask_former_semantic_dataset_mapper INFO: [MaskFormerSemanticDatasetMapper] Augmentations used in inference: []
[01/13 18:14:34] detectron2.data.datasets.coco WARNING: Directory /scratch/local/ssd/anjun/datasets/VOCdevkit/VOC2010/JPEGImages and /scratch/local/ssd/anjun/datasets/VOCdevkit/VOC2010/annotations_detectron2/pc59_val has 11321 and 5104 files, respectively.
[01/13 18:14:34] detectron2.data.datasets.coco WARNING: Will use their intersection of 5104 files.
[01/13 18:14:34] detectron2.data.datasets.coco INFO: Loaded 5104 images with semantic segmentation from /scratch/local/ssd/anjun/datasets/VOCdevkit/VOC2010/JPEGImages
[01/13 18:14:34] detectron2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[01/13 18:14:34] detectron2.data.common INFO: Serializing 5104 elements to byte tensors and concatenating them all ...
[01/13 18:14:34] detectron2.data.common INFO: Serialized dataset takes 1.37 MiB
[01/13 18:14:34] detectron2.data.datasets.coco WARNING: Directory /scratch/local/ssd/anjun/datasets/VOCdevkit/VOC2010/JPEGImages and /scratch/local/ssd/anjun/datasets/VOCdevkit/VOC2010/annotations_detectron2/pc59_val has 11321 and 5104 files, respectively.
[01/13 18:14:34] detectron2.data.datasets.coco WARNING: Will use their intersection of 5104 files.
[01/13 18:14:34] detectron2.data.datasets.coco INFO: Loaded 5104 images with semantic segmentation from /scratch/local/ssd/anjun/datasets/VOCdevkit/VOC2010/JPEGImages
[01/13 18:14:34] detectron2.evaluation.evaluator INFO: Start inference on 1276 batches
[01/13 18:14:42] detectron2.evaluation.evaluator INFO: Inference done 11/1276. Dataloading: 0.0008 s/iter. Inference: 0.1289 s/iter. Eval: 0.0060 s/iter. Total: 0.1357 s/iter. ETA=0:02:51
[01/13 18:14:47] detectron2.evaluation.evaluator INFO: Inference done 48/1276. Dataloading: 0.0012 s/iter. Inference: 0.1300 s/iter. Eval: 0.0061 s/iter. Total: 0.1374 s/iter. ETA=0:02:48
[01/13 18:14:53] detectron2.evaluation.evaluator INFO: Inference done 85/1276. Dataloading: 0.0013 s/iter. Inference: 0.1302 s/iter. Eval: 0.0060 s/iter. Total: 0.1376 s/iter. ETA=0:02:43
[01/13 18:14:58] detectron2.evaluation.evaluator INFO: Inference done 122/1276. Dataloading: 0.0014 s/iter. Inference: 0.1297 s/iter. Eval: 0.0060 s/iter. Total: 0.1371 s/iter. ETA=0:02:38
[01/13 18:15:03] detectron2.evaluation.evaluator INFO: Inference done 159/1276. Dataloading: 0.0014 s/iter. Inference: 0.1300 s/iter. Eval: 0.0059 s/iter. Total: 0.1374 s/iter. ETA=0:02:33
[01/13 18:15:08] detectron2.evaluation.evaluator INFO: Inference done 196/1276. Dataloading: 0.0014 s/iter. Inference: 0.1300 s/iter. Eval: 0.0059 s/iter. Total: 0.1373 s/iter. ETA=0:02:28
[01/13 18:15:13] detectron2.evaluation.evaluator INFO: Inference done 233/1276. Dataloading: 0.0014 s/iter. Inference: 0.1301 s/iter. Eval: 0.0059 s/iter. Total: 0.1375 s/iter. ETA=0:02:23
[01/13 18:15:18] detectron2.evaluation.evaluator INFO: Inference done 270/1276. Dataloading: 0.0014 s/iter. Inference: 0.1302 s/iter. Eval: 0.0060 s/iter. Total: 0.1377 s/iter. ETA=0:02:18
[01/13 18:15:23] detectron2.evaluation.evaluator INFO: Inference done 306/1276. Dataloading: 0.0014 s/iter. Inference: 0.1305 s/iter. Eval: 0.0060 s/iter. Total: 0.1379 s/iter. ETA=0:02:13
[01/13 18:15:28] detectron2.evaluation.evaluator INFO: Inference done 342/1276. Dataloading: 0.0014 s/iter. Inference: 0.1307 s/iter. Eval: 0.0059 s/iter. Total: 0.1381 s/iter. ETA=0:02:08
[01/13 18:15:33] detectron2.evaluation.evaluator INFO: Inference done 378/1276. Dataloading: 0.0014 s/iter. Inference: 0.1310 s/iter. Eval: 0.0060 s/iter. Total: 0.1384 s/iter. ETA=0:02:04
[01/13 18:15:38] detectron2.evaluation.evaluator INFO: Inference done 415/1276. Dataloading: 0.0015 s/iter. Inference: 0.1308 s/iter. Eval: 0.0060 s/iter. Total: 0.1382 s/iter. ETA=0:01:58
[01/13 18:15:43] detectron2.evaluation.evaluator INFO: Inference done 451/1276. Dataloading: 0.0015 s/iter. Inference: 0.1310 s/iter. Eval: 0.0060 s/iter. Total: 0.1385 s/iter. ETA=0:01:54
[01/13 18:15:48] detectron2.evaluation.evaluator INFO: Inference done 488/1276. Dataloading: 0.0015 s/iter. Inference: 0.1309 s/iter. Eval: 0.0060 s/iter. Total: 0.1384 s/iter. ETA=0:01:49
[01/13 18:15:53] detectron2.evaluation.evaluator INFO: Inference done 524/1276. Dataloading: 0.0015 s/iter. Inference: 0.1309 s/iter. Eval: 0.0060 s/iter. Total: 0.1384 s/iter. ETA=0:01:44
[01/13 18:15:58] detectron2.evaluation.evaluator INFO: Inference done 561/1276. Dataloading: 0.0015 s/iter. Inference: 0.1308 s/iter. Eval: 0.0060 s/iter. Total: 0.1383 s/iter. ETA=0:01:38
[01/13 18:16:03] detectron2.evaluation.evaluator INFO: Inference done 597/1276. Dataloading: 0.0015 s/iter. Inference: 0.1309 s/iter. Eval: 0.0060 s/iter. Total: 0.1384 s/iter. ETA=0:01:33
[01/13 18:16:09] detectron2.evaluation.evaluator INFO: Inference done 634/1276. Dataloading: 0.0015 s/iter. Inference: 0.1308 s/iter. Eval: 0.0060 s/iter. Total: 0.1384 s/iter. ETA=0:01:28
[01/13 18:16:14] detectron2.evaluation.evaluator INFO: Inference done 671/1276. Dataloading: 0.0015 s/iter. Inference: 0.1307 s/iter. Eval: 0.0060 s/iter. Total: 0.1382 s/iter. ETA=0:01:23
[01/13 18:16:19] detectron2.evaluation.evaluator INFO: Inference done 707/1276. Dataloading: 0.0015 s/iter. Inference: 0.1308 s/iter. Eval: 0.0060 s/iter. Total: 0.1383 s/iter. ETA=0:01:18
[01/13 18:16:24] detectron2.evaluation.evaluator INFO: Inference done 743/1276. Dataloading: 0.0015 s/iter. Inference: 0.1309 s/iter. Eval: 0.0060 s/iter. Total: 0.1385 s/iter. ETA=0:01:13
[01/13 18:16:29] detectron2.evaluation.evaluator INFO: Inference done 780/1276. Dataloading: 0.0015 s/iter. Inference: 0.1309 s/iter. Eval: 0.0060 s/iter. Total: 0.1385 s/iter. ETA=0:01:08
[01/13 18:16:34] detectron2.evaluation.evaluator INFO: Inference done 816/1276. Dataloading: 0.0015 s/iter. Inference: 0.1310 s/iter. Eval: 0.0061 s/iter. Total: 0.1386 s/iter. ETA=0:01:03
[01/13 18:16:39] detectron2.evaluation.evaluator INFO: Inference done 852/1276. Dataloading: 0.0015 s/iter. Inference: 0.1311 s/iter. Eval: 0.0061 s/iter. Total: 0.1387 s/iter. ETA=0:00:58
[01/13 18:16:44] detectron2.evaluation.evaluator INFO: Inference done 888/1276. Dataloading: 0.0015 s/iter. Inference: 0.1311 s/iter. Eval: 0.0061 s/iter. Total: 0.1387 s/iter. ETA=0:00:53
[01/13 18:16:49] detectron2.evaluation.evaluator INFO: Inference done 924/1276. Dataloading: 0.0015 s/iter. Inference: 0.1311 s/iter. Eval: 0.0061 s/iter. Total: 0.1388 s/iter. ETA=0:00:48
[01/13 18:16:54] detectron2.evaluation.evaluator INFO: Inference done 960/1276. Dataloading: 0.0015 s/iter. Inference: 0.1312 s/iter. Eval: 0.0061 s/iter. Total: 0.1389 s/iter. ETA=0:00:43
[01/13 18:16:59] detectron2.evaluation.evaluator INFO: Inference done 994/1276. Dataloading: 0.0015 s/iter. Inference: 0.1315 s/iter. Eval: 0.0062 s/iter. Total: 0.1393 s/iter. ETA=0:00:39
[01/13 18:17:04] detectron2.evaluation.evaluator INFO: Inference done 1031/1276. Dataloading: 0.0015 s/iter. Inference: 0.1315 s/iter. Eval: 0.0062 s/iter. Total: 0.1393 s/iter. ETA=0:00:34
[01/13 18:17:09] detectron2.evaluation.evaluator INFO: Inference done 1067/1276. Dataloading: 0.0015 s/iter. Inference: 0.1315 s/iter. Eval: 0.0062 s/iter. Total: 0.1393 s/iter. ETA=0:00:29
[01/13 18:17:14] detectron2.evaluation.evaluator INFO: Inference done 1103/1276. Dataloading: 0.0015 s/iter. Inference: 0.1315 s/iter. Eval: 0.0062 s/iter. Total: 0.1393 s/iter. ETA=0:00:24
[01/13 18:17:20] detectron2.evaluation.evaluator INFO: Inference done 1139/1276. Dataloading: 0.0015 s/iter. Inference: 0.1315 s/iter. Eval: 0.0062 s/iter. Total: 0.1393 s/iter. ETA=0:00:19
[01/13 18:17:25] detectron2.evaluation.evaluator INFO: Inference done 1176/1276. Dataloading: 0.0015 s/iter. Inference: 0.1315 s/iter. Eval: 0.0062 s/iter. Total: 0.1393 s/iter. ETA=0:00:13
[01/13 18:17:30] detectron2.evaluation.evaluator INFO: Inference done 1213/1276. Dataloading: 0.0015 s/iter. Inference: 0.1314 s/iter. Eval: 0.0062 s/iter. Total: 0.1392 s/iter. ETA=0:00:08
[01/13 18:17:35] detectron2.evaluation.evaluator INFO: Inference done 1250/1276. Dataloading: 0.0015 s/iter. Inference: 0.1313 s/iter. Eval: 0.0062 s/iter. Total: 0.1391 s/iter. ETA=0:00:03
[01/13 18:17:39] detectron2.evaluation.evaluator INFO: Total inference time: 0:02:57.419041 (0.139590 s / iter per device, on 4 devices)
[01/13 18:17:39] detectron2.evaluation.evaluator INFO: Total inference pure compute time: 0:02:46 (0.131272 s / iter per device, on 4 devices)
[01/13 18:36:36] mask_former.data.dataset_mappers.mask_former_semantic_dataset_mapper INFO: [MaskFormerSemanticDatasetMapper] Augmentations used in inference: []
[01/13 18:36:36] detectron2.data.datasets.coco WARNING: Directory /scratch/local/ssd/anjun/datasets/VOCdevkit/VOC2010/JPEGImages and /scratch/local/ssd/anjun/datasets/VOCdevkit/VOC2010/annotations_detectron2/pc59_val has 11321 and 5104 files, respectively.
[01/13 18:36:36] detectron2.data.datasets.coco WARNING: Will use their intersection of 5104 files.
[01/13 18:36:36] detectron2.data.datasets.coco INFO: Loaded 5104 images with semantic segmentation from /scratch/local/ssd/anjun/datasets/VOCdevkit/VOC2010/JPEGImages
[01/13 18:36:36] detectron2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[01/13 18:36:36] detectron2.data.common INFO: Serializing 5104 elements to byte tensors and concatenating them all ...
[01/13 18:36:36] detectron2.data.common INFO: Serialized dataset takes 1.37 MiB
[01/13 18:36:36] detectron2.data.datasets.coco WARNING: Directory /scratch/local/ssd/anjun/datasets/VOCdevkit/VOC2010/JPEGImages and /scratch/local/ssd/anjun/datasets/VOCdevkit/VOC2010/annotations_detectron2/pc59_val has 11321 and 5104 files, respectively.
[01/13 18:36:36] detectron2.data.datasets.coco WARNING: Will use their intersection of 5104 files.
[01/13 18:36:36] detectron2.data.datasets.coco INFO: Loaded 5104 images with semantic segmentation from /scratch/local/ssd/anjun/datasets/VOCdevkit/VOC2010/JPEGImages
[01/13 18:36:36] detectron2.evaluation.evaluator INFO: Start inference on 1276 batches
[01/13 18:36:44] detectron2.evaluation.evaluator INFO: Inference done 11/1276. Dataloading: 0.0006 s/iter. Inference: 0.1286 s/iter. Eval: 0.0059 s/iter. Total: 0.1351 s/iter. ETA=0:02:50
[01/13 18:36:49] detectron2.evaluation.evaluator INFO: Inference done 48/1276. Dataloading: 0.0014 s/iter. Inference: 0.1306 s/iter. Eval: 0.0061 s/iter. Total: 0.1382 s/iter. ETA=0:02:49
[01/13 18:36:54] detectron2.evaluation.evaluator INFO: Inference done 84/1276. Dataloading: 0.0015 s/iter. Inference: 0.1315 s/iter. Eval: 0.0062 s/iter. Total: 0.1392 s/iter. ETA=0:02:45
[01/13 18:36:59] detectron2.evaluation.evaluator INFO: Inference done 121/1276. Dataloading: 0.0015 s/iter. Inference: 0.1309 s/iter. Eval: 0.0061 s/iter. Total: 0.1385 s/iter. ETA=0:02:40
[01/13 18:37:04] detectron2.evaluation.evaluator INFO: Inference done 158/1276. Dataloading: 0.0015 s/iter. Inference: 0.1304 s/iter. Eval: 0.0060 s/iter. Total: 0.1380 s/iter. ETA=0:02:34
[01/13 18:37:09] detectron2.evaluation.evaluator INFO: Inference done 194/1276. Dataloading: 0.0016 s/iter. Inference: 0.1306 s/iter. Eval: 0.0059 s/iter. Total: 0.1382 s/iter. ETA=0:02:29
[01/13 18:37:14] detectron2.evaluation.evaluator INFO: Inference done 231/1276. Dataloading: 0.0016 s/iter. Inference: 0.1306 s/iter. Eval: 0.0060 s/iter. Total: 0.1382 s/iter. ETA=0:02:24
[01/13 18:37:19] detectron2.evaluation.evaluator INFO: Inference done 268/1276. Dataloading: 0.0016 s/iter. Inference: 0.1305 s/iter. Eval: 0.0060 s/iter. Total: 0.1380 s/iter. ETA=0:02:19
[01/13 18:37:24] detectron2.evaluation.evaluator INFO: Inference done 305/1276. Dataloading: 0.0015 s/iter. Inference: 0.1305 s/iter. Eval: 0.0059 s/iter. Total: 0.1381 s/iter. ETA=0:02:14
[01/13 18:37:29] detectron2.evaluation.evaluator INFO: Inference done 342/1276. Dataloading: 0.0016 s/iter. Inference: 0.1305 s/iter. Eval: 0.0059 s/iter. Total: 0.1380 s/iter. ETA=0:02:08
[01/13 18:37:34] detectron2.evaluation.evaluator INFO: Inference done 379/1276. Dataloading: 0.0015 s/iter. Inference: 0.1305 s/iter. Eval: 0.0060 s/iter. Total: 0.1380 s/iter. ETA=0:02:03
[01/13 18:37:39] detectron2.evaluation.evaluator INFO: Inference done 417/1276. Dataloading: 0.0015 s/iter. Inference: 0.1301 s/iter. Eval: 0.0059 s/iter. Total: 0.1376 s/iter. ETA=0:01:58
[01/13 18:37:44] detectron2.evaluation.evaluator INFO: Inference done 454/1276. Dataloading: 0.0015 s/iter. Inference: 0.1299 s/iter. Eval: 0.0059 s/iter. Total: 0.1374 s/iter. ETA=0:01:52
[01/13 18:37:50] detectron2.evaluation.evaluator INFO: Inference done 491/1276. Dataloading: 0.0015 s/iter. Inference: 0.1299 s/iter. Eval: 0.0060 s/iter. Total: 0.1374 s/iter. ETA=0:01:47
[01/13 18:37:55] detectron2.evaluation.evaluator INFO: Inference done 528/1276. Dataloading: 0.0015 s/iter. Inference: 0.1299 s/iter. Eval: 0.0059 s/iter. Total: 0.1374 s/iter. ETA=0:01:42
[01/13 18:38:00] detectron2.evaluation.evaluator INFO: Inference done 565/1276. Dataloading: 0.0015 s/iter. Inference: 0.1298 s/iter. Eval: 0.0060 s/iter. Total: 0.1373 s/iter. ETA=0:01:37
[01/13 18:38:05] detectron2.evaluation.evaluator INFO: Inference done 602/1276. Dataloading: 0.0015 s/iter. Inference: 0.1297 s/iter. Eval: 0.0060 s/iter. Total: 0.1373 s/iter. ETA=0:01:32
[01/13 18:38:10] detectron2.evaluation.evaluator INFO: Inference done 639/1276. Dataloading: 0.0015 s/iter. Inference: 0.1298 s/iter. Eval: 0.0060 s/iter. Total: 0.1373 s/iter. ETA=0:01:27
[01/13 18:38:15] detectron2.evaluation.evaluator INFO: Inference done 676/1276. Dataloading: 0.0015 s/iter. Inference: 0.1297 s/iter. Eval: 0.0060 s/iter. Total: 0.1372 s/iter. ETA=0:01:22
[01/13 18:38:20] detectron2.evaluation.evaluator INFO: Inference done 712/1276. Dataloading: 0.0015 s/iter. Inference: 0.1298 s/iter. Eval: 0.0060 s/iter. Total: 0.1373 s/iter. ETA=0:01:17
[01/13 18:38:25] detectron2.evaluation.evaluator INFO: Inference done 749/1276. Dataloading: 0.0015 s/iter. Inference: 0.1298 s/iter. Eval: 0.0060 s/iter. Total: 0.1374 s/iter. ETA=0:01:12
[01/13 18:38:30] detectron2.evaluation.evaluator INFO: Inference done 786/1276. Dataloading: 0.0015 s/iter. Inference: 0.1298 s/iter. Eval: 0.0060 s/iter. Total: 0.1374 s/iter. ETA=0:01:07
[01/13 18:38:35] detectron2.evaluation.evaluator INFO: Inference done 823/1276. Dataloading: 0.0015 s/iter. Inference: 0.1299 s/iter. Eval: 0.0060 s/iter. Total: 0.1374 s/iter. ETA=0:01:02
[01/13 18:38:40] detectron2.evaluation.evaluator INFO: Inference done 860/1276. Dataloading: 0.0015 s/iter. Inference: 0.1299 s/iter. Eval: 0.0060 s/iter. Total: 0.1375 s/iter. ETA=0:00:57
[01/13 18:38:45] detectron2.evaluation.evaluator INFO: Inference done 897/1276. Dataloading: 0.0015 s/iter. Inference: 0.1299 s/iter. Eval: 0.0060 s/iter. Total: 0.1374 s/iter. ETA=0:00:52
[01/13 18:38:50] detectron2.evaluation.evaluator INFO: Inference done 933/1276. Dataloading: 0.0015 s/iter. Inference: 0.1300 s/iter. Eval: 0.0060 s/iter. Total: 0.1375 s/iter. ETA=0:00:47
[01/13 18:38:56] detectron2.evaluation.evaluator INFO: Inference done 970/1276. Dataloading: 0.0015 s/iter. Inference: 0.1300 s/iter. Eval: 0.0060 s/iter. Total: 0.1375 s/iter. ETA=0:00:42
[01/13 18:39:01] detectron2.evaluation.evaluator INFO: Inference done 1007/1276. Dataloading: 0.0015 s/iter. Inference: 0.1300 s/iter. Eval: 0.0060 s/iter. Total: 0.1376 s/iter. ETA=0:00:37
[01/13 18:39:06] detectron2.evaluation.evaluator INFO: Inference done 1044/1276. Dataloading: 0.0015 s/iter. Inference: 0.1300 s/iter. Eval: 0.0060 s/iter. Total: 0.1375 s/iter. ETA=0:00:31
[01/13 18:39:11] detectron2.evaluation.evaluator INFO: Inference done 1082/1276. Dataloading: 0.0016 s/iter. Inference: 0.1298 s/iter. Eval: 0.0060 s/iter. Total: 0.1374 s/iter. ETA=0:00:26
[01/13 18:39:16] detectron2.evaluation.evaluator INFO: Inference done 1119/1276. Dataloading: 0.0016 s/iter. Inference: 0.1299 s/iter. Eval: 0.0060 s/iter. Total: 0.1374 s/iter. ETA=0:00:21
[01/13 18:39:21] detectron2.evaluation.evaluator INFO: Inference done 1156/1276. Dataloading: 0.0015 s/iter. Inference: 0.1299 s/iter. Eval: 0.0060 s/iter. Total: 0.1374 s/iter. ETA=0:00:16
[01/13 18:39:26] detectron2.evaluation.evaluator INFO: Inference done 1193/1276. Dataloading: 0.0015 s/iter. Inference: 0.1298 s/iter. Eval: 0.0060 s/iter. Total: 0.1374 s/iter. ETA=0:00:11
[01/13 18:39:31] detectron2.evaluation.evaluator INFO: Inference done 1230/1276. Dataloading: 0.0016 s/iter. Inference: 0.1298 s/iter. Eval: 0.0060 s/iter. Total: 0.1373 s/iter. ETA=0:00:06
[01/13 18:39:36] detectron2.evaluation.evaluator INFO: Inference done 1267/1276. Dataloading: 0.0016 s/iter. Inference: 0.1298 s/iter. Eval: 0.0060 s/iter. Total: 0.1373 s/iter. ETA=0:00:01
[01/13 18:39:38] detectron2.evaluation.evaluator INFO: Total inference time: 0:02:55.561217 (0.138128 s / iter per device, on 4 devices)
[01/13 18:39:38] detectron2.evaluation.evaluator INFO: Total inference pure compute time: 0:02:44 (0.129785 s / iter per device, on 4 devices)
[01/13 18:58:35] mask_former.data.dataset_mappers.mask_former_semantic_dataset_mapper INFO: [MaskFormerSemanticDatasetMapper] Augmentations used in inference: []
[01/13 18:58:35] detectron2.data.datasets.coco WARNING: Directory /scratch/local/ssd/anjun/datasets/VOCdevkit/VOC2010/JPEGImages and /scratch/local/ssd/anjun/datasets/VOCdevkit/VOC2010/annotations_detectron2/pc59_val has 11321 and 5104 files, respectively.
[01/13 18:58:35] detectron2.data.datasets.coco WARNING: Will use their intersection of 5104 files.
[01/13 18:58:35] detectron2.data.datasets.coco INFO: Loaded 5104 images with semantic segmentation from /scratch/local/ssd/anjun/datasets/VOCdevkit/VOC2010/JPEGImages
[01/13 18:58:35] detectron2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[01/13 18:58:35] detectron2.data.common INFO: Serializing 5104 elements to byte tensors and concatenating them all ...
[01/13 18:58:35] detectron2.data.common INFO: Serialized dataset takes 1.37 MiB
[01/13 18:58:35] detectron2.data.datasets.coco WARNING: Directory /scratch/local/ssd/anjun/datasets/VOCdevkit/VOC2010/JPEGImages and /scratch/local/ssd/anjun/datasets/VOCdevkit/VOC2010/annotations_detectron2/pc59_val has 11321 and 5104 files, respectively.
[01/13 18:58:35] detectron2.data.datasets.coco WARNING: Will use their intersection of 5104 files.
[01/13 18:58:35] detectron2.data.datasets.coco INFO: Loaded 5104 images with semantic segmentation from /scratch/local/ssd/anjun/datasets/VOCdevkit/VOC2010/JPEGImages
[01/13 18:58:35] detectron2.evaluation.evaluator INFO: Start inference on 1276 batches
[01/13 18:58:43] detectron2.evaluation.evaluator INFO: Inference done 11/1276. Dataloading: 0.0007 s/iter. Inference: 0.1311 s/iter. Eval: 0.0057 s/iter. Total: 0.1375 s/iter. ETA=0:02:53
[01/13 18:58:48] detectron2.evaluation.evaluator INFO: Inference done 48/1276. Dataloading: 0.0014 s/iter. Inference: 0.1310 s/iter. Eval: 0.0062 s/iter. Total: 0.1386 s/iter. ETA=0:02:50
[01/13 18:58:54] detectron2.evaluation.evaluator INFO: Inference done 85/1276. Dataloading: 0.0014 s/iter. Inference: 0.1310 s/iter. Eval: 0.0063 s/iter. Total: 0.1387 s/iter. ETA=0:02:45
[01/13 18:58:59] detectron2.evaluation.evaluator INFO: Inference done 121/1276. Dataloading: 0.0015 s/iter. Inference: 0.1308 s/iter. Eval: 0.0066 s/iter. Total: 0.1390 s/iter. ETA=0:02:40
[01/13 18:59:04] detectron2.evaluation.evaluator INFO: Inference done 158/1276. Dataloading: 0.0015 s/iter. Inference: 0.1302 s/iter. Eval: 0.0064 s/iter. Total: 0.1381 s/iter. ETA=0:02:34
[01/13 18:59:09] detectron2.evaluation.evaluator INFO: Inference done 195/1276. Dataloading: 0.0015 s/iter. Inference: 0.1301 s/iter. Eval: 0.0062 s/iter. Total: 0.1379 s/iter. ETA=0:02:29
[01/13 18:59:14] detectron2.evaluation.evaluator INFO: Inference done 232/1276. Dataloading: 0.0015 s/iter. Inference: 0.1302 s/iter. Eval: 0.0062 s/iter. Total: 0.1379 s/iter. ETA=0:02:23
[01/13 18:59:19] detectron2.evaluation.evaluator INFO: Inference done 269/1276. Dataloading: 0.0015 s/iter. Inference: 0.1302 s/iter. Eval: 0.0061 s/iter. Total: 0.1378 s/iter. ETA=0:02:18
[01/13 18:59:24] detectron2.evaluation.evaluator INFO: Inference done 306/1276. Dataloading: 0.0015 s/iter. Inference: 0.1301 s/iter. Eval: 0.0061 s/iter. Total: 0.1377 s/iter. ETA=0:02:13
[01/13 18:59:29] detectron2.evaluation.evaluator INFO: Inference done 343/1276. Dataloading: 0.0015 s/iter. Inference: 0.1300 s/iter. Eval: 0.0060 s/iter. Total: 0.1375 s/iter. ETA=0:02:08
[01/13 18:59:34] detectron2.evaluation.evaluator INFO: Inference done 378/1276. Dataloading: 0.0015 s/iter. Inference: 0.1304 s/iter. Eval: 0.0062 s/iter. Total: 0.1380 s/iter. ETA=0:02:03
[01/13 18:59:39] detectron2.evaluation.evaluator INFO: Inference done 416/1276. Dataloading: 0.0015 s/iter. Inference: 0.1301 s/iter. Eval: 0.0061 s/iter. Total: 0.1378 s/iter. ETA=0:01:58
[01/13 18:59:44] detectron2.evaluation.evaluator INFO: Inference done 453/1276. Dataloading: 0.0015 s/iter. Inference: 0.1300 s/iter. Eval: 0.0061 s/iter. Total: 0.1377 s/iter. ETA=0:01:53
[01/13 18:59:49] detectron2.evaluation.evaluator INFO: Inference done 490/1276. Dataloading: 0.0015 s/iter. Inference: 0.1300 s/iter. Eval: 0.0061 s/iter. Total: 0.1377 s/iter. ETA=0:01:48
[01/13 18:59:54] detectron2.evaluation.evaluator INFO: Inference done 527/1276. Dataloading: 0.0015 s/iter. Inference: 0.1300 s/iter. Eval: 0.0061 s/iter. Total: 0.1377 s/iter. ETA=0:01:43
[01/13 18:59:59] detectron2.evaluation.evaluator INFO: Inference done 564/1276. Dataloading: 0.0015 s/iter. Inference: 0.1300 s/iter. Eval: 0.0061 s/iter. Total: 0.1376 s/iter. ETA=0:01:37
[01/13 19:00:04] detectron2.evaluation.evaluator INFO: Inference done 600/1276. Dataloading: 0.0015 s/iter. Inference: 0.1300 s/iter. Eval: 0.0061 s/iter. Total: 0.1377 s/iter. ETA=0:01:33
[01/13 19:00:09] detectron2.evaluation.evaluator INFO: Inference done 637/1276. Dataloading: 0.0015 s/iter. Inference: 0.1300 s/iter. Eval: 0.0061 s/iter. Total: 0.1377 s/iter. ETA=0:01:27
[01/13 19:00:15] detectron2.evaluation.evaluator INFO: Inference done 675/1276. Dataloading: 0.0015 s/iter. Inference: 0.1299 s/iter. Eval: 0.0061 s/iter. Total: 0.1375 s/iter. ETA=0:01:22
[01/13 19:00:20] detectron2.evaluation.evaluator INFO: Inference done 711/1276. Dataloading: 0.0015 s/iter. Inference: 0.1299 s/iter. Eval: 0.0061 s/iter. Total: 0.1376 s/iter. ETA=0:01:17
[01/13 19:00:25] detectron2.evaluation.evaluator INFO: Inference done 747/1276. Dataloading: 0.0015 s/iter. Inference: 0.1301 s/iter. Eval: 0.0061 s/iter. Total: 0.1378 s/iter. ETA=0:01:12
[01/13 19:00:30] detectron2.evaluation.evaluator INFO: Inference done 784/1276. Dataloading: 0.0015 s/iter. Inference: 0.1301 s/iter. Eval: 0.0061 s/iter. Total: 0.1378 s/iter. ETA=0:01:07
[01/13 19:00:35] detectron2.evaluation.evaluator INFO: Inference done 821/1276. Dataloading: 0.0015 s/iter. Inference: 0.1302 s/iter. Eval: 0.0061 s/iter. Total: 0.1379 s/iter. ETA=0:01:02
[01/13 19:00:40] detectron2.evaluation.evaluator INFO: Inference done 856/1276. Dataloading: 0.0016 s/iter. Inference: 0.1303 s/iter. Eval: 0.0062 s/iter. Total: 0.1381 s/iter. ETA=0:00:57
[01/13 19:00:45] detectron2.evaluation.evaluator INFO: Inference done 893/1276. Dataloading: 0.0016 s/iter. Inference: 0.1303 s/iter. Eval: 0.0062 s/iter. Total: 0.1380 s/iter. ETA=0:00:52
[01/13 19:00:50] detectron2.evaluation.evaluator INFO: Inference done 930/1276. Dataloading: 0.0016 s/iter. Inference: 0.1303 s/iter. Eval: 0.0062 s/iter. Total: 0.1380 s/iter. ETA=0:00:47
[01/13 19:00:55] detectron2.evaluation.evaluator INFO: Inference done 966/1276. Dataloading: 0.0016 s/iter. Inference: 0.1303 s/iter. Eval: 0.0062 s/iter. Total: 0.1381 s/iter. ETA=0:00:42
[01/13 19:01:00] detectron2.evaluation.evaluator INFO: Inference done 1002/1276. Dataloading: 0.0016 s/iter. Inference: 0.1305 s/iter. Eval: 0.0062 s/iter. Total: 0.1382 s/iter. ETA=0:00:37
[01/13 19:01:05] detectron2.evaluation.evaluator INFO: Inference done 1039/1276. Dataloading: 0.0016 s/iter. Inference: 0.1304 s/iter. Eval: 0.0062 s/iter. Total: 0.1381 s/iter. ETA=0:00:32
[01/13 19:01:10] detectron2.evaluation.evaluator INFO: Inference done 1077/1276. Dataloading: 0.0016 s/iter. Inference: 0.1302 s/iter. Eval: 0.0061 s/iter. Total: 0.1380 s/iter. ETA=0:00:27
[01/13 19:01:15] detectron2.evaluation.evaluator INFO: Inference done 1114/1276. Dataloading: 0.0016 s/iter. Inference: 0.1302 s/iter. Eval: 0.0061 s/iter. Total: 0.1380 s/iter. ETA=0:00:22
[01/13 19:01:21] detectron2.evaluation.evaluator INFO: Inference done 1151/1276. Dataloading: 0.0016 s/iter. Inference: 0.1302 s/iter. Eval: 0.0061 s/iter. Total: 0.1379 s/iter. ETA=0:00:17
[01/13 19:01:26] detectron2.evaluation.evaluator INFO: Inference done 1188/1276. Dataloading: 0.0016 s/iter. Inference: 0.1302 s/iter. Eval: 0.0061 s/iter. Total: 0.1379 s/iter. ETA=0:00:12
[01/13 19:01:31] detectron2.evaluation.evaluator INFO: Inference done 1226/1276. Dataloading: 0.0016 s/iter. Inference: 0.1301 s/iter. Eval: 0.0061 s/iter. Total: 0.1378 s/iter. ETA=0:00:06
[01/13 19:01:36] detectron2.evaluation.evaluator INFO: Inference done 1263/1276. Dataloading: 0.0016 s/iter. Inference: 0.1301 s/iter. Eval: 0.0061 s/iter. Total: 0.1378 s/iter. ETA=0:00:01
[01/13 19:01:38] detectron2.evaluation.evaluator INFO: Total inference time: 0:02:55.918682 (0.138410 s / iter per device, on 4 devices)
[01/13 19:01:38] detectron2.evaluation.evaluator INFO: Total inference pure compute time: 0:02:45 (0.130089 s / iter per device, on 4 devices)
[01/13 19:20:35] mask_former.data.dataset_mappers.mask_former_semantic_dataset_mapper INFO: [MaskFormerSemanticDatasetMapper] Augmentations used in inference: []
[01/13 19:20:35] detectron2.data.datasets.coco WARNING: Directory /scratch/local/ssd/anjun/datasets/VOCdevkit/VOC2010/JPEGImages and /scratch/local/ssd/anjun/datasets/VOCdevkit/VOC2010/annotations_detectron2/pc59_val has 11321 and 5104 files, respectively.
[01/13 19:20:35] detectron2.data.datasets.coco WARNING: Will use their intersection of 5104 files.
[01/13 19:20:35] detectron2.data.datasets.coco INFO: Loaded 5104 images with semantic segmentation from /scratch/local/ssd/anjun/datasets/VOCdevkit/VOC2010/JPEGImages
[01/13 19:20:35] detectron2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[01/13 19:20:35] detectron2.data.common INFO: Serializing 5104 elements to byte tensors and concatenating them all ...
[01/13 19:20:35] detectron2.data.common INFO: Serialized dataset takes 1.37 MiB
[01/13 19:20:35] detectron2.data.datasets.coco WARNING: Directory /scratch/local/ssd/anjun/datasets/VOCdevkit/VOC2010/JPEGImages and /scratch/local/ssd/anjun/datasets/VOCdevkit/VOC2010/annotations_detectron2/pc59_val has 11321 and 5104 files, respectively.
[01/13 19:20:35] detectron2.data.datasets.coco WARNING: Will use their intersection of 5104 files.
[01/13 19:20:35] detectron2.data.datasets.coco INFO: Loaded 5104 images with semantic segmentation from /scratch/local/ssd/anjun/datasets/VOCdevkit/VOC2010/JPEGImages
[01/13 19:20:35] detectron2.evaluation.evaluator INFO: Start inference on 1276 batches
[01/13 19:20:44] detectron2.evaluation.evaluator INFO: Inference done 11/1276. Dataloading: 0.0009 s/iter. Inference: 0.1255 s/iter. Eval: 0.0066 s/iter. Total: 0.1330 s/iter. ETA=0:02:48
[01/13 19:20:49] detectron2.evaluation.evaluator INFO: Inference done 49/1276. Dataloading: 0.0014 s/iter. Inference: 0.1253 s/iter. Eval: 0.0064 s/iter. Total: 0.1332 s/iter. ETA=0:02:43
[01/13 19:20:54] detectron2.evaluation.evaluator INFO: Inference done 88/1276. Dataloading: 0.0013 s/iter. Inference: 0.1247 s/iter. Eval: 0.0062 s/iter. Total: 0.1323 s/iter. ETA=0:02:37
[01/13 19:20:59] detectron2.evaluation.evaluator INFO: Inference done 127/1276. Dataloading: 0.0013 s/iter. Inference: 0.1238 s/iter. Eval: 0.0061 s/iter. Total: 0.1312 s/iter. ETA=0:02:30
[01/13 19:21:04] detectron2.evaluation.evaluator INFO: Inference done 166/1276. Dataloading: 0.0013 s/iter. Inference: 0.1236 s/iter. Eval: 0.0059 s/iter. Total: 0.1309 s/iter. ETA=0:02:25
[01/13 19:21:09] detectron2.evaluation.evaluator INFO: Inference done 205/1276. Dataloading: 0.0013 s/iter. Inference: 0.1236 s/iter. Eval: 0.0058 s/iter. Total: 0.1308 s/iter. ETA=0:02:20
[01/13 19:21:14] detectron2.evaluation.evaluator INFO: Inference done 243/1276. Dataloading: 0.0013 s/iter. Inference: 0.1238 s/iter. Eval: 0.0058 s/iter. Total: 0.1310 s/iter. ETA=0:02:15
[01/13 19:21:19] detectron2.evaluation.evaluator INFO: Inference done 282/1276. Dataloading: 0.0013 s/iter. Inference: 0.1238 s/iter. Eval: 0.0058 s/iter. Total: 0.1309 s/iter. ETA=0:02:10
[01/13 19:21:24] detectron2.evaluation.evaluator INFO: Inference done 320/1276. Dataloading: 0.0013 s/iter. Inference: 0.1239 s/iter. Eval: 0.0058 s/iter. Total: 0.1311 s/iter. ETA=0:02:05
[01/13 19:21:29] detectron2.evaluation.evaluator INFO: Inference done 359/1276. Dataloading: 0.0013 s/iter. Inference: 0.1238 s/iter. Eval: 0.0058 s/iter. Total: 0.1309 s/iter. ETA=0:02:00
[01/13 19:21:34] detectron2.evaluation.evaluator INFO: Inference done 398/1276. Dataloading: 0.0014 s/iter. Inference: 0.1237 s/iter. Eval: 0.0058 s/iter. Total: 0.1309 s/iter. ETA=0:01:54
[01/13 19:21:39] detectron2.evaluation.evaluator INFO: Inference done 437/1276. Dataloading: 0.0014 s/iter. Inference: 0.1236 s/iter. Eval: 0.0058 s/iter. Total: 0.1309 s/iter. ETA=0:01:49
[01/13 19:21:44] detectron2.evaluation.evaluator INFO: Inference done 476/1276. Dataloading: 0.0014 s/iter. Inference: 0.1236 s/iter. Eval: 0.0058 s/iter. Total: 0.1309 s/iter. ETA=0:01:44
[01/13 19:21:49] detectron2.evaluation.evaluator INFO: Inference done 514/1276. Dataloading: 0.0014 s/iter. Inference: 0.1237 s/iter. Eval: 0.0058 s/iter. Total: 0.1309 s/iter. ETA=0:01:39
[01/13 19:21:54] detectron2.evaluation.evaluator INFO: Inference done 552/1276. Dataloading: 0.0014 s/iter. Inference: 0.1238 s/iter. Eval: 0.0058 s/iter. Total: 0.1311 s/iter. ETA=0:01:34
[01/13 19:22:00] detectron2.evaluation.evaluator INFO: Inference done 590/1276. Dataloading: 0.0014 s/iter. Inference: 0.1239 s/iter. Eval: 0.0058 s/iter. Total: 0.1312 s/iter. ETA=0:01:29
[01/13 19:22:05] detectron2.evaluation.evaluator INFO: Inference done 629/1276. Dataloading: 0.0014 s/iter. Inference: 0.1238 s/iter. Eval: 0.0058 s/iter. Total: 0.1311 s/iter. ETA=0:01:24
[01/13 19:22:10] detectron2.evaluation.evaluator INFO: Inference done 667/1276. Dataloading: 0.0014 s/iter. Inference: 0.1239 s/iter. Eval: 0.0058 s/iter. Total: 0.1312 s/iter. ETA=0:01:19
[01/13 19:22:15] detectron2.evaluation.evaluator INFO: Inference done 705/1276. Dataloading: 0.0014 s/iter. Inference: 0.1240 s/iter. Eval: 0.0059 s/iter. Total: 0.1313 s/iter. ETA=0:01:14
[01/13 19:22:20] detectron2.evaluation.evaluator INFO: Inference done 742/1276. Dataloading: 0.0014 s/iter. Inference: 0.1242 s/iter. Eval: 0.0059 s/iter. Total: 0.1315 s/iter. ETA=0:01:10
[01/13 19:22:25] detectron2.evaluation.evaluator INFO: Inference done 780/1276. Dataloading: 0.0015 s/iter. Inference: 0.1242 s/iter. Eval: 0.0059 s/iter. Total: 0.1316 s/iter. ETA=0:01:05
[01/13 19:22:30] detectron2.evaluation.evaluator INFO: Inference done 818/1276. Dataloading: 0.0015 s/iter. Inference: 0.1243 s/iter. Eval: 0.0059 s/iter. Total: 0.1316 s/iter. ETA=0:01:00
[01/13 19:22:35] detectron2.evaluation.evaluator INFO: Inference done 856/1276. Dataloading: 0.0014 s/iter. Inference: 0.1243 s/iter. Eval: 0.0059 s/iter. Total: 0.1317 s/iter. ETA=0:00:55
[01/13 19:22:40] detectron2.evaluation.evaluator INFO: Inference done 895/1276. Dataloading: 0.0014 s/iter. Inference: 0.1242 s/iter. Eval: 0.0059 s/iter. Total: 0.1316 s/iter. ETA=0:00:50
[01/13 19:22:45] detectron2.evaluation.evaluator INFO: Inference done 934/1276. Dataloading: 0.0014 s/iter. Inference: 0.1242 s/iter. Eval: 0.0059 s/iter. Total: 0.1316 s/iter. ETA=0:00:44
[01/13 19:22:50] detectron2.evaluation.evaluator INFO: Inference done 972/1276. Dataloading: 0.0014 s/iter. Inference: 0.1242 s/iter. Eval: 0.0059 s/iter. Total: 0.1316 s/iter. ETA=0:00:40
[01/13 19:22:55] detectron2.evaluation.evaluator INFO: Inference done 1010/1276. Dataloading: 0.0014 s/iter. Inference: 0.1243 s/iter. Eval: 0.0059 s/iter. Total: 0.1316 s/iter. ETA=0:00:35
[01/13 19:23:00] detectron2.evaluation.evaluator INFO: Inference done 1049/1276. Dataloading: 0.0014 s/iter. Inference: 0.1242 s/iter. Eval: 0.0059 s/iter. Total: 0.1316 s/iter. ETA=0:00:29
[01/13 19:23:05] detectron2.evaluation.evaluator INFO: Inference done 1088/1276. Dataloading: 0.0014 s/iter. Inference: 0.1242 s/iter. Eval: 0.0059 s/iter. Total: 0.1315 s/iter. ETA=0:00:24
[01/13 19:23:10] detectron2.evaluation.evaluator INFO: Inference done 1126/1276. Dataloading: 0.0014 s/iter. Inference: 0.1242 s/iter. Eval: 0.0059 s/iter. Total: 0.1316 s/iter. ETA=0:00:19
[01/13 19:23:15] detectron2.evaluation.evaluator INFO: Inference done 1165/1276. Dataloading: 0.0014 s/iter. Inference: 0.1242 s/iter. Eval: 0.0059 s/iter. Total: 0.1315 s/iter. ETA=0:00:14
[01/13 19:23:20] detectron2.evaluation.evaluator INFO: Inference done 1204/1276. Dataloading: 0.0014 s/iter. Inference: 0.1242 s/iter. Eval: 0.0059 s/iter. Total: 0.1315 s/iter. ETA=0:00:09
[01/13 19:23:26] detectron2.evaluation.evaluator INFO: Inference done 1243/1276. Dataloading: 0.0014 s/iter. Inference: 0.1241 s/iter. Eval: 0.0059 s/iter. Total: 0.1314 s/iter. ETA=0:00:04
[01/13 19:23:31] detectron2.evaluation.evaluator INFO: Total inference time: 0:02:47.941851 (0.132134 s / iter per device, on 4 devices)
[01/13 19:23:31] detectron2.evaluation.evaluator INFO: Total inference pure compute time: 0:02:37 (0.124064 s / iter per device, on 4 devices)
[01/13 19:42:34] mask_former.data.dataset_mappers.mask_former_semantic_dataset_mapper INFO: [MaskFormerSemanticDatasetMapper] Augmentations used in inference: []
[01/13 19:42:34] detectron2.data.datasets.coco WARNING: Directory /scratch/local/ssd/anjun/datasets/VOCdevkit/VOC2010/JPEGImages and /scratch/local/ssd/anjun/datasets/VOCdevkit/VOC2010/annotations_detectron2/pc59_val has 11321 and 5104 files, respectively.
[01/13 19:42:34] detectron2.data.datasets.coco WARNING: Will use their intersection of 5104 files.
[01/13 19:42:34] detectron2.data.datasets.coco INFO: Loaded 5104 images with semantic segmentation from /scratch/local/ssd/anjun/datasets/VOCdevkit/VOC2010/JPEGImages
[01/13 19:42:34] detectron2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[01/13 19:42:34] detectron2.data.common INFO: Serializing 5104 elements to byte tensors and concatenating them all ...
[01/13 19:42:34] detectron2.data.common INFO: Serialized dataset takes 1.37 MiB
[01/13 19:42:34] detectron2.data.datasets.coco WARNING: Directory /scratch/local/ssd/anjun/datasets/VOCdevkit/VOC2010/JPEGImages and /scratch/local/ssd/anjun/datasets/VOCdevkit/VOC2010/annotations_detectron2/pc59_val has 11321 and 5104 files, respectively.
[01/13 19:42:34] detectron2.data.datasets.coco WARNING: Will use their intersection of 5104 files.
[01/13 19:42:34] detectron2.data.datasets.coco INFO: Loaded 5104 images with semantic segmentation from /scratch/local/ssd/anjun/datasets/VOCdevkit/VOC2010/JPEGImages
[01/13 19:42:34] detectron2.evaluation.evaluator INFO: Start inference on 1276 batches
[01/13 19:42:42] detectron2.evaluation.evaluator INFO: Inference done 11/1276. Dataloading: 0.0007 s/iter. Inference: 0.1235 s/iter. Eval: 0.0062 s/iter. Total: 0.1304 s/iter. ETA=0:02:44
[01/13 19:42:47] detectron2.evaluation.evaluator INFO: Inference done 49/1276. Dataloading: 0.0016 s/iter. Inference: 0.1240 s/iter. Eval: 0.0061 s/iter. Total: 0.1317 s/iter. ETA=0:02:41
[01/13 19:42:52] detectron2.evaluation.evaluator INFO: Inference done 87/1276. Dataloading: 0.0017 s/iter. Inference: 0.1242 s/iter. Eval: 0.0060 s/iter. Total: 0.1320 s/iter. ETA=0:02:36
[01/13 19:42:57] detectron2.evaluation.evaluator INFO: Inference done 126/1276. Dataloading: 0.0017 s/iter. Inference: 0.1233 s/iter. Eval: 0.0060 s/iter. Total: 0.1311 s/iter. ETA=0:02:30
[01/13 19:43:02] detectron2.evaluation.evaluator INFO: Inference done 165/1276. Dataloading: 0.0017 s/iter. Inference: 0.1233 s/iter. Eval: 0.0058 s/iter. Total: 0.1309 s/iter. ETA=0:02:25
[01/13 19:43:07] detectron2.evaluation.evaluator INFO: Inference done 204/1276. Dataloading: 0.0017 s/iter. Inference: 0.1234 s/iter. Eval: 0.0058 s/iter. Total: 0.1309 s/iter. ETA=0:02:20
[01/13 19:43:12] detectron2.evaluation.evaluator INFO: Inference done 242/1276. Dataloading: 0.0017 s/iter. Inference: 0.1236 s/iter. Eval: 0.0058 s/iter. Total: 0.1311 s/iter. ETA=0:02:15
[01/13 19:43:17] detectron2.evaluation.evaluator INFO: Inference done 281/1276. Dataloading: 0.0017 s/iter. Inference: 0.1235 s/iter. Eval: 0.0058 s/iter. Total: 0.1311 s/iter. ETA=0:02:10
[01/13 19:43:22] detectron2.evaluation.evaluator INFO: Inference done 319/1276. Dataloading: 0.0017 s/iter. Inference: 0.1237 s/iter. Eval: 0.0058 s/iter. Total: 0.1312 s/iter. ETA=0:02:05
[01/13 19:43:27] detectron2.evaluation.evaluator INFO: Inference done 358/1276. Dataloading: 0.0017 s/iter. Inference: 0.1236 s/iter. Eval: 0.0058 s/iter. Total: 0.1311 s/iter. ETA=0:02:00
[01/13 19:43:32] detectron2.evaluation.evaluator INFO: Inference done 397/1276. Dataloading: 0.0017 s/iter. Inference: 0.1234 s/iter. Eval: 0.0057 s/iter. Total: 0.1309 s/iter. ETA=0:01:55
[01/13 19:43:38] detectron2.evaluation.evaluator INFO: Inference done 436/1276. Dataloading: 0.0017 s/iter. Inference: 0.1234 s/iter. Eval: 0.0058 s/iter. Total: 0.1309 s/iter. ETA=0:01:49
[01/13 19:43:43] detectron2.evaluation.evaluator INFO: Inference done 475/1276. Dataloading: 0.0017 s/iter. Inference: 0.1234 s/iter. Eval: 0.0058 s/iter. Total: 0.1309 s/iter. ETA=0:01:44
[01/13 19:43:48] detectron2.evaluation.evaluator INFO: Inference done 514/1276. Dataloading: 0.0017 s/iter. Inference: 0.1234 s/iter. Eval: 0.0058 s/iter. Total: 0.1309 s/iter. ETA=0:01:39
[01/13 19:43:53] detectron2.evaluation.evaluator INFO: Inference done 552/1276. Dataloading: 0.0017 s/iter. Inference: 0.1234 s/iter. Eval: 0.0058 s/iter. Total: 0.1310 s/iter. ETA=0:01:34
[01/13 19:43:58] detectron2.evaluation.evaluator INFO: Inference done 590/1276. Dataloading: 0.0017 s/iter. Inference: 0.1235 s/iter. Eval: 0.0058 s/iter. Total: 0.1311 s/iter. ETA=0:01:29
[01/13 19:44:03] detectron2.evaluation.evaluator INFO: Inference done 629/1276. Dataloading: 0.0017 s/iter. Inference: 0.1234 s/iter. Eval: 0.0058 s/iter. Total: 0.1310 s/iter. ETA=0:01:24
[01/13 19:44:08] detectron2.evaluation.evaluator INFO: Inference done 668/1276. Dataloading: 0.0017 s/iter. Inference: 0.1234 s/iter. Eval: 0.0058 s/iter. Total: 0.1309 s/iter. ETA=0:01:19
[01/13 19:44:13] detectron2.evaluation.evaluator INFO: Inference done 706/1276. Dataloading: 0.0017 s/iter. Inference: 0.1235 s/iter. Eval: 0.0058 s/iter. Total: 0.1310 s/iter. ETA=0:01:14
[01/13 19:44:18] detectron2.evaluation.evaluator INFO: Inference done 743/1276. Dataloading: 0.0017 s/iter. Inference: 0.1237 s/iter. Eval: 0.0059 s/iter. Total: 0.1313 s/iter. ETA=0:01:09
[01/13 19:44:23] detectron2.evaluation.evaluator INFO: Inference done 780/1276. Dataloading: 0.0017 s/iter. Inference: 0.1240 s/iter. Eval: 0.0059 s/iter. Total: 0.1315 s/iter. ETA=0:01:05
[01/13 19:44:28] detectron2.evaluation.evaluator INFO: Inference done 818/1276. Dataloading: 0.0017 s/iter. Inference: 0.1240 s/iter. Eval: 0.0059 s/iter. Total: 0.1316 s/iter. ETA=0:01:00
[01/13 19:44:33] detectron2.evaluation.evaluator INFO: Inference done 855/1276. Dataloading: 0.0018 s/iter. Inference: 0.1241 s/iter. Eval: 0.0059 s/iter. Total: 0.1319 s/iter. ETA=0:00:55
[01/13 19:44:38] detectron2.evaluation.evaluator INFO: Inference done 887/1276. Dataloading: 0.0023 s/iter. Inference: 0.1246 s/iter. Eval: 0.0060 s/iter. Total: 0.1328 s/iter. ETA=0:00:51
[01/13 19:44:43] detectron2.evaluation.evaluator INFO: Inference done 919/1276. Dataloading: 0.0023 s/iter. Inference: 0.1253 s/iter. Eval: 0.0061 s/iter. Total: 0.1337 s/iter. ETA=0:00:47
[01/13 19:44:48] detectron2.evaluation.evaluator INFO: Inference done 955/1276. Dataloading: 0.0023 s/iter. Inference: 0.1256 s/iter. Eval: 0.0061 s/iter. Total: 0.1340 s/iter. ETA=0:00:43
[01/13 19:44:53] detectron2.evaluation.evaluator INFO: Inference done 978/1276. Dataloading: 0.0022 s/iter. Inference: 0.1277 s/iter. Eval: 0.0061 s/iter. Total: 0.1361 s/iter. ETA=0:00:40
[01/13 19:44:58] detectron2.evaluation.evaluator INFO: Inference done 1002/1276. Dataloading: 0.0023 s/iter. Inference: 0.1294 s/iter. Eval: 0.0061 s/iter. Total: 0.1378 s/iter. ETA=0:00:37
[01/13 19:45:04] detectron2.evaluation.evaluator INFO: Inference done 1040/1276. Dataloading: 0.0023 s/iter. Inference: 0.1292 s/iter. Eval: 0.0061 s/iter. Total: 0.1376 s/iter. ETA=0:00:32
[01/13 19:45:09] detectron2.evaluation.evaluator INFO: Inference done 1080/1276. Dataloading: 0.0022 s/iter. Inference: 0.1289 s/iter. Eval: 0.0061 s/iter. Total: 0.1373 s/iter. ETA=0:00:26
[01/13 19:45:14] detectron2.evaluation.evaluator INFO: Inference done 1118/1276. Dataloading: 0.0022 s/iter. Inference: 0.1288 s/iter. Eval: 0.0061 s/iter. Total: 0.1371 s/iter. ETA=0:00:21
[01/13 19:45:19] detectron2.evaluation.evaluator INFO: Inference done 1156/1276. Dataloading: 0.0022 s/iter. Inference: 0.1286 s/iter. Eval: 0.0061 s/iter. Total: 0.1370 s/iter. ETA=0:00:16
[01/13 19:45:24] detectron2.evaluation.evaluator INFO: Inference done 1195/1276. Dataloading: 0.0022 s/iter. Inference: 0.1284 s/iter. Eval: 0.0061 s/iter. Total: 0.1367 s/iter. ETA=0:00:11
[01/13 19:45:29] detectron2.evaluation.evaluator INFO: Inference done 1234/1276. Dataloading: 0.0022 s/iter. Inference: 0.1282 s/iter. Eval: 0.0061 s/iter. Total: 0.1365 s/iter. ETA=0:00:05
[01/13 19:45:34] detectron2.evaluation.evaluator INFO: Inference done 1273/1276. Dataloading: 0.0022 s/iter. Inference: 0.1281 s/iter. Eval: 0.0060 s/iter. Total: 0.1363 s/iter. ETA=0:00:00
[01/13 19:45:35] detectron2.evaluation.evaluator INFO: Total inference time: 0:02:54.159838 (0.137026 s / iter per device, on 4 devices)
[01/13 19:45:35] detectron2.evaluation.evaluator INFO: Total inference pure compute time: 0:02:42 (0.128039 s / iter per device, on 4 devices)
[01/13 19:55:46] detectron2.engine.hooks INFO: Overall training speed: 8522 iterations in 2:40:50 (1.1324 s / it)
[01/13 19:55:46] detectron2.engine.hooks INFO: Total training time: 3:05:58 (0:25:08 on hooks)
